{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FraudDetectionTrainModulized_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNt5M58jjCQ5Y/Yb4v7hWwk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffrey82221/cc_fraud_delection/blob/main/FraudDetectionTrainModulized_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_LNZJTIAhKS"
      },
      "source": [
        "# Functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zkh1wY9GAi6y"
      },
      "source": [
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import recall_score, precision_score, precision_recall_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "############################ Preprocessing ###################################\n",
        "def extend_with_log_scale_features(data, log_scale_feature_list):\n",
        "  c_data = copy.copy(data)\n",
        "  for f_name in log_scale_feature_list:\n",
        "    c_data[f_name + '_LOG_SCALE'] = np.log10(data[f_name])\n",
        "  return c_data\n",
        "def extend_with_null_or_not_features(data, has_null_feature_list):\n",
        "  c_data = copy.copy(data)\n",
        "  for f_name in has_null_feature_list:\n",
        "    c_data[f_name + '_NULL_OR_NOT'] = data[f_name].isna().astype(int)\n",
        "  return c_data\n",
        "def extend_with_detailed_time(data, weekday = True, hour = True):\n",
        "  '''\n",
        "  Add WEEKDAY and HOUR and convert DATETIME into strptime format. \n",
        "  '''\n",
        "  c_data = copy.copy(data)\n",
        "  c_data[\"DATETIME\"] = c_data[\"DATETIME\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
        "  if weekday:\n",
        "    c_data[\"WEEKDAY\"] = c_data[\"DATETIME\"].apply(lambda x: x.weekday() + 1)\n",
        "  if hour:\n",
        "    c_data[\"HOUR\"] = c_data[\"DATETIME\"].apply(lambda x: x.hour + 1)\n",
        "  return c_data \n",
        "### Features calculated from current and previous transaction \n",
        "def extend_with_same_shop_features(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_shop_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['MCHNO'].shift(time_shift)\n",
        "    name = \"MCHNO\" + '_SAME' + str(time_shift)\n",
        "    df[name] = (df[\"MCHNO\"] == df['shift']).astype(int)\n",
        "    df[name][df['MCHNO'].isna()] = -1\n",
        "    df[name][df['shift'].isna()] = -1\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add shop identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_shop_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def extend_with_same_MCC(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_MCC_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['MCC'].shift(time_shift)\n",
        "    name = \"MCC\" + '_SAME' + str(time_shift)\n",
        "    df[name] = (df[\"MCC\"] == df['shift']).astype(int)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add MCC identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_MCC_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def extend_with_same_STOCN(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_STOCN_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['STOCN'].shift(time_shift)\n",
        "    name = \"STOCN\" + '_SAME' + str(time_shift)\n",
        "    df[name] = (df[\"STOCN\"] == df['shift']).astype(int)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add STOCN identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_STOCN_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def extend_with_same_FLAM1(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_FLAM1_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['FLAM1'].shift(time_shift)\n",
        "    name = \"FLAM1\" + '_DIFF' + str(time_shift)\n",
        "    df[name] = (df[\"FLAM1\"] - df['shift']).fillna(0)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add FLAM1 identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_FLAM1_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def extend_with_same_class_between_transactions(data, f_name, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_MCC_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])[f_name].shift(time_shift)\n",
        "    name = f_name + '_SAME' + str(time_shift)\n",
        "    df[name] = (df[f_name] == df['shift']).astype(int)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add \" + f_name + \" identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_MCC_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def extend_with_strang_weekday_transaction_change(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def strange_week_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['WEEKDAY'].shift(time_shift)\n",
        "    name = 'WEEKLY_TRANS' + '_STRANGE' + str(time_shift)\n",
        "    df[name] = ((df['WEEKDAY']!=6) & (df['WEEKDAY']!=7) & ((df['shift']==6)|(df['shift']==7))).astype(int)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add \" + 'WEEKDAY' + \" identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = strange_week_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def overall_preprocessing(train_data):\n",
        "  has_null_feature_list = [\n",
        "    \"AVAILABLE_LIMIT_AMT\",\n",
        "    \"BONUS_POINTS\",\n",
        "    \"CURRENT_CASH_ADV_AMT\",\n",
        "    \"CURRENT_FEE\",\n",
        "    \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "    \"CURRENT_PURCH_AMT\",\n",
        "    \"LST_CYCLE_UNPAID_BAL\"\n",
        "    ]\n",
        "  tmp_data = extend_with_null_or_not_features(train_data, has_null_feature_list)\n",
        "\n",
        "  log_scale_feature_list = [\n",
        "    'BNSPT',\n",
        "    'FLAM1',\n",
        "    'ACCT_VINTAGE',\n",
        "    'AVAILABLE_LIMIT_AMT',\n",
        "    'BONUS_POINTS',\n",
        "    'CREDIT_LIMIT_AMT',\n",
        "    'CREDIT_REVOLVING_RATE',\n",
        "    'CREDIT_USE_RATE',\n",
        "    'CURRENT_CASH_ADV_AMT',\n",
        "    'CURRENT_FEE',\n",
        "    'CURRENT_INSTALLMENT_BAL',\n",
        "    'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "    'CURRENT_PURCH_AMT',\n",
        "    'LST_CYCLE_UNPAID_BAL',\n",
        "    'REVOLVING_AMT'\n",
        "  ]\n",
        "  tmp_data = extend_with_log_scale_features(tmp_data, log_scale_feature_list)\n",
        "  tmp_data = extend_with_detailed_time(tmp_data, \n",
        "    weekday = True, hour = True)\n",
        "  tmp_data = extend_with_time_difference_features(tmp_data, \n",
        "    max_time_shift = 20, pivot_feature = 'CHID')\n",
        "  tmp_data = extend_with_same_FLAM1(tmp_data, \n",
        "    max_time_shift = 20, pivot_feature = 'CHID')\n",
        "  tmp_data = extend_with_strang_weekday_transaction_change(tmp_data, \n",
        "    max_time_shift = 5, pivot_feature = 'CHID')\n",
        "  for class_name in ['ECFG', 'PAY_TYPE', 'CONTP', 'ETYMD', 'STOCN', 'SCITY', 'APPFG', 'MCC', 'MCHNO', 'FALLBACK_IND']:\n",
        "    tmp_data = extend_with_same_class_between_transactions(tmp_data, class_name,\n",
        "      max_time_shift = 20, pivot_feature = 'CHID')\n",
        "    \n",
        "  tmp_data = preprocessing(tmp_data)\n",
        "  return tmp_data\n",
        "def extend_with_time_difference_features(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def date_diff(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])[\"DATETIME\"].shift(time_shift)\n",
        "    name = 'DATETIME' + '_DIF' + str(time_shift)\n",
        "    df[name] = (df[\"DATETIME\"] - df['shift']).dt.total_seconds().fillna(0)\n",
        "    # \n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add time difference between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = date_diff(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "\n",
        "def preprocess_null_values(data):\n",
        "  # 將空值填補\n",
        "  c_data = copy.copy(data)\n",
        "  c_data[\n",
        "        c_data.select_dtypes(include=['object']).columns\n",
        "      ] = c_data[\n",
        "        c_data.select_dtypes(include=['object']).columns\n",
        "      ].fillna(\"NULL\")\n",
        "  c_data[\n",
        "      c_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    ] = c_data[\n",
        "      c_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    ].fillna(-1)\n",
        "  return c_data\n",
        "\n",
        "\n",
        "def encode_labels(data):\n",
        "  #將object欄位使用Label Encoder\n",
        "  c_data = copy.copy(data)\n",
        "  labelencoder = LabelEncoder()\n",
        "  obj_col = c_data.select_dtypes(include=['object']).columns.to_list()\n",
        "  for col in obj_col:\n",
        "      c_data[col] = labelencoder.fit_transform(c_data[col])\n",
        "  return c_data\n",
        "def preprocessing(data):\n",
        "  r_data = preprocess_null_values(data)\n",
        "  return encode_labels(r_data)\n",
        "############################ Training Preprocess ############################\n",
        "def resample(data, sampling_rate=0.7, sample_type='downsample'):\n",
        "  # note that testing data should not be re-sampled. \n",
        "  assert sample_type == 'downsample' or sample_type == 'upsample'\n",
        "  c_data = copy.copy(data) \n",
        "  #將資料切分為train&test\n",
        "  if sample_type == 'downsample': \n",
        "    df_fraud = c_data[c_data[\"FRAUD_IND\"] == 1]\n",
        "    df_not_fraud = c_data[c_data[\"FRAUD_IND\"] != 1].sample(frac=sampling_rate, random_state=42)\n",
        "  elif sample_type == 'upsample':\n",
        "    df_fraud = c_data[c_data[\"FRAUD_IND\"] == 1].sample(frac=1./sampling_rate, replace = True, random_state=42)\n",
        "    df_not_fraud = c_data[c_data[\"FRAUD_IND\"] != 1]\n",
        "  df_train = pd.concat([df_fraud, df_not_fraud], 0)\n",
        "  return df_train\n",
        "\n",
        "def create_X(data, drop_list = []):\n",
        "  if drop_list:\n",
        "    return data.drop(drop_list, 1)\n",
        "  else:\n",
        "    return data\n",
        "\n",
        "def create_X_y(data, drop_list = ['FRAUD_IND']):\n",
        "  X = data.drop(drop_list, 1)\n",
        "  y = data[\"FRAUD_IND\"]\n",
        "  return X,y\n",
        "\n",
        "############################ Model Build ####################################\n",
        "def train_lgb(x_train, x_test, y_train, y_test, max_depth = 8, learning_rate = 0.05, n_estimators = 1000):\n",
        "  # n_estimators: number of trees \n",
        "  lgb_train = lgb.Dataset(x_train, y_train)\n",
        "  lgb_test = lgb.Dataset(x_test, y_test)\n",
        "  params = {\n",
        "      \"boosting_type\": \"gbdt\",\n",
        "      \"objective\": \"binary\",\n",
        "      \"metric\": \"binary_logloss\",\n",
        "      \"max_depth\": max_depth,\n",
        "      \"learning_rate\": learning_rate,\n",
        "      \"n_estimators\": n_estimators,\n",
        "  }\n",
        "  trained_model = lgb.train(\n",
        "      params,\n",
        "      lgb_train,\n",
        "      num_boost_round=5000,\n",
        "      valid_sets=[lgb_train, lgb_test],\n",
        "      early_stopping_rounds=30,\n",
        "      verbose_eval=50\n",
        "  )\n",
        "  return trained_model\n",
        "##### Get Result Generated from Model #####################################\n",
        "def evaluate(clf, x_test, y_test):\n",
        "  y_pred = clf.predict(x_test)\n",
        "  precision, recall, threshold = precision_recall_curve(y_test, y_pred)\n",
        "  performance = {\"precision\": precision[0:-1],\n",
        "                \"recall\": recall[0:-1],\n",
        "                \"threshold\": threshold\n",
        "                }\n",
        "  performance[\"f1\"] = 2 * (performance[\"precision\"] * performance[\"recall\"]) / (performance[\"precision\"] + performance[\"recall\"])\n",
        "  performance = pd.DataFrame(performance)\n",
        "  thr = performance[performance[\"f1\"] == max(performance[\"f1\"])][\"threshold\"].values[0]\n",
        "  recall = performance[performance[\"f1\"] == max(performance[\"f1\"])][\"recall\"].values[0]\n",
        "  precision = performance[performance[\"f1\"] == max(performance[\"f1\"])][\"precision\"].values[0]\n",
        "  print(\"Recall Score:\", recall)\n",
        "  print(\"Precision Score:\", precision)\n",
        "  print(\"F1 Score:\", 2 * (precision * recall) / (precision + recall))\n",
        "  print(\"Threshold: \", thr)\n",
        "def get_important_feature_table(clf, x_train):\n",
        "  importance = {\n",
        "  \"col\": np.array(x_train.columns),\n",
        "  \"imp\": lgb.Booster.feature_importance(clf)\n",
        "  }\n",
        "  df_imp = pd.DataFrame(importance).sort_values(by='imp', ascending=False)\n",
        "  return df_imp"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pgE9GA0Ama6"
      },
      "source": [
        "# First Run (for selecting unimportant features) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "2jI2EqA7Arf7",
        "outputId": "562f2d3d-6570-4b22-b70c-19bed6fad084"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "# add AGE \n",
        "# remove weekday and hour \n",
        "tmp_train_data = extend_with_detailed_time(train_data, \n",
        "  weekday = False, hour = False)\n",
        "preprocessed_train_data = preprocessing(tmp_train_data)\n",
        "resampled_train_data = resample(preprocessed_train_data, \n",
        "  sampling_rate=0.7, sample_type='downsample')\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = [\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"])\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=val_percentage, \n",
        "  shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.05, n_estimators = 1000)\n",
        "evaluate(clf, x_test, y_test)\n",
        "important_feature_table = get_important_feature_table(clf, x_train)\n",
        "important_feature_table.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "shape of train data: (533202, 59)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.162201\tvalid_1's binary_logloss: 0.162768\n",
            "[100]\ttraining's binary_logloss: 0.13457\tvalid_1's binary_logloss: 0.13604\n",
            "[150]\ttraining's binary_logloss: 0.12292\tvalid_1's binary_logloss: 0.125427\n",
            "[200]\ttraining's binary_logloss: 0.115362\tvalid_1's binary_logloss: 0.118842\n",
            "[250]\ttraining's binary_logloss: 0.109307\tvalid_1's binary_logloss: 0.113742\n",
            "[300]\ttraining's binary_logloss: 0.103638\tvalid_1's binary_logloss: 0.108953\n",
            "[350]\ttraining's binary_logloss: 0.0989313\tvalid_1's binary_logloss: 0.105065\n",
            "[400]\ttraining's binary_logloss: 0.0948469\tvalid_1's binary_logloss: 0.101827\n",
            "[450]\ttraining's binary_logloss: 0.0910325\tvalid_1's binary_logloss: 0.0986804\n",
            "[500]\ttraining's binary_logloss: 0.0874282\tvalid_1's binary_logloss: 0.0957317\n",
            "[550]\ttraining's binary_logloss: 0.0840682\tvalid_1's binary_logloss: 0.0930062\n",
            "[600]\ttraining's binary_logloss: 0.081057\tvalid_1's binary_logloss: 0.0906978\n",
            "[650]\ttraining's binary_logloss: 0.0782779\tvalid_1's binary_logloss: 0.0886361\n",
            "[700]\ttraining's binary_logloss: 0.0756893\tvalid_1's binary_logloss: 0.0866863\n",
            "[750]\ttraining's binary_logloss: 0.0733625\tvalid_1's binary_logloss: 0.084959\n",
            "[800]\ttraining's binary_logloss: 0.0712952\tvalid_1's binary_logloss: 0.0834625\n",
            "[850]\ttraining's binary_logloss: 0.0691694\tvalid_1's binary_logloss: 0.0818883\n",
            "[900]\ttraining's binary_logloss: 0.0674926\tvalid_1's binary_logloss: 0.0807932\n",
            "[950]\ttraining's binary_logloss: 0.0655437\tvalid_1's binary_logloss: 0.0793593\n",
            "[1000]\ttraining's binary_logloss: 0.0636926\tvalid_1's binary_logloss: 0.0779864\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's binary_logloss: 0.0636926\tvalid_1's binary_logloss: 0.0779864\n",
            "Recall Score: 0.9389421933232892\n",
            "Precision Score: 0.926380848067595\n",
            "F1 Score: 0.9326192257708819\n",
            "Threshold:  0.5136044918384114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col</th>\n",
              "      <th>imp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>CC_VINTAGE</td>\n",
              "      <td>2721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MCC</td>\n",
              "      <td>2437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>SCITY</td>\n",
              "      <td>1885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>FLAM1</td>\n",
              "      <td>1810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>BONUS_POINTS</td>\n",
              "      <td>1545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             col   imp\n",
              "27    CC_VINTAGE  2721\n",
              "0            MCC  2437\n",
              "10         SCITY  1885\n",
              "8          FLAM1  1810\n",
              "37  BONUS_POINTS  1545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1IHBN-CDG3Z"
      },
      "source": [
        "# Best in v2 (Strategy 8)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYmHUtQVC7q8"
      },
      "source": [
        "def overall_preprocessing(train_data):\n",
        "  has_null_feature_list = [\n",
        "    \"AVAILABLE_LIMIT_AMT\",\n",
        "    \"BONUS_POINTS\",\n",
        "    \"CURRENT_CASH_ADV_AMT\",\n",
        "    \"CURRENT_FEE\",\n",
        "    \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "    \"CURRENT_PURCH_AMT\",\n",
        "    \"LST_CYCLE_UNPAID_BAL\"\n",
        "    ]\n",
        "  tmp_data = extend_with_null_or_not_features(train_data, has_null_feature_list)\n",
        "\n",
        "  log_scale_feature_list = [\n",
        "    'BNSPT',\n",
        "    'FLAM1',\n",
        "    'ACCT_VINTAGE',\n",
        "    'AVAILABLE_LIMIT_AMT',\n",
        "    'BONUS_POINTS',\n",
        "    'CREDIT_LIMIT_AMT',\n",
        "    'CREDIT_REVOLVING_RATE',\n",
        "    'CREDIT_USE_RATE',\n",
        "    'CURRENT_CASH_ADV_AMT',\n",
        "    'CURRENT_FEE',\n",
        "    'CURRENT_INSTALLMENT_BAL',\n",
        "    'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "    'CURRENT_PURCH_AMT',\n",
        "    'LST_CYCLE_UNPAID_BAL',\n",
        "    'REVOLVING_AMT'\n",
        "  ]\n",
        "  tmp_data = extend_with_log_scale_features(tmp_data, log_scale_feature_list)\n",
        "  tmp_data = extend_with_detailed_time(tmp_data, \n",
        "    weekday = True, hour = True)\n",
        "  tmp_data = extend_with_time_difference_features(tmp_data, \n",
        "    max_time_shift = 20, pivot_feature = 'CHID')\n",
        "  tmp_data = extend_with_same_FLAM1(tmp_data, \n",
        "    max_time_shift = 20, pivot_feature = 'CHID')\n",
        "  tmp_data = extend_with_strang_weekday_transaction_change(tmp_data, \n",
        "    max_time_shift = 5, pivot_feature = 'CHID')\n",
        "  for class_name in ['ECFG', 'PAY_TYPE', 'CONTP', 'ETYMD', 'STOCN', 'SCITY', 'APPFG', 'MCC', 'MCHNO', 'FALLBACK_IND']:\n",
        "    tmp_data = extend_with_same_class_between_transactions(tmp_data, class_name,\n",
        "      max_time_shift = 20, pivot_feature = 'CHID')\n",
        "    \n",
        "  tmp_data = preprocessing(tmp_data)\n",
        "  return tmp_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8yOqye4hAtJr",
        "outputId": "28597af2-af61-4ecb-a90f-32ffff5b211c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "\n",
        "tmp_data = overall_preprocessing(train_data)\n",
        "resampled_train_data = resample(tmp_data, \n",
        "  sampling_rate=0.14, sample_type='upsample')\n",
        "removed_unimportant_feature_count = 5\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "  test_size=val_percentage, shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.1, n_estimators = 3000)\n",
        "evaluate(clf, x_test, y_test)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "shape of train data: (533202, 59)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n",
            "add FLAM1 identical index between current and 1th-last transaction\n",
            "add FLAM1 identical index between current and 2th-last transaction\n",
            "add FLAM1 identical index between current and 3th-last transaction\n",
            "add FLAM1 identical index between current and 4th-last transaction\n",
            "add FLAM1 identical index between current and 5th-last transaction\n",
            "add FLAM1 identical index between current and 6th-last transaction\n",
            "add FLAM1 identical index between current and 7th-last transaction\n",
            "add FLAM1 identical index between current and 8th-last transaction\n",
            "add FLAM1 identical index between current and 9th-last transaction\n",
            "add FLAM1 identical index between current and 10th-last transaction\n",
            "add FLAM1 identical index between current and 11th-last transaction\n",
            "add FLAM1 identical index between current and 12th-last transaction\n",
            "add FLAM1 identical index between current and 13th-last transaction\n",
            "add FLAM1 identical index between current and 14th-last transaction\n",
            "add FLAM1 identical index between current and 15th-last transaction\n",
            "add FLAM1 identical index between current and 16th-last transaction\n",
            "add FLAM1 identical index between current and 17th-last transaction\n",
            "add FLAM1 identical index between current and 18th-last transaction\n",
            "add FLAM1 identical index between current and 19th-last transaction\n",
            "add FLAM1 identical index between current and 20th-last transaction\n",
            "add WEEKDAY identical index between current and 1th-last transaction\n",
            "add WEEKDAY identical index between current and 2th-last transaction\n",
            "add WEEKDAY identical index between current and 3th-last transaction\n",
            "add WEEKDAY identical index between current and 4th-last transaction\n",
            "add WEEKDAY identical index between current and 5th-last transaction\n",
            "add ECFG identical index between current and 1th-last transaction\n",
            "add ECFG identical index between current and 2th-last transaction\n",
            "add ECFG identical index between current and 3th-last transaction\n",
            "add ECFG identical index between current and 4th-last transaction\n",
            "add ECFG identical index between current and 5th-last transaction\n",
            "add ECFG identical index between current and 6th-last transaction\n",
            "add ECFG identical index between current and 7th-last transaction\n",
            "add ECFG identical index between current and 8th-last transaction\n",
            "add ECFG identical index between current and 9th-last transaction\n",
            "add ECFG identical index between current and 10th-last transaction\n",
            "add ECFG identical index between current and 11th-last transaction\n",
            "add ECFG identical index between current and 12th-last transaction\n",
            "add ECFG identical index between current and 13th-last transaction\n",
            "add ECFG identical index between current and 14th-last transaction\n",
            "add ECFG identical index between current and 15th-last transaction\n",
            "add ECFG identical index between current and 16th-last transaction\n",
            "add ECFG identical index between current and 17th-last transaction\n",
            "add ECFG identical index between current and 18th-last transaction\n",
            "add ECFG identical index between current and 19th-last transaction\n",
            "add ECFG identical index between current and 20th-last transaction\n",
            "add PAY_TYPE identical index between current and 1th-last transaction\n",
            "add PAY_TYPE identical index between current and 2th-last transaction\n",
            "add PAY_TYPE identical index between current and 3th-last transaction\n",
            "add PAY_TYPE identical index between current and 4th-last transaction\n",
            "add PAY_TYPE identical index between current and 5th-last transaction\n",
            "add PAY_TYPE identical index between current and 6th-last transaction\n",
            "add PAY_TYPE identical index between current and 7th-last transaction\n",
            "add PAY_TYPE identical index between current and 8th-last transaction\n",
            "add PAY_TYPE identical index between current and 9th-last transaction\n",
            "add PAY_TYPE identical index between current and 10th-last transaction\n",
            "add PAY_TYPE identical index between current and 11th-last transaction\n",
            "add PAY_TYPE identical index between current and 12th-last transaction\n",
            "add PAY_TYPE identical index between current and 13th-last transaction\n",
            "add PAY_TYPE identical index between current and 14th-last transaction\n",
            "add PAY_TYPE identical index between current and 15th-last transaction\n",
            "add PAY_TYPE identical index between current and 16th-last transaction\n",
            "add PAY_TYPE identical index between current and 17th-last transaction\n",
            "add PAY_TYPE identical index between current and 18th-last transaction\n",
            "add PAY_TYPE identical index between current and 19th-last transaction\n",
            "add PAY_TYPE identical index between current and 20th-last transaction\n",
            "add CONTP identical index between current and 1th-last transaction\n",
            "add CONTP identical index between current and 2th-last transaction\n",
            "add CONTP identical index between current and 3th-last transaction\n",
            "add CONTP identical index between current and 4th-last transaction\n",
            "add CONTP identical index between current and 5th-last transaction\n",
            "add CONTP identical index between current and 6th-last transaction\n",
            "add CONTP identical index between current and 7th-last transaction\n",
            "add CONTP identical index between current and 8th-last transaction\n",
            "add CONTP identical index between current and 9th-last transaction\n",
            "add CONTP identical index between current and 10th-last transaction\n",
            "add CONTP identical index between current and 11th-last transaction\n",
            "add CONTP identical index between current and 12th-last transaction\n",
            "add CONTP identical index between current and 13th-last transaction\n",
            "add CONTP identical index between current and 14th-last transaction\n",
            "add CONTP identical index between current and 15th-last transaction\n",
            "add CONTP identical index between current and 16th-last transaction\n",
            "add CONTP identical index between current and 17th-last transaction\n",
            "add CONTP identical index between current and 18th-last transaction\n",
            "add CONTP identical index between current and 19th-last transaction\n",
            "add CONTP identical index between current and 20th-last transaction\n",
            "add ETYMD identical index between current and 1th-last transaction\n",
            "add ETYMD identical index between current and 2th-last transaction\n",
            "add ETYMD identical index between current and 3th-last transaction\n",
            "add ETYMD identical index between current and 4th-last transaction\n",
            "add ETYMD identical index between current and 5th-last transaction\n",
            "add ETYMD identical index between current and 6th-last transaction\n",
            "add ETYMD identical index between current and 7th-last transaction\n",
            "add ETYMD identical index between current and 8th-last transaction\n",
            "add ETYMD identical index between current and 9th-last transaction\n",
            "add ETYMD identical index between current and 10th-last transaction\n",
            "add ETYMD identical index between current and 11th-last transaction\n",
            "add ETYMD identical index between current and 12th-last transaction\n",
            "add ETYMD identical index between current and 13th-last transaction\n",
            "add ETYMD identical index between current and 14th-last transaction\n",
            "add ETYMD identical index between current and 15th-last transaction\n",
            "add ETYMD identical index between current and 16th-last transaction\n",
            "add ETYMD identical index between current and 17th-last transaction\n",
            "add ETYMD identical index between current and 18th-last transaction\n",
            "add ETYMD identical index between current and 19th-last transaction\n",
            "add ETYMD identical index between current and 20th-last transaction\n",
            "add STOCN identical index between current and 1th-last transaction\n",
            "add STOCN identical index between current and 2th-last transaction\n",
            "add STOCN identical index between current and 3th-last transaction\n",
            "add STOCN identical index between current and 4th-last transaction\n",
            "add STOCN identical index between current and 5th-last transaction\n",
            "add STOCN identical index between current and 6th-last transaction\n",
            "add STOCN identical index between current and 7th-last transaction\n",
            "add STOCN identical index between current and 8th-last transaction\n",
            "add STOCN identical index between current and 9th-last transaction\n",
            "add STOCN identical index between current and 10th-last transaction\n",
            "add STOCN identical index between current and 11th-last transaction\n",
            "add STOCN identical index between current and 12th-last transaction\n",
            "add STOCN identical index between current and 13th-last transaction\n",
            "add STOCN identical index between current and 14th-last transaction\n",
            "add STOCN identical index between current and 15th-last transaction\n",
            "add STOCN identical index between current and 16th-last transaction\n",
            "add STOCN identical index between current and 17th-last transaction\n",
            "add STOCN identical index between current and 18th-last transaction\n",
            "add STOCN identical index between current and 19th-last transaction\n",
            "add STOCN identical index between current and 20th-last transaction\n",
            "add SCITY identical index between current and 1th-last transaction\n",
            "add SCITY identical index between current and 2th-last transaction\n",
            "add SCITY identical index between current and 3th-last transaction\n",
            "add SCITY identical index between current and 4th-last transaction\n",
            "add SCITY identical index between current and 5th-last transaction\n",
            "add SCITY identical index between current and 6th-last transaction\n",
            "add SCITY identical index between current and 7th-last transaction\n",
            "add SCITY identical index between current and 8th-last transaction\n",
            "add SCITY identical index between current and 9th-last transaction\n",
            "add SCITY identical index between current and 10th-last transaction\n",
            "add SCITY identical index between current and 11th-last transaction\n",
            "add SCITY identical index between current and 12th-last transaction\n",
            "add SCITY identical index between current and 13th-last transaction\n",
            "add SCITY identical index between current and 14th-last transaction\n",
            "add SCITY identical index between current and 15th-last transaction\n",
            "add SCITY identical index between current and 16th-last transaction\n",
            "add SCITY identical index between current and 17th-last transaction\n",
            "add SCITY identical index between current and 18th-last transaction\n",
            "add SCITY identical index between current and 19th-last transaction\n",
            "add SCITY identical index between current and 20th-last transaction\n",
            "add APPFG identical index between current and 1th-last transaction\n",
            "add APPFG identical index between current and 2th-last transaction\n",
            "add APPFG identical index between current and 3th-last transaction\n",
            "add APPFG identical index between current and 4th-last transaction\n",
            "add APPFG identical index between current and 5th-last transaction\n",
            "add APPFG identical index between current and 6th-last transaction\n",
            "add APPFG identical index between current and 7th-last transaction\n",
            "add APPFG identical index between current and 8th-last transaction\n",
            "add APPFG identical index between current and 9th-last transaction\n",
            "add APPFG identical index between current and 10th-last transaction\n",
            "add APPFG identical index between current and 11th-last transaction\n",
            "add APPFG identical index between current and 12th-last transaction\n",
            "add APPFG identical index between current and 13th-last transaction\n",
            "add APPFG identical index between current and 14th-last transaction\n",
            "add APPFG identical index between current and 15th-last transaction\n",
            "add APPFG identical index between current and 16th-last transaction\n",
            "add APPFG identical index between current and 17th-last transaction\n",
            "add APPFG identical index between current and 18th-last transaction\n",
            "add APPFG identical index between current and 19th-last transaction\n",
            "add APPFG identical index between current and 20th-last transaction\n",
            "add MCC identical index between current and 1th-last transaction\n",
            "add MCC identical index between current and 2th-last transaction\n",
            "add MCC identical index between current and 3th-last transaction\n",
            "add MCC identical index between current and 4th-last transaction\n",
            "add MCC identical index between current and 5th-last transaction\n",
            "add MCC identical index between current and 6th-last transaction\n",
            "add MCC identical index between current and 7th-last transaction\n",
            "add MCC identical index between current and 8th-last transaction\n",
            "add MCC identical index between current and 9th-last transaction\n",
            "add MCC identical index between current and 10th-last transaction\n",
            "add MCC identical index between current and 11th-last transaction\n",
            "add MCC identical index between current and 12th-last transaction\n",
            "add MCC identical index between current and 13th-last transaction\n",
            "add MCC identical index between current and 14th-last transaction\n",
            "add MCC identical index between current and 15th-last transaction\n",
            "add MCC identical index between current and 16th-last transaction\n",
            "add MCC identical index between current and 17th-last transaction\n",
            "add MCC identical index between current and 18th-last transaction\n",
            "add MCC identical index between current and 19th-last transaction\n",
            "add MCC identical index between current and 20th-last transaction\n",
            "add MCHNO identical index between current and 1th-last transaction\n",
            "add MCHNO identical index between current and 2th-last transaction\n",
            "add MCHNO identical index between current and 3th-last transaction\n",
            "add MCHNO identical index between current and 4th-last transaction\n",
            "add MCHNO identical index between current and 5th-last transaction\n",
            "add MCHNO identical index between current and 6th-last transaction\n",
            "add MCHNO identical index between current and 7th-last transaction\n",
            "add MCHNO identical index between current and 8th-last transaction\n",
            "add MCHNO identical index between current and 9th-last transaction\n",
            "add MCHNO identical index between current and 10th-last transaction\n",
            "add MCHNO identical index between current and 11th-last transaction\n",
            "add MCHNO identical index between current and 12th-last transaction\n",
            "add MCHNO identical index between current and 13th-last transaction\n",
            "add MCHNO identical index between current and 14th-last transaction\n",
            "add MCHNO identical index between current and 15th-last transaction\n",
            "add MCHNO identical index between current and 16th-last transaction\n",
            "add MCHNO identical index between current and 17th-last transaction\n",
            "add MCHNO identical index between current and 18th-last transaction\n",
            "add MCHNO identical index between current and 19th-last transaction\n",
            "add MCHNO identical index between current and 20th-last transaction\n",
            "add FALLBACK_IND identical index between current and 1th-last transaction\n",
            "add FALLBACK_IND identical index between current and 2th-last transaction\n",
            "add FALLBACK_IND identical index between current and 3th-last transaction\n",
            "add FALLBACK_IND identical index between current and 4th-last transaction\n",
            "add FALLBACK_IND identical index between current and 5th-last transaction\n",
            "add FALLBACK_IND identical index between current and 6th-last transaction\n",
            "add FALLBACK_IND identical index between current and 7th-last transaction\n",
            "add FALLBACK_IND identical index between current and 8th-last transaction\n",
            "add FALLBACK_IND identical index between current and 9th-last transaction\n",
            "add FALLBACK_IND identical index between current and 10th-last transaction\n",
            "add FALLBACK_IND identical index between current and 11th-last transaction\n",
            "add FALLBACK_IND identical index between current and 12th-last transaction\n",
            "add FALLBACK_IND identical index between current and 13th-last transaction\n",
            "add FALLBACK_IND identical index between current and 14th-last transaction\n",
            "add FALLBACK_IND identical index between current and 15th-last transaction\n",
            "add FALLBACK_IND identical index between current and 16th-last transaction\n",
            "add FALLBACK_IND identical index between current and 17th-last transaction\n",
            "add FALLBACK_IND identical index between current and 18th-last transaction\n",
            "add FALLBACK_IND identical index between current and 19th-last transaction\n",
            "add FALLBACK_IND identical index between current and 20th-last transaction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'clf = train_lgb(x_train, x_test, y_train, y_test, \\n  max_depth = 8, learning_rate = 0.1, n_estimators = 3000)\\nevaluate(clf, x_test, y_test)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5ktH5T4DgIh"
      },
      "source": [
        "# Generate Testing Result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3k1GKK_DhxP"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/test.csv')\n",
        "#查看資料筆數\n",
        "print(\"shape of test data:\" , test_data.shape)\n",
        "\n",
        "tmp_data = overall_preprocessing(test_data)\n",
        "\n",
        "X = create_X(resampled_train_data, \n",
        "  drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "y_pred = clf.predict(X)\n",
        "\n",
        "threshold = 0.998\n",
        "y_result = (y_pred > threshold).astype(int).T\n",
        "result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "print(\"imbalance rate of test data:\", result_table['FRAUD_IND'].mean())\n",
        "result_table.to_csv('tmp_submission.csv')\n",
        "print('csv saved.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Js2fer-Dr8g"
      },
      "source": [
        "# Strategy 9: Add One-Hot Encoding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy4l5eDQEp6w"
      },
      "source": [
        "data = copy.copy(train_data)\n",
        "for f_name in features_to_be_onehot_encoded:\n",
        "  data[f_name] = data[f_name].fillna('NULL')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "TcWUWXdFGHQx",
        "outputId": "8bdc8600-dd7e-47bf-a7de-d8eb7b93c05a"
      },
      "source": [
        "ohe.fit_transform(data[f_name])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f36037a73a14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \"\"\"\n\u001b[1;32m    371\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \"\"\"\n\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_idx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_drop_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mX_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iloc'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# if not a dataframe, do normal check_array validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mX_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             if (not hasattr(X, 'dtype')\n\u001b[1;32m     45\u001b[0m                     and np.issubdtype(X_temp.dtype, np.str_)):\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['0.0' '0.0' '0.0' ... 'NULL' '0.0' '0.0'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "KqviLuY3FJXm",
        "outputId": "047f92c6-a9b8-4ef7-f029-6b7cd390b47b"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "onehotencoder = OneHotEncoder()\n",
        "onehotencoder.fit(train_data[f_name])\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-aa77db67e087>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0monehotencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0monehotencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \"\"\"\n\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_idx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_drop_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mX_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iloc'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# if not a dataframe, do normal check_array validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mX_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             if (not hasattr(X, 'dtype')\n\u001b[1;32m     45\u001b[0m                     and np.issubdtype(X_temp.dtype, np.str_)):\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 0.  0.  0. ... nan  0.  0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGspBm-cEFLk"
      },
      "source": [
        "features_to_be_onehot_encoded = [\"CONTP\", \n",
        "  \"ETYMD\", \n",
        "  \"STOCN\", \n",
        "  \"PAY_TYPE\", \n",
        "  \"CATP1\", \n",
        "  \"CUORG\", \n",
        "  \"TSCFG\", \n",
        "  \"CC_CUST_LEVEL\", \n",
        "  \"EDU_CODE\", \n",
        "  \"INCOME_RANGE_CODE\", \n",
        "  \"MARITAL_STATUS_CODE\", \n",
        "  \"OCUP_CODE\", \n",
        "  \"POSITION_CODE\", \n",
        "  \"CC_PAY_LEVEL_CODE\", \n",
        "  \"CURRENT_VIO_AMT\"\n",
        "]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMugfwQuEjyM"
      },
      "source": [
        "features_to_be_onehot_encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmWIkxkODxL_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "\n",
        "tmp_data = overall_preprocessing(train_data)\n",
        "resampled_train_data = resample(tmp_data, \n",
        "  sampling_rate=0.14, sample_type='upsample')\n",
        "removed_unimportant_feature_count = 5\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "  test_size=val_percentage, shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.1, n_estimators = 3000)\n",
        "evaluate(clf, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}