{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FraudDetectionTrainModulized_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7Yybxs4yU6mHo8jcCY5Ng",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffrey82221/cc_fraud_delection/blob/main/FraudDetectionTrainModulized_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_LNZJTIAhKS"
      },
      "source": [
        "# Functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zkh1wY9GAi6y"
      },
      "source": [
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import recall_score, precision_score, precision_recall_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "############################ Preprocessing ###################################\n",
        "def extend_with_log_scale_features(data, log_scale_feature_list):\n",
        "  c_data = copy.copy(data)\n",
        "  for f_name in log_scale_feature_list:\n",
        "    c_data[f_name + '_LOG_SCALE'] = np.log10(data[f_name])\n",
        "  return c_data\n",
        "def extend_with_null_or_not_features(data, has_null_feature_list):\n",
        "  c_data = copy.copy(data)\n",
        "  for f_name in has_null_feature_list:\n",
        "    c_data[f_name + '_NULL_OR_NOT'] = data[f_name].isna().astype(int)\n",
        "  return c_data\n",
        "def extend_with_detailed_time(data, weekday = True, hour = True):\n",
        "  '''\n",
        "  Add WEEKDAY and HOUR and convert DATETIME into strptime format. \n",
        "  '''\n",
        "  c_data = copy.copy(data)\n",
        "  c_data[\"DATETIME\"] = c_data[\"DATETIME\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
        "  if weekday:\n",
        "    c_data[\"WEEKDAY\"] = c_data[\"DATETIME\"].apply(lambda x: x.weekday() + 1)\n",
        "  if hour:\n",
        "    c_data[\"HOUR\"] = c_data[\"DATETIME\"].apply(lambda x: x.hour + 1)\n",
        "  return c_data \n",
        "### Features calculated from current and previous transaction \n",
        "def extend_with_same_shop_features(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_shop_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['MCHNO'].shift(time_shift)\n",
        "    name = \"MCHNO\" + '_SAME' + str(time_shift)\n",
        "    df[name] = (df[\"MCHNO\"] == df['shift']).astype(int)\n",
        "    df[name][df['MCHNO'].isna()] = -1\n",
        "    df[name][df['shift'].isna()] = -1\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add shop identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_shop_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def extend_with_same_MCC(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_MCC_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['MCC'].shift(time_shift)\n",
        "    name = \"MCC\" + '_SAME' + str(time_shift)\n",
        "    df[name] = (df[\"MCC\"] == df['shift']).astype(int)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add MCC identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_MCC_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def extend_with_same_STOCN(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_STOCN_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['STOCN'].shift(time_shift)\n",
        "    name = \"STOCN\" + '_SAME' + str(time_shift)\n",
        "    df[name] = (df[\"STOCN\"] == df['shift']).astype(int)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add STOCN identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_STOCN_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def extend_with_same_FLAM1(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_FLAM1_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['FLAM1'].shift(time_shift)\n",
        "    name = \"FLAM1\" + '_DIFF' + str(time_shift)\n",
        "    df[name] = (df[\"FLAM1\"] - df['shift']).fillna(0)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add FLAM1 identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_FLAM1_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def extend_with_same_class_between_transactions(data, f_name, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_MCC_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])[f_name].shift(time_shift)\n",
        "    name = f_name + '_SAME' + str(time_shift)\n",
        "    df[name] = (df[f_name] == df['shift']).astype(int)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add \" + f_name + \" identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_MCC_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def extend_with_strang_weekday_transaction_change(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def strange_week_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['WEEKDAY'].shift(time_shift)\n",
        "    name = 'WEEKLY_TRANS' + '_STRANGE' + str(time_shift)\n",
        "    df[name] = ((df['WEEKDAY']!=6) & (df['WEEKDAY']!=7) & ((df['shift']==6)|(df['shift']==7))).astype(int)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add \" + 'WEEKDAY' + \" identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = strange_week_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def overall_preprocessing(train_data):\n",
        "  has_null_feature_list = [\n",
        "    \"AVAILABLE_LIMIT_AMT\",\n",
        "    \"BONUS_POINTS\",\n",
        "    \"CURRENT_CASH_ADV_AMT\",\n",
        "    \"CURRENT_FEE\",\n",
        "    \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "    \"CURRENT_PURCH_AMT\",\n",
        "    \"LST_CYCLE_UNPAID_BAL\"\n",
        "    ]\n",
        "  tmp_data = extend_with_null_or_not_features(train_data, has_null_feature_list)\n",
        "\n",
        "  log_scale_feature_list = [\n",
        "    'BNSPT',\n",
        "    'FLAM1',\n",
        "    'ACCT_VINTAGE',\n",
        "    'AVAILABLE_LIMIT_AMT',\n",
        "    'BONUS_POINTS',\n",
        "    'CREDIT_LIMIT_AMT',\n",
        "    'CREDIT_REVOLVING_RATE',\n",
        "    'CREDIT_USE_RATE',\n",
        "    'CURRENT_CASH_ADV_AMT',\n",
        "    'CURRENT_FEE',\n",
        "    'CURRENT_INSTALLMENT_BAL',\n",
        "    'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "    'CURRENT_PURCH_AMT',\n",
        "    'LST_CYCLE_UNPAID_BAL',\n",
        "    'REVOLVING_AMT'\n",
        "  ]\n",
        "  tmp_data = extend_with_log_scale_features(tmp_data, log_scale_feature_list)\n",
        "  tmp_data = extend_with_detailed_time(tmp_data, \n",
        "    weekday = True, hour = True)\n",
        "  tmp_data = extend_with_time_difference_features(tmp_data, \n",
        "    max_time_shift = 20, pivot_feature = 'CHID')\n",
        "  tmp_data = extend_with_same_FLAM1(tmp_data, \n",
        "    max_time_shift = 20, pivot_feature = 'CHID')\n",
        "  tmp_data = extend_with_strang_weekday_transaction_change(tmp_data, \n",
        "    max_time_shift = 5, pivot_feature = 'CHID')\n",
        "  for class_name in ['ECFG', 'PAY_TYPE', 'CONTP', 'ETYMD', 'STOCN', 'SCITY', 'APPFG', 'MCC', 'MCHNO', 'FALLBACK_IND']:\n",
        "    tmp_data = extend_with_same_class_between_transactions(tmp_data, class_name,\n",
        "      max_time_shift = 20, pivot_feature = 'CHID')\n",
        "    \n",
        "  tmp_data = preprocessing(tmp_data)\n",
        "  return tmp_data\n",
        "def extend_with_time_difference_features(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def date_diff(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])[\"DATETIME\"].shift(time_shift)\n",
        "    name = 'DATETIME' + '_DIF' + str(time_shift)\n",
        "    df[name] = (df[\"DATETIME\"] - df['shift']).dt.total_seconds().fillna(0)\n",
        "    # \n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add time difference between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = date_diff(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "\n",
        "def preprocess_null_values(data):\n",
        "  # 將空值填補\n",
        "  c_data = copy.copy(data)\n",
        "  c_data[\n",
        "        c_data.select_dtypes(include=['object']).columns\n",
        "      ] = c_data[\n",
        "        c_data.select_dtypes(include=['object']).columns\n",
        "      ].fillna(\"NULL\")\n",
        "  c_data[\n",
        "      c_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    ] = c_data[\n",
        "      c_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    ].fillna(-1)\n",
        "  return c_data\n",
        "\n",
        "\n",
        "def encode_labels(data):\n",
        "  #將object欄位使用Label Encoder\n",
        "  c_data = copy.copy(data)\n",
        "  labelencoder = LabelEncoder()\n",
        "  obj_col = c_data.select_dtypes(include=['object']).columns.to_list()\n",
        "  for col in obj_col:\n",
        "    c_data[col] = labelencoder.fit_transform(c_data[col])\n",
        "  return c_data\n",
        "def preprocessing(data):\n",
        "  r_data = preprocess_null_values(data)\n",
        "  return encode_labels(r_data)\n",
        "############################ Training Preprocess ############################\n",
        "def resample(data, sampling_rate=0.7, sample_type='downsample'):\n",
        "  # note that testing data should not be re-sampled. \n",
        "  assert sample_type == 'downsample' or sample_type == 'upsample'\n",
        "  c_data = copy.copy(data) \n",
        "  #將資料切分為train&test\n",
        "  if sample_type == 'downsample': \n",
        "    df_fraud = c_data[c_data[\"FRAUD_IND\"] == 1]\n",
        "    df_not_fraud = c_data[c_data[\"FRAUD_IND\"] != 1].sample(frac=sampling_rate, random_state=42)\n",
        "  elif sample_type == 'upsample':\n",
        "    df_fraud = c_data[c_data[\"FRAUD_IND\"] == 1].sample(frac=1./sampling_rate, replace = True, random_state=42)\n",
        "    df_not_fraud = c_data[c_data[\"FRAUD_IND\"] != 1]\n",
        "  df_train = pd.concat([df_fraud, df_not_fraud], 0)\n",
        "  return df_train\n",
        "\n",
        "def create_X(data, drop_list = []):\n",
        "  if drop_list:\n",
        "    return data.drop(drop_list, 1)\n",
        "  else:\n",
        "    return data\n",
        "\n",
        "def create_X_y(data, drop_list = ['FRAUD_IND']):\n",
        "  X = data.drop(drop_list, 1)\n",
        "  y = data[\"FRAUD_IND\"]\n",
        "  return X,y\n",
        "\n",
        "############################ Model Build ####################################\n",
        "def train_lgb(x_train, x_test, y_train, y_test, max_depth = 8, learning_rate = 0.05, n_estimators = 1000):\n",
        "  # n_estimators: number of trees \n",
        "  lgb_train = lgb.Dataset(x_train, y_train)\n",
        "  lgb_test = lgb.Dataset(x_test, y_test)\n",
        "  params = {\n",
        "      \"boosting_type\": \"gbdt\",\n",
        "      \"objective\": \"binary\",\n",
        "      \"metric\": \"binary_logloss\",\n",
        "      \"max_depth\": max_depth,\n",
        "      \"learning_rate\": learning_rate,\n",
        "      \"n_estimators\": n_estimators,\n",
        "  }\n",
        "  trained_model = lgb.train(\n",
        "      params,\n",
        "      lgb_train,\n",
        "      num_boost_round=5000,\n",
        "      valid_sets=[lgb_train, lgb_test],\n",
        "      early_stopping_rounds=30,\n",
        "      verbose_eval=50\n",
        "  )\n",
        "  return trained_model\n",
        "##### Get Result Generated from Model #####################################\n",
        "def evaluate(clf, x_test, y_test):\n",
        "  y_pred = clf.predict(x_test)\n",
        "  precision, recall, threshold = precision_recall_curve(y_test, y_pred)\n",
        "  performance = {\"precision\": precision[0:-1],\n",
        "                \"recall\": recall[0:-1],\n",
        "                \"threshold\": threshold\n",
        "                }\n",
        "  performance[\"f1\"] = 2 * (performance[\"precision\"] * performance[\"recall\"]) / (performance[\"precision\"] + performance[\"recall\"])\n",
        "  performance = pd.DataFrame(performance)\n",
        "  thr = performance[performance[\"f1\"] == max(performance[\"f1\"])][\"threshold\"].values[0]\n",
        "  recall = performance[performance[\"f1\"] == max(performance[\"f1\"])][\"recall\"].values[0]\n",
        "  precision = performance[performance[\"f1\"] == max(performance[\"f1\"])][\"precision\"].values[0]\n",
        "  print(\"Recall Score:\", round(recall,4))\n",
        "  print(\"Precision Score:\", round(precision,4))\n",
        "  f1 = 2 * (precision * recall) / (precision + recall)\n",
        "  print(\"F1 Score:\", round(f1,4))\n",
        "  print(\"Threshold: \", round(thr,4))\n",
        "def get_important_feature_table(clf, x_train):\n",
        "  importance = {\n",
        "  \"col\": np.array(x_train.columns),\n",
        "  \"imp\": lgb.Booster.feature_importance(clf)\n",
        "  }\n",
        "  df_imp = pd.DataFrame(importance).sort_values(by='imp', ascending=False)\n",
        "  return df_imp"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pgE9GA0Ama6"
      },
      "source": [
        "# First Run (for selecting unimportant features) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "2jI2EqA7Arf7",
        "outputId": "99430e68-d1d5-4903-b308-8bd755390fe4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "# add AGE \n",
        "# remove weekday and hour \n",
        "tmp_train_data = extend_with_detailed_time(train_data, \n",
        "  weekday = False, hour = False)\n",
        "preprocessed_train_data = preprocessing(tmp_train_data)\n",
        "resampled_train_data = resample(preprocessed_train_data, \n",
        "  sampling_rate=0.7, sample_type='downsample')\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = [\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"])\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=val_percentage, \n",
        "  shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.05, n_estimators = 1000)\n",
        "evaluate(clf, x_test, y_test)\n",
        "important_feature_table = get_important_feature_table(clf, x_train)\n",
        "important_feature_table.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "shape of train data: (533202, 59)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.162201\tvalid_1's binary_logloss: 0.162768\n",
            "[100]\ttraining's binary_logloss: 0.13457\tvalid_1's binary_logloss: 0.13604\n",
            "[150]\ttraining's binary_logloss: 0.12292\tvalid_1's binary_logloss: 0.125427\n",
            "[200]\ttraining's binary_logloss: 0.115362\tvalid_1's binary_logloss: 0.118842\n",
            "[250]\ttraining's binary_logloss: 0.109307\tvalid_1's binary_logloss: 0.113742\n",
            "[300]\ttraining's binary_logloss: 0.103638\tvalid_1's binary_logloss: 0.108953\n",
            "[350]\ttraining's binary_logloss: 0.0989313\tvalid_1's binary_logloss: 0.105065\n",
            "[400]\ttraining's binary_logloss: 0.0948469\tvalid_1's binary_logloss: 0.101827\n",
            "[450]\ttraining's binary_logloss: 0.0910325\tvalid_1's binary_logloss: 0.0986804\n",
            "[500]\ttraining's binary_logloss: 0.0874282\tvalid_1's binary_logloss: 0.0957317\n",
            "[550]\ttraining's binary_logloss: 0.0840682\tvalid_1's binary_logloss: 0.0930062\n",
            "[600]\ttraining's binary_logloss: 0.081057\tvalid_1's binary_logloss: 0.0906978\n",
            "[650]\ttraining's binary_logloss: 0.0782779\tvalid_1's binary_logloss: 0.0886361\n",
            "[700]\ttraining's binary_logloss: 0.0756893\tvalid_1's binary_logloss: 0.0866863\n",
            "[750]\ttraining's binary_logloss: 0.0733625\tvalid_1's binary_logloss: 0.084959\n",
            "[800]\ttraining's binary_logloss: 0.0712952\tvalid_1's binary_logloss: 0.0834625\n",
            "[850]\ttraining's binary_logloss: 0.0691694\tvalid_1's binary_logloss: 0.0818883\n",
            "[900]\ttraining's binary_logloss: 0.0674926\tvalid_1's binary_logloss: 0.0807932\n",
            "[950]\ttraining's binary_logloss: 0.0655437\tvalid_1's binary_logloss: 0.0793593\n",
            "[1000]\ttraining's binary_logloss: 0.0636926\tvalid_1's binary_logloss: 0.0779864\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's binary_logloss: 0.0636926\tvalid_1's binary_logloss: 0.0779864\n",
            "Recall Score: 0.9389421933232892\n",
            "Precision Score: 0.926380848067595\n",
            "F1 Score: 0.9326192257708819\n",
            "Threshold:  0.5136044918384114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col</th>\n",
              "      <th>imp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>CC_VINTAGE</td>\n",
              "      <td>2721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MCC</td>\n",
              "      <td>2437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>SCITY</td>\n",
              "      <td>1885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>FLAM1</td>\n",
              "      <td>1810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>BONUS_POINTS</td>\n",
              "      <td>1545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             col   imp\n",
              "27    CC_VINTAGE  2721\n",
              "0            MCC  2437\n",
              "10         SCITY  1885\n",
              "8          FLAM1  1810\n",
              "37  BONUS_POINTS  1545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1IHBN-CDG3Z"
      },
      "source": [
        "# Best in v2 (Strategy 8)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYmHUtQVC7q8"
      },
      "source": [
        "def overall_preprocessing(train_data):\n",
        "  has_null_feature_list = [\n",
        "    \"AVAILABLE_LIMIT_AMT\",\n",
        "    \"BONUS_POINTS\",\n",
        "    \"CURRENT_CASH_ADV_AMT\",\n",
        "    \"CURRENT_FEE\",\n",
        "    \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "    \"CURRENT_PURCH_AMT\",\n",
        "    \"LST_CYCLE_UNPAID_BAL\"\n",
        "    ]\n",
        "  tmp_data = extend_with_null_or_not_features(train_data, has_null_feature_list)\n",
        "\n",
        "  log_scale_feature_list = [\n",
        "    'BNSPT',\n",
        "    'FLAM1',\n",
        "    'ACCT_VINTAGE',\n",
        "    'AVAILABLE_LIMIT_AMT',\n",
        "    'BONUS_POINTS',\n",
        "    'CREDIT_LIMIT_AMT',\n",
        "    'CREDIT_REVOLVING_RATE',\n",
        "    'CREDIT_USE_RATE',\n",
        "    'CURRENT_CASH_ADV_AMT',\n",
        "    'CURRENT_FEE',\n",
        "    'CURRENT_INSTALLMENT_BAL',\n",
        "    'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "    'CURRENT_PURCH_AMT',\n",
        "    'LST_CYCLE_UNPAID_BAL',\n",
        "    'REVOLVING_AMT'\n",
        "  ]\n",
        "  tmp_data = extend_with_log_scale_features(tmp_data, log_scale_feature_list)\n",
        "  tmp_data = extend_with_detailed_time(tmp_data, \n",
        "    weekday = True, hour = True)\n",
        "  tmp_data = extend_with_time_difference_features(tmp_data, \n",
        "    max_time_shift = 20, pivot_feature = 'CHID')\n",
        "  tmp_data = extend_with_same_FLAM1(tmp_data, \n",
        "    max_time_shift = 20, pivot_feature = 'CHID')\n",
        "  tmp_data = extend_with_strang_weekday_transaction_change(tmp_data, \n",
        "    max_time_shift = 20, pivot_feature = 'CHID')\n",
        "  for class_name in ['ECFG', 'PAY_TYPE', 'CONTP', 'ETYMD', 'STOCN', 'SCITY', 'APPFG', 'MCC', 'MCHNO', 'FALLBACK_IND']:\n",
        "    tmp_data = extend_with_same_class_between_transactions(tmp_data, class_name,\n",
        "      max_time_shift = 5, pivot_feature = 'CHID')\n",
        "    \n",
        "  tmp_data = preprocessing(tmp_data)\n",
        "  return tmp_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yOqye4hAtJr",
        "outputId": "a77c35b5-13ee-481d-c9b0-4d83de2e218e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "tmp_data = overall_preprocessing(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of train data: (533202, 59)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n",
            "add FLAM1 identical index between current and 1th-last transaction\n",
            "add FLAM1 identical index between current and 2th-last transaction\n",
            "add FLAM1 identical index between current and 3th-last transaction\n",
            "add FLAM1 identical index between current and 4th-last transaction\n",
            "add FLAM1 identical index between current and 5th-last transaction\n",
            "add FLAM1 identical index between current and 6th-last transaction\n",
            "add FLAM1 identical index between current and 7th-last transaction\n",
            "add FLAM1 identical index between current and 8th-last transaction\n",
            "add FLAM1 identical index between current and 9th-last transaction\n",
            "add FLAM1 identical index between current and 10th-last transaction\n",
            "add FLAM1 identical index between current and 11th-last transaction\n",
            "add FLAM1 identical index between current and 12th-last transaction\n",
            "add FLAM1 identical index between current and 13th-last transaction\n",
            "add FLAM1 identical index between current and 14th-last transaction\n",
            "add FLAM1 identical index between current and 15th-last transaction\n",
            "add FLAM1 identical index between current and 16th-last transaction\n",
            "add FLAM1 identical index between current and 17th-last transaction\n",
            "add FLAM1 identical index between current and 18th-last transaction\n",
            "add FLAM1 identical index between current and 19th-last transaction\n",
            "add FLAM1 identical index between current and 20th-last transaction\n",
            "add WEEKDAY identical index between current and 1th-last transaction\n",
            "add WEEKDAY identical index between current and 2th-last transaction\n",
            "add WEEKDAY identical index between current and 3th-last transaction\n",
            "add WEEKDAY identical index between current and 4th-last transaction\n",
            "add WEEKDAY identical index between current and 5th-last transaction\n",
            "add WEEKDAY identical index between current and 6th-last transaction\n",
            "add WEEKDAY identical index between current and 7th-last transaction\n",
            "add WEEKDAY identical index between current and 8th-last transaction\n",
            "add WEEKDAY identical index between current and 9th-last transaction\n",
            "add WEEKDAY identical index between current and 10th-last transaction\n",
            "add WEEKDAY identical index between current and 11th-last transaction\n",
            "add WEEKDAY identical index between current and 12th-last transaction\n",
            "add WEEKDAY identical index between current and 13th-last transaction\n",
            "add WEEKDAY identical index between current and 14th-last transaction\n",
            "add WEEKDAY identical index between current and 15th-last transaction\n",
            "add WEEKDAY identical index between current and 16th-last transaction\n",
            "add WEEKDAY identical index between current and 17th-last transaction\n",
            "add WEEKDAY identical index between current and 18th-last transaction\n",
            "add WEEKDAY identical index between current and 19th-last transaction\n",
            "add WEEKDAY identical index between current and 20th-last transaction\n",
            "add ECFG identical index between current and 1th-last transaction\n",
            "add ECFG identical index between current and 2th-last transaction\n",
            "add ECFG identical index between current and 3th-last transaction\n",
            "add ECFG identical index between current and 4th-last transaction\n",
            "add ECFG identical index between current and 5th-last transaction\n",
            "add PAY_TYPE identical index between current and 1th-last transaction\n",
            "add PAY_TYPE identical index between current and 2th-last transaction\n",
            "add PAY_TYPE identical index between current and 3th-last transaction\n",
            "add PAY_TYPE identical index between current and 4th-last transaction\n",
            "add PAY_TYPE identical index between current and 5th-last transaction\n",
            "add CONTP identical index between current and 1th-last transaction\n",
            "add CONTP identical index between current and 2th-last transaction\n",
            "add CONTP identical index between current and 3th-last transaction\n",
            "add CONTP identical index between current and 4th-last transaction\n",
            "add CONTP identical index between current and 5th-last transaction\n",
            "add ETYMD identical index between current and 1th-last transaction\n",
            "add ETYMD identical index between current and 2th-last transaction\n",
            "add ETYMD identical index between current and 3th-last transaction\n",
            "add ETYMD identical index between current and 4th-last transaction\n",
            "add ETYMD identical index between current and 5th-last transaction\n",
            "add STOCN identical index between current and 1th-last transaction\n",
            "add STOCN identical index between current and 2th-last transaction\n",
            "add STOCN identical index between current and 3th-last transaction\n",
            "add STOCN identical index between current and 4th-last transaction\n",
            "add STOCN identical index between current and 5th-last transaction\n",
            "add SCITY identical index between current and 1th-last transaction\n",
            "add SCITY identical index between current and 2th-last transaction\n",
            "add SCITY identical index between current and 3th-last transaction\n",
            "add SCITY identical index between current and 4th-last transaction\n",
            "add SCITY identical index between current and 5th-last transaction\n",
            "add APPFG identical index between current and 1th-last transaction\n",
            "add APPFG identical index between current and 2th-last transaction\n",
            "add APPFG identical index between current and 3th-last transaction\n",
            "add APPFG identical index between current and 4th-last transaction\n",
            "add APPFG identical index between current and 5th-last transaction\n",
            "add MCC identical index between current and 1th-last transaction\n",
            "add MCC identical index between current and 2th-last transaction\n",
            "add MCC identical index between current and 3th-last transaction\n",
            "add MCC identical index between current and 4th-last transaction\n",
            "add MCC identical index between current and 5th-last transaction\n",
            "add MCHNO identical index between current and 1th-last transaction\n",
            "add MCHNO identical index between current and 2th-last transaction\n",
            "add MCHNO identical index between current and 3th-last transaction\n",
            "add MCHNO identical index between current and 4th-last transaction\n",
            "add MCHNO identical index between current and 5th-last transaction\n",
            "add FALLBACK_IND identical index between current and 1th-last transaction\n",
            "add FALLBACK_IND identical index between current and 2th-last transaction\n",
            "add FALLBACK_IND identical index between current and 3th-last transaction\n",
            "add FALLBACK_IND identical index between current and 4th-last transaction\n",
            "add FALLBACK_IND identical index between current and 5th-last transaction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qZnFmvrsmwm"
      },
      "source": [
        "## training parameter tuning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR6hUjjh9mvS"
      },
      "source": [
        "def model_train_and_evaluate(tmp_data, sampling_rate, n_estimators):\n",
        "  resampled_train_data = resample(tmp_data, \n",
        "      sampling_rate=sampling_rate, sample_type='upsample')\n",
        "  X, y = create_X_y(resampled_train_data, \n",
        "    drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"]))\n",
        "  )\n",
        "  val_percentage = 0.33\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "    test_size=val_percentage, shuffle=True, random_state=42)\n",
        "  clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "    max_depth = 8, learning_rate = 0.05, n_estimators = n_estimators)\n",
        "  evaluate(clf, x_test, y_test)\n",
        "  return clf"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUkt6yBDsmJK"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "for sampling_rate in [0.05]:\n",
        "  for n_estimators in [800, 1600]:\n",
        "    print(sampling_rate, n_estimators)\n",
        "    model_train_and_evaluate(tmp_data, sampling_rate, n_estimators)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DDVZcsF9ffP"
      },
      "source": [
        "## Model Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyGQA7Xq9BPG",
        "outputId": "3b4339d6-f3bc-4b02-fa5b-ae3c7e5e5bcd"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "clf = model_train_and_evaluate(tmp_data, 0.1, 800)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.104635\tvalid_1's binary_logloss: 0.10486\n",
            "[100]\ttraining's binary_logloss: 0.0583369\tvalid_1's binary_logloss: 0.0589735\n",
            "[150]\ttraining's binary_logloss: 0.046379\tvalid_1's binary_logloss: 0.0470974\n",
            "[200]\ttraining's binary_logloss: 0.0407136\tvalid_1's binary_logloss: 0.0415769\n",
            "[250]\ttraining's binary_logloss: 0.0364612\tvalid_1's binary_logloss: 0.0374831\n",
            "[300]\ttraining's binary_logloss: 0.033114\tvalid_1's binary_logloss: 0.0342815\n",
            "[350]\ttraining's binary_logloss: 0.0299493\tvalid_1's binary_logloss: 0.0312621\n",
            "[400]\ttraining's binary_logloss: 0.0275395\tvalid_1's binary_logloss: 0.028957\n",
            "[450]\ttraining's binary_logloss: 0.0250833\tvalid_1's binary_logloss: 0.0266414\n",
            "[500]\ttraining's binary_logloss: 0.023127\tvalid_1's binary_logloss: 0.0248117\n",
            "[550]\ttraining's binary_logloss: 0.0214406\tvalid_1's binary_logloss: 0.023288\n",
            "[600]\ttraining's binary_logloss: 0.0197817\tvalid_1's binary_logloss: 0.0217171\n",
            "[650]\ttraining's binary_logloss: 0.0182912\tvalid_1's binary_logloss: 0.0203639\n",
            "[700]\ttraining's binary_logloss: 0.0169265\tvalid_1's binary_logloss: 0.0191103\n",
            "[750]\ttraining's binary_logloss: 0.0157819\tvalid_1's binary_logloss: 0.0180428\n",
            "[800]\ttraining's binary_logloss: 0.0146767\tvalid_1's binary_logloss: 0.0170486\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[800]\ttraining's binary_logloss: 0.0146767\tvalid_1's binary_logloss: 0.0170486\n",
            "Recall Score: 0.9977\n",
            "Precision Score: 0.9953\n",
            "F1 Score: 0.9965\n",
            "Threshold:  0.577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5ktH5T4DgIh"
      },
      "source": [
        "## Generate Testing Result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKVVDCS0J663"
      },
      "source": [
        "'''for threshold in [0.9991, 0.9993, 0.9995, 0.9997]: # 0.5, 0.7, 0.9, 0.99, 0.999\n",
        "  y_result = (y_pred > threshold).astype(int).T\n",
        "  result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "  result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "  result_table.set_index('TXKEY')\n",
        "  imb_ratio = result_table['FRAUD_IND'].mean()\n",
        "  print(\"imbalance rate of test data:\", imb_ratio)\n",
        "  file_name = 'tmp_submission_th_'+str(threshold)+\"_imr_\"+str(imb_ratio)+'.csv'\n",
        "  result_table.to_csv(file_name, mode = 'w', index= False)\n",
        "  print(file_name+ ' saved.')'''\n",
        "def save_submition_file_with_optimal_threshold(y_pred, imb_ratio=0.006):\n",
        "  def calculate_threshold_from_imb_ratio(imb_ratio):\n",
        "    sorted_y_pred = copy.copy(y_pred)\n",
        "    sorted_y_pred.sort()\n",
        "    sorted_y_pred = sorted_y_pred[::-1]\n",
        "    threshold = sorted_y_pred[int(len(y_pred) * imb_ratio)]\n",
        "    return threshold\n",
        "  threshold = calculate_threshold_from_imb_ratio(imb_ratio)\n",
        "  y_result = (y_pred > threshold).astype(int).T\n",
        "  result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "  result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "  result_table.set_index('TXKEY')\n",
        "  imb_ratio = result_table['FRAUD_IND'].mean()\n",
        "  print(\"imbalance rate of test data:\", imb_ratio)\n",
        "  file_name = 'tmp_submission_th_'+str(threshold)+\"_imr_\"+str(imb_ratio)+'.csv'\n",
        "  result_table.to_csv(file_name, mode = 'w', index= False)\n",
        "  print(file_name+ ' saved.')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3k1GKK_DhxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d9433b6-f938-42d7-c591-632133203f53"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/test.csv')\n",
        "#查看資料筆數\n",
        "print(\"shape of test data:\" , test_data.shape)\n",
        "tmp_data = overall_preprocessing(test_data)\n",
        "X = create_X(tmp_data, \n",
        "  drop_list = list(set([\"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"]\n",
        "  ))\n",
        ")\n",
        "y_pred = clf.predict(X)\n",
        "save_submition_file_with_optimal_threshold(y_pred, imb_ratio=0.006)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "shape of test data: (472335, 58)\n",
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n",
            "add FLAM1 identical index between current and 1th-last transaction\n",
            "add FLAM1 identical index between current and 2th-last transaction\n",
            "add FLAM1 identical index between current and 3th-last transaction\n",
            "add FLAM1 identical index between current and 4th-last transaction\n",
            "add FLAM1 identical index between current and 5th-last transaction\n",
            "add FLAM1 identical index between current and 6th-last transaction\n",
            "add FLAM1 identical index between current and 7th-last transaction\n",
            "add FLAM1 identical index between current and 8th-last transaction\n",
            "add FLAM1 identical index between current and 9th-last transaction\n",
            "add FLAM1 identical index between current and 10th-last transaction\n",
            "add FLAM1 identical index between current and 11th-last transaction\n",
            "add FLAM1 identical index between current and 12th-last transaction\n",
            "add FLAM1 identical index between current and 13th-last transaction\n",
            "add FLAM1 identical index between current and 14th-last transaction\n",
            "add FLAM1 identical index between current and 15th-last transaction\n",
            "add FLAM1 identical index between current and 16th-last transaction\n",
            "add FLAM1 identical index between current and 17th-last transaction\n",
            "add FLAM1 identical index between current and 18th-last transaction\n",
            "add FLAM1 identical index between current and 19th-last transaction\n",
            "add FLAM1 identical index between current and 20th-last transaction\n",
            "add WEEKDAY identical index between current and 1th-last transaction\n",
            "add WEEKDAY identical index between current and 2th-last transaction\n",
            "add WEEKDAY identical index between current and 3th-last transaction\n",
            "add WEEKDAY identical index between current and 4th-last transaction\n",
            "add WEEKDAY identical index between current and 5th-last transaction\n",
            "add WEEKDAY identical index between current and 6th-last transaction\n",
            "add WEEKDAY identical index between current and 7th-last transaction\n",
            "add WEEKDAY identical index between current and 8th-last transaction\n",
            "add WEEKDAY identical index between current and 9th-last transaction\n",
            "add WEEKDAY identical index between current and 10th-last transaction\n",
            "add WEEKDAY identical index between current and 11th-last transaction\n",
            "add WEEKDAY identical index between current and 12th-last transaction\n",
            "add WEEKDAY identical index between current and 13th-last transaction\n",
            "add WEEKDAY identical index between current and 14th-last transaction\n",
            "add WEEKDAY identical index between current and 15th-last transaction\n",
            "add WEEKDAY identical index between current and 16th-last transaction\n",
            "add WEEKDAY identical index between current and 17th-last transaction\n",
            "add WEEKDAY identical index between current and 18th-last transaction\n",
            "add WEEKDAY identical index between current and 19th-last transaction\n",
            "add WEEKDAY identical index between current and 20th-last transaction\n",
            "add ECFG identical index between current and 1th-last transaction\n",
            "add ECFG identical index between current and 2th-last transaction\n",
            "add ECFG identical index between current and 3th-last transaction\n",
            "add ECFG identical index between current and 4th-last transaction\n",
            "add ECFG identical index between current and 5th-last transaction\n",
            "add PAY_TYPE identical index between current and 1th-last transaction\n",
            "add PAY_TYPE identical index between current and 2th-last transaction\n",
            "add PAY_TYPE identical index between current and 3th-last transaction\n",
            "add PAY_TYPE identical index between current and 4th-last transaction\n",
            "add PAY_TYPE identical index between current and 5th-last transaction\n",
            "add CONTP identical index between current and 1th-last transaction\n",
            "add CONTP identical index between current and 2th-last transaction\n",
            "add CONTP identical index between current and 3th-last transaction\n",
            "add CONTP identical index between current and 4th-last transaction\n",
            "add CONTP identical index between current and 5th-last transaction\n",
            "add ETYMD identical index between current and 1th-last transaction\n",
            "add ETYMD identical index between current and 2th-last transaction\n",
            "add ETYMD identical index between current and 3th-last transaction\n",
            "add ETYMD identical index between current and 4th-last transaction\n",
            "add ETYMD identical index between current and 5th-last transaction\n",
            "add STOCN identical index between current and 1th-last transaction\n",
            "add STOCN identical index between current and 2th-last transaction\n",
            "add STOCN identical index between current and 3th-last transaction\n",
            "add STOCN identical index between current and 4th-last transaction\n",
            "add STOCN identical index between current and 5th-last transaction\n",
            "add SCITY identical index between current and 1th-last transaction\n",
            "add SCITY identical index between current and 2th-last transaction\n",
            "add SCITY identical index between current and 3th-last transaction\n",
            "add SCITY identical index between current and 4th-last transaction\n",
            "add SCITY identical index between current and 5th-last transaction\n",
            "add APPFG identical index between current and 1th-last transaction\n",
            "add APPFG identical index between current and 2th-last transaction\n",
            "add APPFG identical index between current and 3th-last transaction\n",
            "add APPFG identical index between current and 4th-last transaction\n",
            "add APPFG identical index between current and 5th-last transaction\n",
            "add MCC identical index between current and 1th-last transaction\n",
            "add MCC identical index between current and 2th-last transaction\n",
            "add MCC identical index between current and 3th-last transaction\n",
            "add MCC identical index between current and 4th-last transaction\n",
            "add MCC identical index between current and 5th-last transaction\n",
            "add MCHNO identical index between current and 1th-last transaction\n",
            "add MCHNO identical index between current and 2th-last transaction\n",
            "add MCHNO identical index between current and 3th-last transaction\n",
            "add MCHNO identical index between current and 4th-last transaction\n",
            "add MCHNO identical index between current and 5th-last transaction\n",
            "add FALLBACK_IND identical index between current and 1th-last transaction\n",
            "add FALLBACK_IND identical index between current and 2th-last transaction\n",
            "add FALLBACK_IND identical index between current and 3th-last transaction\n",
            "add FALLBACK_IND identical index between current and 4th-last transaction\n",
            "add FALLBACK_IND identical index between current and 5th-last transaction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Js2fer-Dr8g"
      },
      "source": [
        "# Strategy 9: Add One-Hot Encoding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy4l5eDQEp6w"
      },
      "source": [
        "def expend_by_onehot_encoded_features(input_data):\n",
        "  '''\n",
        "  features_to_be_onehot_encoded = [\n",
        "    \"CONTP\", \n",
        "    \"ETYMD\", \n",
        "    #\"STOCN\", \n",
        "    \"PAY_TYPE\", \n",
        "    \"CATP1\", \n",
        "    \"CUORG\", \n",
        "    \"TSCFG\", \n",
        "    #\"EDU_CODE\", \n",
        "    #\"INCOME_RANGE_CODE\", \n",
        "    \"OCUP_CODE\", \n",
        "    \"POSITION_CODE\", \n",
        "    ]\n",
        "  '''\n",
        "  features_to_be_onehot_encoded = [\n",
        "    \"CONTP\", \n",
        "    \"ETYMD\", \n",
        "    #\"STOCN\", \n",
        "    \"PAY_TYPE\", \n",
        "    \"CATP1\", \n",
        "    \"CUORG\", \n",
        "    \"TSCFG\", \n",
        "    #\"EDU_CODE\", \n",
        "    #\"INCOME_RANGE_CODE\", \n",
        "    \"OCUP_CODE\", \n",
        "    \"POSITION_CODE\", \n",
        "    ]\n",
        "  data = copy.copy(input_data)\n",
        "  for f_name in features_to_be_onehot_encoded:\n",
        "    data[f_name] = data[f_name].fillna('NULL').astype(str)\n",
        "    classes = list(set(data[f_name]))\n",
        "    for c in classes:\n",
        "      data[f_name + \"_\" + c] = (data[f_name] == c).astype(int)\n",
        "  return data\n",
        "def overall_preprocessing(train_data):\n",
        "  '''has_null_feature_list = [\n",
        "    \"AVAILABLE_LIMIT_AMT\",\n",
        "    \"BONUS_POINTS\",\n",
        "    \"CURRENT_CASH_ADV_AMT\",\n",
        "    \"CURRENT_FEE\",\n",
        "    \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "    \"CURRENT_PURCH_AMT\",\n",
        "    \"LST_CYCLE_UNPAID_BAL\"\n",
        "    ]\n",
        "  tmp_data = extend_with_null_or_not_features(train_data, has_null_feature_list)\n",
        "  print(\"after extend_with_null_or_not_features:\",tmp_data.shape[1])'''\n",
        "  \n",
        "  tmp_data = expend_by_onehot_encoded_features(train_data)\n",
        "  print(\"after expend_by_onehot_encoded_features: \", tmp_data.shape[1])\n",
        "  '''log_scale_feature_list = [\n",
        "    'BNSPT',\n",
        "    'FLAM1',\n",
        "    'ACCT_VINTAGE',\n",
        "    'AVAILABLE_LIMIT_AMT',\n",
        "    'BONUS_POINTS',\n",
        "    'CREDIT_LIMIT_AMT',\n",
        "    'CREDIT_REVOLVING_RATE',\n",
        "    'CREDIT_USE_RATE',\n",
        "    'CURRENT_CASH_ADV_AMT',\n",
        "    'CURRENT_FEE',\n",
        "    'CURRENT_INSTALLMENT_BAL',\n",
        "    'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "    'CURRENT_PURCH_AMT',\n",
        "    'LST_CYCLE_UNPAID_BAL',\n",
        "    'REVOLVING_AMT'\n",
        "  ]\n",
        "  tmp_data = extend_with_log_scale_features(tmp_data, log_scale_feature_list)\n",
        "  print(\"after extend_with_log_scale_features: \", tmp_data.shape[1])'''\n",
        "  tmp_data = extend_with_detailed_time(tmp_data, \n",
        "    weekday = True, hour = True)\n",
        "  print(\"after extend_with_detailed_time: \", tmp_data.shape[1])\n",
        "  tmp_data = extend_with_time_difference_features(tmp_data, \n",
        "    max_time_shift = 10, pivot_feature = 'CHID')\n",
        "  print(\"after extend_with_time_difference_features: \", tmp_data.shape[1])\n",
        "  tmp_data = extend_with_same_FLAM1(tmp_data, \n",
        "    max_time_shift = 10, pivot_feature = 'CHID')\n",
        "  print(\"after extend_with_same_FLAM1: \", tmp_data.shape[1])\n",
        "  '''tmp_data = extend_with_strang_weekday_transaction_change(tmp_data, \n",
        "    max_time_shift = 10, pivot_feature = 'CHID')\n",
        "  print(\"after extend_with_strang_weekday_transaction_change: \", tmp_data.shape[1])'''\n",
        "  for class_name in ['ECFG', 'PAY_TYPE', 'CONTP', 'ETYMD', 'STOCN', 'SCITY', 'APPFG', 'MCC', 'MCHNO', 'FALLBACK_IND']:\n",
        "    tmp_data = extend_with_same_class_between_transactions(tmp_data, class_name,\n",
        "      max_time_shift = 3, pivot_feature = 'CHID')\n",
        "  print(\"after extend_with_same_class_between_transactions: \", tmp_data.shape[1])\n",
        "  tmp_data = preprocessing(tmp_data)\n",
        "  print(\"after preprocessing: \", tmp_data.shape[1])\n",
        "  return tmp_data"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fn1PTqwKj_l",
        "outputId": "9e4cd654-ce5b-4bb8-c623-11b3c6eccd95"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUc1ZlGjKxMM"
      },
      "source": [
        "## Model Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ_rpYEPKv-r"
      },
      "source": [
        "def model_train_and_evaluate(tmp_data, sampling_rate, n_estimators):\n",
        "  resampled_train_data = resample(tmp_data, \n",
        "      sampling_rate=sampling_rate, sample_type='upsample')\n",
        "  X, y = create_X_y(resampled_train_data, \n",
        "    drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"]))\n",
        "  )\n",
        "  val_percentage = 0.33\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "    test_size=val_percentage, shuffle=True, random_state=42)\n",
        "  clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "    max_depth = 8, learning_rate = 0.05, n_estimators = n_estimators)\n",
        "  evaluate(clf, x_test, y_test)\n",
        "  return clf"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60mtxnLWK2zD",
        "outputId": "d26df994-a665-4eed-986a-c16411ee8ff3"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "clf = model_train_and_evaluate(\n",
        "    overall_preprocessing(\n",
        "        pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "        ), 0.1, 800)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after expend_by_onehot_encoded_features:  149\n",
            "after extend_with_detailed_time:  151\n",
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "after extend_with_time_difference_features:  161\n",
            "add FLAM1 identical index between current and 1th-last transaction\n",
            "add FLAM1 identical index between current and 2th-last transaction\n",
            "add FLAM1 identical index between current and 3th-last transaction\n",
            "add FLAM1 identical index between current and 4th-last transaction\n",
            "add FLAM1 identical index between current and 5th-last transaction\n",
            "add FLAM1 identical index between current and 6th-last transaction\n",
            "add FLAM1 identical index between current and 7th-last transaction\n",
            "add FLAM1 identical index between current and 8th-last transaction\n",
            "add FLAM1 identical index between current and 9th-last transaction\n",
            "add FLAM1 identical index between current and 10th-last transaction\n",
            "after extend_with_same_FLAM1:  171\n",
            "add ECFG identical index between current and 1th-last transaction\n",
            "add ECFG identical index between current and 2th-last transaction\n",
            "add ECFG identical index between current and 3th-last transaction\n",
            "add PAY_TYPE identical index between current and 1th-last transaction\n",
            "add PAY_TYPE identical index between current and 2th-last transaction\n",
            "add PAY_TYPE identical index between current and 3th-last transaction\n",
            "add CONTP identical index between current and 1th-last transaction\n",
            "add CONTP identical index between current and 2th-last transaction\n",
            "add CONTP identical index between current and 3th-last transaction\n",
            "add ETYMD identical index between current and 1th-last transaction\n",
            "add ETYMD identical index between current and 2th-last transaction\n",
            "add ETYMD identical index between current and 3th-last transaction\n",
            "add STOCN identical index between current and 1th-last transaction\n",
            "add STOCN identical index between current and 2th-last transaction\n",
            "add STOCN identical index between current and 3th-last transaction\n",
            "add SCITY identical index between current and 1th-last transaction\n",
            "add SCITY identical index between current and 2th-last transaction\n",
            "add SCITY identical index between current and 3th-last transaction\n",
            "add APPFG identical index between current and 1th-last transaction\n",
            "add APPFG identical index between current and 2th-last transaction\n",
            "add APPFG identical index between current and 3th-last transaction\n",
            "add MCC identical index between current and 1th-last transaction\n",
            "add MCC identical index between current and 2th-last transaction\n",
            "add MCC identical index between current and 3th-last transaction\n",
            "add MCHNO identical index between current and 1th-last transaction\n",
            "add MCHNO identical index between current and 2th-last transaction\n",
            "add MCHNO identical index between current and 3th-last transaction\n",
            "add FALLBACK_IND identical index between current and 1th-last transaction\n",
            "add FALLBACK_IND identical index between current and 2th-last transaction\n",
            "add FALLBACK_IND identical index between current and 3th-last transaction\n",
            "after extend_with_same_class_between_transactions:  201\n",
            "after preprocessing:  201\n",
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.110539\tvalid_1's binary_logloss: 0.110638\n",
            "[100]\ttraining's binary_logloss: 0.0649432\tvalid_1's binary_logloss: 0.065265\n",
            "[150]\ttraining's binary_logloss: 0.0533006\tvalid_1's binary_logloss: 0.0538425\n",
            "[200]\ttraining's binary_logloss: 0.0466764\tvalid_1's binary_logloss: 0.0474547\n",
            "[250]\ttraining's binary_logloss: 0.0416782\tvalid_1's binary_logloss: 0.0427107\n",
            "[300]\ttraining's binary_logloss: 0.0376223\tvalid_1's binary_logloss: 0.0387404\n",
            "[350]\ttraining's binary_logloss: 0.0338569\tvalid_1's binary_logloss: 0.0350637\n",
            "[400]\ttraining's binary_logloss: 0.0312918\tvalid_1's binary_logloss: 0.0326238\n",
            "[450]\ttraining's binary_logloss: 0.0289628\tvalid_1's binary_logloss: 0.0304487\n",
            "[500]\ttraining's binary_logloss: 0.0268937\tvalid_1's binary_logloss: 0.0285336\n",
            "[550]\ttraining's binary_logloss: 0.0248615\tvalid_1's binary_logloss: 0.0266157\n",
            "[600]\ttraining's binary_logloss: 0.0233373\tvalid_1's binary_logloss: 0.0251955\n",
            "[650]\ttraining's binary_logloss: 0.0215575\tvalid_1's binary_logloss: 0.0235514\n",
            "[700]\ttraining's binary_logloss: 0.0200572\tvalid_1's binary_logloss: 0.0221446\n",
            "[750]\ttraining's binary_logloss: 0.0186801\tvalid_1's binary_logloss: 0.0208882\n",
            "[800]\ttraining's binary_logloss: 0.0171979\tvalid_1's binary_logloss: 0.0195034\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[800]\ttraining's binary_logloss: 0.0171979\tvalid_1's binary_logloss: 0.0195034\n",
            "Recall Score: 0.9978\n",
            "Precision Score: 0.9943\n",
            "F1 Score: 0.996\n",
            "Threshold:  0.5449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiUcxslkQxPI"
      },
      "source": [
        "## Generate Testing Result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KJ_GPT1LKgW"
      },
      "source": [
        "'''for threshold in [0.9991, 0.9993, 0.9995, 0.9997]: # 0.5, 0.7, 0.9, 0.99, 0.999\n",
        "  y_result = (y_pred > threshold).astype(int).T\n",
        "  result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "  result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "  result_table.set_index('TXKEY')\n",
        "  imb_ratio = result_table['FRAUD_IND'].mean()\n",
        "  print(\"imbalance rate of test data:\", imb_ratio)\n",
        "  file_name = 'tmp_submission_th_'+str(threshold)+\"_imr_\"+str(imb_ratio)+'.csv'\n",
        "  result_table.to_csv(file_name, mode = 'w', index= False)\n",
        "  print(file_name+ ' saved.')'''\n",
        "def save_submition_file_with_optimal_threshold(test_data, y_pred, imb_ratio=0.006, new_parameter = None):\n",
        "  def calculate_threshold_from_imb_ratio(imb_ratio):\n",
        "    sorted_y_pred = copy.copy(y_pred)\n",
        "    sorted_y_pred.sort()\n",
        "    sorted_y_pred = sorted_y_pred[::-1]\n",
        "    threshold = sorted_y_pred[int(len(y_pred) * imb_ratio)]\n",
        "    return threshold\n",
        "  threshold = calculate_threshold_from_imb_ratio(imb_ratio)\n",
        "  y_result = (y_pred > threshold).astype(int).T\n",
        "  result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "  result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "  result_table.set_index('TXKEY')\n",
        "  imb_ratio = result_table['FRAUD_IND'].mean()\n",
        "  print(\"imbalance rate of test data:\", imb_ratio)\n",
        "  if new_parameter:\n",
        "    file_name = str(new_parameter) + 'tmp_submission_th_'+str(threshold)+\"_imr_\"+str(imb_ratio)+'.csv'\n",
        "  else:\n",
        "    file_name = 'tmp_submission_th_'+str(threshold)+\"_imr_\"+str(imb_ratio)+'.csv'\n",
        "  result_table.to_csv(file_name, mode = 'w', index= False)\n",
        "  print(file_name+ ' saved.')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKZ8rMT9LHBD",
        "outputId": "85293c36-e3c4-4b51-a034-3a9d2b2ada38"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/test.csv')\n",
        "y_pred = clf.predict(\n",
        "    create_X(\n",
        "        overall_preprocessing(\n",
        "          test_data\n",
        "        ), \n",
        "        drop_list = list(set([\"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"]))\n",
        "    )\n",
        "  )\n",
        "save_submition_file_with_optimal_threshold(test_data, y_pred, imb_ratio=0.006)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "after expend_by_onehot_encoded_features:  148\n",
            "after extend_with_detailed_time:  150\n",
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "after extend_with_time_difference_features:  160\n",
            "add FLAM1 identical index between current and 1th-last transaction\n",
            "add FLAM1 identical index between current and 2th-last transaction\n",
            "add FLAM1 identical index between current and 3th-last transaction\n",
            "add FLAM1 identical index between current and 4th-last transaction\n",
            "add FLAM1 identical index between current and 5th-last transaction\n",
            "add FLAM1 identical index between current and 6th-last transaction\n",
            "add FLAM1 identical index between current and 7th-last transaction\n",
            "add FLAM1 identical index between current and 8th-last transaction\n",
            "add FLAM1 identical index between current and 9th-last transaction\n",
            "add FLAM1 identical index between current and 10th-last transaction\n",
            "after extend_with_same_FLAM1:  170\n",
            "add ECFG identical index between current and 1th-last transaction\n",
            "add ECFG identical index between current and 2th-last transaction\n",
            "add ECFG identical index between current and 3th-last transaction\n",
            "add PAY_TYPE identical index between current and 1th-last transaction\n",
            "add PAY_TYPE identical index between current and 2th-last transaction\n",
            "add PAY_TYPE identical index between current and 3th-last transaction\n",
            "add CONTP identical index between current and 1th-last transaction\n",
            "add CONTP identical index between current and 2th-last transaction\n",
            "add CONTP identical index between current and 3th-last transaction\n",
            "add ETYMD identical index between current and 1th-last transaction\n",
            "add ETYMD identical index between current and 2th-last transaction\n",
            "add ETYMD identical index between current and 3th-last transaction\n",
            "add STOCN identical index between current and 1th-last transaction\n",
            "add STOCN identical index between current and 2th-last transaction\n",
            "add STOCN identical index between current and 3th-last transaction\n",
            "add SCITY identical index between current and 1th-last transaction\n",
            "add SCITY identical index between current and 2th-last transaction\n",
            "add SCITY identical index between current and 3th-last transaction\n",
            "add APPFG identical index between current and 1th-last transaction\n",
            "add APPFG identical index between current and 2th-last transaction\n",
            "add APPFG identical index between current and 3th-last transaction\n",
            "add MCC identical index between current and 1th-last transaction\n",
            "add MCC identical index between current and 2th-last transaction\n",
            "add MCC identical index between current and 3th-last transaction\n",
            "add MCHNO identical index between current and 1th-last transaction\n",
            "add MCHNO identical index between current and 2th-last transaction\n",
            "add MCHNO identical index between current and 3th-last transaction\n",
            "add FALLBACK_IND identical index between current and 1th-last transaction\n",
            "add FALLBACK_IND identical index between current and 2th-last transaction\n",
            "add FALLBACK_IND identical index between current and 3th-last transaction\n",
            "after extend_with_same_class_between_transactions:  200\n",
            "after preprocessing:  200\n",
            "imbalance rate of test data: 0.005999978828585643\n",
            "tmp_submission_th_0.9991637777169805_imr_0.005999978828585643.csv saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhOdgSn8Tj5F"
      },
      "source": [
        "# Strategy 10: Increase n_estimate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db65vD-fPdnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de6de35-41c3-45f2-a07e-7a45edab70ee"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "for n_estimate in [1000, 1600, 2000, 3200]:\n",
        "  clf = model_train_and_evaluate(\n",
        "      overall_preprocessing(\n",
        "          pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "          ), 0.1, n_estimate)\n",
        "  test_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/test.csv')\n",
        "  y_pred = clf.predict(\n",
        "      create_X(\n",
        "          overall_preprocessing(\n",
        "            test_data\n",
        "          ), \n",
        "          drop_list = list(set([\"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"]))\n",
        "      )\n",
        "    )\n",
        "  save_submition_file_with_optimal_threshold(test_data, y_pred, imb_ratio=0.006, new_parameter=n_estimate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "after expend_by_onehot_encoded_features:  149\n",
            "after extend_with_detailed_time:  151\n",
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "after extend_with_time_difference_features:  161\n",
            "add FLAM1 identical index between current and 1th-last transaction\n",
            "add FLAM1 identical index between current and 2th-last transaction\n",
            "add FLAM1 identical index between current and 3th-last transaction\n",
            "add FLAM1 identical index between current and 4th-last transaction\n",
            "add FLAM1 identical index between current and 5th-last transaction\n",
            "add FLAM1 identical index between current and 6th-last transaction\n",
            "add FLAM1 identical index between current and 7th-last transaction\n",
            "add FLAM1 identical index between current and 8th-last transaction\n",
            "add FLAM1 identical index between current and 9th-last transaction\n",
            "add FLAM1 identical index between current and 10th-last transaction\n",
            "after extend_with_same_FLAM1:  171\n",
            "add ECFG identical index between current and 1th-last transaction\n",
            "add ECFG identical index between current and 2th-last transaction\n",
            "add ECFG identical index between current and 3th-last transaction\n",
            "add PAY_TYPE identical index between current and 1th-last transaction\n",
            "add PAY_TYPE identical index between current and 2th-last transaction\n",
            "add PAY_TYPE identical index between current and 3th-last transaction\n",
            "add CONTP identical index between current and 1th-last transaction\n",
            "add CONTP identical index between current and 2th-last transaction\n",
            "add CONTP identical index between current and 3th-last transaction\n",
            "add ETYMD identical index between current and 1th-last transaction\n",
            "add ETYMD identical index between current and 2th-last transaction\n",
            "add ETYMD identical index between current and 3th-last transaction\n",
            "add STOCN identical index between current and 1th-last transaction\n",
            "add STOCN identical index between current and 2th-last transaction\n",
            "add STOCN identical index between current and 3th-last transaction\n",
            "add SCITY identical index between current and 1th-last transaction\n",
            "add SCITY identical index between current and 2th-last transaction\n",
            "add SCITY identical index between current and 3th-last transaction\n",
            "add APPFG identical index between current and 1th-last transaction\n",
            "add APPFG identical index between current and 2th-last transaction\n",
            "add APPFG identical index between current and 3th-last transaction\n",
            "add MCC identical index between current and 1th-last transaction\n",
            "add MCC identical index between current and 2th-last transaction\n",
            "add MCC identical index between current and 3th-last transaction\n",
            "add MCHNO identical index between current and 1th-last transaction\n",
            "add MCHNO identical index between current and 2th-last transaction\n",
            "add MCHNO identical index between current and 3th-last transaction\n",
            "add FALLBACK_IND identical index between current and 1th-last transaction\n",
            "add FALLBACK_IND identical index between current and 2th-last transaction\n",
            "add FALLBACK_IND identical index between current and 3th-last transaction\n",
            "after extend_with_same_class_between_transactions:  201\n",
            "after preprocessing:  201\n",
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.110539\tvalid_1's binary_logloss: 0.110638\n",
            "[100]\ttraining's binary_logloss: 0.0649432\tvalid_1's binary_logloss: 0.065265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fm1pPUGejX1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}