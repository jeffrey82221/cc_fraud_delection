{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FraudDetectionTrainModulized_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN7SwDveRLiMwsb0IkdqFwA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffrey82221/cc_fraud_delection/blob/main/FraudDetectionTrainModulized_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHKUXmpFyUik"
      },
      "source": [
        "# Import Packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4td8ka0yM-8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import recall_score, precision_score, precision_recall_curve\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR_-LzQbyX_b"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwtoIWvXyZzy"
      },
      "source": [
        "import copy\n",
        "############################ Preprocessing ###################################\n",
        "def extend_with_detailed_time(data, weekday = True, hour = True):\n",
        "  '''\n",
        "  Add WEEKDAY and HOUR and convert DATETIME into strptime format. \n",
        "  '''\n",
        "  c_data = copy.copy(data)\n",
        "  c_data[\"DATETIME\"] = c_data[\"DATETIME\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
        "  if weekday:\n",
        "    c_data[\"WEEKDAY\"] = c_data[\"DATETIME\"].apply(lambda x: x.weekday() + 1)\n",
        "  if hour:\n",
        "    c_data[\"HOUR\"] = c_data[\"DATETIME\"].apply(lambda x: x.hour + 1)\n",
        "  return c_data \n",
        "\n",
        "def extend_with_time_difference_features(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def date_diff(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])[\"DATETIME\"].shift(time_shift)\n",
        "    name = pivot_feature + '_DIF' + str(time_shift)\n",
        "    df[name] = (df[\"DATETIME\"] - df['shift']).dt.total_seconds().fillna(0)\n",
        "    # \n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add time difference between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = date_diff(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "\n",
        "def preprocess_null_values(data):\n",
        "  # 將空值填補\n",
        "  c_data = copy.copy(data)\n",
        "  c_data[\n",
        "        c_data.select_dtypes(include=['object']).columns\n",
        "      ] = c_data[\n",
        "        c_data.select_dtypes(include=['object']).columns\n",
        "      ].fillna(\"NULL\")\n",
        "  c_data[\n",
        "      c_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    ] = c_data[\n",
        "      c_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    ].fillna(-1)\n",
        "  return c_data\n",
        "\n",
        "\n",
        "def encode_labels(data):\n",
        "  #將object欄位使用Label Encoder\n",
        "  c_data = copy.copy(data)\n",
        "  labelencoder = LabelEncoder()\n",
        "  obj_col = c_data.select_dtypes(include=['object']).columns.to_list()\n",
        "  for col in obj_col:\n",
        "      c_data[col] = labelencoder.fit_transform(c_data[col])\n",
        "  return c_data\n",
        "def preprocessing(data):\n",
        "  r_data = preprocess_null_values(data)\n",
        "  return encode_labels(r_data)\n",
        "############################ Training Preprocess ############################\n",
        "def resample(data, sampling_rate=0.7, sample_type='downsample'):\n",
        "  # note that testing data should not be re-sampled. \n",
        "  assert sample_type == 'downsample' or sample_type == 'upsample'\n",
        "  c_data = copy.copy(data) \n",
        "  #將資料切分為train&test\n",
        "  if sample_type == 'downsample': \n",
        "    df_fraud = c_data[c_data[\"FRAUD_IND\"] == 1]\n",
        "    df_not_fraud = c_data[c_data[\"FRAUD_IND\"] != 1].sample(frac=sampling_rate, random_state=42)\n",
        "  elif sample_type == 'upsample':\n",
        "    df_fraud = c_data[c_data[\"FRAUD_IND\"] == 1].sample(frac=1./sampling_rate, replace = True, random_state=42)\n",
        "    df_not_fraud = c_data[c_data[\"FRAUD_IND\"] != 1]\n",
        "  df_train = pd.concat([df_fraud, df_not_fraud], 0)\n",
        "  return df_train\n",
        "\n",
        "def create_X(data, drop_list = []):\n",
        "  if drop_list:\n",
        "    return data.drop(drop_list, 1)\n",
        "  else:\n",
        "    return data\n",
        "\n",
        "def create_X_y(data, drop_list = ['FRAUD_IND']):\n",
        "  X = data.drop(drop_list, 1)\n",
        "  y = data[\"FRAUD_IND\"]\n",
        "  return X,y\n",
        "\n",
        "############################ Model Build ####################################\n",
        "def train_lgb(x_train, x_test, y_train, y_test, max_depth = 8, learning_rate = 0.05, n_estimators = 1000):\n",
        "  # n_estimators: number of trees \n",
        "  lgb_train = lgb.Dataset(x_train, y_train)\n",
        "  lgb_test = lgb.Dataset(x_test, y_test)\n",
        "  params = {\n",
        "      \"boosting_type\": \"gbdt\",\n",
        "      \"objective\": \"binary\",\n",
        "      \"metric\": \"binary_logloss\",\n",
        "      \"max_depth\": max_depth,\n",
        "      \"learning_rate\": learning_rate,\n",
        "      \"n_estimators\": n_estimators,\n",
        "  }\n",
        "  trained_model = lgb.train(\n",
        "      params,\n",
        "      lgb_train,\n",
        "      num_boost_round=5000,\n",
        "      valid_sets=[lgb_train, lgb_test],\n",
        "      early_stopping_rounds=30,\n",
        "      verbose_eval=50\n",
        "  )\n",
        "  return trained_model\n",
        "##### Get Result Generated from Model #####################################\n",
        "def evaluate(clf, x_test, y_test):\n",
        "  y_pred = clf.predict(x_test)\n",
        "  precision, recall, threshold = precision_recall_curve(y_test, y_pred)\n",
        "  performance = {\"precision\": precision[0:-1],\n",
        "                \"recall\": recall[0:-1],\n",
        "                \"threshold\": threshold\n",
        "                }\n",
        "  performance[\"f1\"] = 2 * (performance[\"precision\"] * performance[\"recall\"]) / (performance[\"precision\"] + performance[\"recall\"])\n",
        "  performance = pd.DataFrame(performance)\n",
        "  thr = performance[performance[\"f1\"] == max(performance[\"f1\"])][\"threshold\"].values[0]\n",
        "  recall = performance[performance[\"f1\"] == max(performance[\"f1\"])][\"recall\"].values[0]\n",
        "  precision = performance[performance[\"f1\"] == max(performance[\"f1\"])][\"precision\"].values[0]\n",
        "  print(\"Recall Score:\", recall)\n",
        "  print(\"Precision Score:\", precision)\n",
        "  print(\"F1 Score:\", 2 * (precision * recall) / (precision + recall))\n",
        "  print(\"Threshold: \", thr)\n",
        "def get_important_feature_table(clf, x_train):\n",
        "  importance = {\n",
        "  \"col\": np.array(x_train.columns),\n",
        "  \"imp\": lgb.Booster.feature_importance(clf)\n",
        "  }\n",
        "  df_imp = pd.DataFrame(importance).sort_values(by='imp', ascending=False)\n",
        "  return df_imp"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSuPU5SyykV5"
      },
      "source": [
        "# First Run (for selecting unimportant features) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "iRMDS2I9yk2B",
        "outputId": "c0a9f337-e97a-47e9-9725-4022f7cfe7bb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "# add AGE \n",
        "# remove weekday and hour \n",
        "tmp_train_data = extend_with_detailed_time(train_data, \n",
        "  weekday = False, hour = False)\n",
        "preprocessed_train_data = preprocessing(tmp_train_data)\n",
        "resampled_train_data = resample(preprocessed_train_data, \n",
        "  sampling_rate=0.7, sample_type='downsample')\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = [\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"])\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=val_percentage, \n",
        "  shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.05, n_estimators = 1000)\n",
        "evaluate(clf, x_test, y_test)\n",
        "important_feature_table = get_important_feature_table(clf, x_train)\n",
        "important_feature_table.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "shape of train data: (533202, 59)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.162201\tvalid_1's binary_logloss: 0.162768\n",
            "[100]\ttraining's binary_logloss: 0.13457\tvalid_1's binary_logloss: 0.13604\n",
            "[150]\ttraining's binary_logloss: 0.12292\tvalid_1's binary_logloss: 0.125427\n",
            "[200]\ttraining's binary_logloss: 0.115362\tvalid_1's binary_logloss: 0.118842\n",
            "[250]\ttraining's binary_logloss: 0.109307\tvalid_1's binary_logloss: 0.113742\n",
            "[300]\ttraining's binary_logloss: 0.103638\tvalid_1's binary_logloss: 0.108953\n",
            "[350]\ttraining's binary_logloss: 0.0989313\tvalid_1's binary_logloss: 0.105065\n",
            "[400]\ttraining's binary_logloss: 0.0948469\tvalid_1's binary_logloss: 0.101827\n",
            "[450]\ttraining's binary_logloss: 0.0910325\tvalid_1's binary_logloss: 0.0986804\n",
            "[500]\ttraining's binary_logloss: 0.0874282\tvalid_1's binary_logloss: 0.0957317\n",
            "[550]\ttraining's binary_logloss: 0.0840682\tvalid_1's binary_logloss: 0.0930062\n",
            "[600]\ttraining's binary_logloss: 0.081057\tvalid_1's binary_logloss: 0.0906978\n",
            "[650]\ttraining's binary_logloss: 0.0782779\tvalid_1's binary_logloss: 0.0886361\n",
            "[700]\ttraining's binary_logloss: 0.0756893\tvalid_1's binary_logloss: 0.0866863\n",
            "[750]\ttraining's binary_logloss: 0.0733625\tvalid_1's binary_logloss: 0.084959\n",
            "[800]\ttraining's binary_logloss: 0.0712952\tvalid_1's binary_logloss: 0.0834625\n",
            "[850]\ttraining's binary_logloss: 0.0691694\tvalid_1's binary_logloss: 0.0818883\n",
            "[900]\ttraining's binary_logloss: 0.0674926\tvalid_1's binary_logloss: 0.0807932\n",
            "[950]\ttraining's binary_logloss: 0.0655437\tvalid_1's binary_logloss: 0.0793593\n",
            "[1000]\ttraining's binary_logloss: 0.0636926\tvalid_1's binary_logloss: 0.0779864\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's binary_logloss: 0.0636926\tvalid_1's binary_logloss: 0.0779864\n",
            "Recall Score: 0.9389421933232892\n",
            "Precision Score: 0.926380848067595\n",
            "F1 Score: 0.9326192257708819\n",
            "Threshold:  0.5136044918384114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col</th>\n",
              "      <th>imp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>CC_VINTAGE</td>\n",
              "      <td>2721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MCC</td>\n",
              "      <td>2437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>SCITY</td>\n",
              "      <td>1885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>FLAM1</td>\n",
              "      <td>1810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>BONUS_POINTS</td>\n",
              "      <td>1545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             col   imp\n",
              "27    CC_VINTAGE  2721\n",
              "0            MCC  2437\n",
              "10         SCITY  1885\n",
              "8          FLAM1  1810\n",
              "37  BONUS_POINTS  1545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b4ggPwyy_55"
      },
      "source": [
        "# Best Run in v1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRJTAIYTy-aj",
        "outputId": "32f3d727-8827-4aae-a137-061ff6fe0edd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "tmp_train_data = extend_with_detailed_time(train_data, \n",
        "  weekday = True, hour = True)\n",
        "train_tmp_data = extend_with_time_difference_features(tmp_train_data, \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "preprocessed_train_data = preprocessing(train_tmp_data)\n",
        "resampled_train_data = resample(preprocessed_train_data, \n",
        "  sampling_rate=0.7, sample_type='downsample')\n",
        "removed_unimportant_feature_count = 5\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "  test_size=val_percentage, shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.05, n_estimators = 1000)\n",
        "evaluate(clf, x_test, y_test)\n",
        "#important_feature_table = get_important_feature_table(clf, x_train)\n",
        "#important_feature_table"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "shape of train data: (533202, 59)\n",
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.083666\tvalid_1's binary_logloss: 0.0839237\n",
            "[100]\ttraining's binary_logloss: 0.0522189\tvalid_1's binary_logloss: 0.0540179\n",
            "[150]\ttraining's binary_logloss: 0.0434371\tvalid_1's binary_logloss: 0.0461507\n",
            "[200]\ttraining's binary_logloss: 0.0390603\tvalid_1's binary_logloss: 0.0424071\n",
            "[250]\ttraining's binary_logloss: 0.0351947\tvalid_1's binary_logloss: 0.0391872\n",
            "[300]\ttraining's binary_logloss: 0.0323212\tvalid_1's binary_logloss: 0.036856\n",
            "[350]\ttraining's binary_logloss: 0.0296248\tvalid_1's binary_logloss: 0.0347479\n",
            "[400]\ttraining's binary_logloss: 0.027224\tvalid_1's binary_logloss: 0.0327181\n",
            "[450]\ttraining's binary_logloss: 0.0251559\tvalid_1's binary_logloss: 0.0310547\n",
            "[500]\ttraining's binary_logloss: 0.0229929\tvalid_1's binary_logloss: 0.0294841\n",
            "[550]\ttraining's binary_logloss: 0.0211482\tvalid_1's binary_logloss: 0.0281075\n",
            "[600]\ttraining's binary_logloss: 0.0193928\tvalid_1's binary_logloss: 0.0266691\n",
            "[650]\ttraining's binary_logloss: 0.0179639\tvalid_1's binary_logloss: 0.0256464\n",
            "[700]\ttraining's binary_logloss: 0.016644\tvalid_1's binary_logloss: 0.0247067\n",
            "[750]\ttraining's binary_logloss: 0.0154126\tvalid_1's binary_logloss: 0.0237928\n",
            "[800]\ttraining's binary_logloss: 0.0144018\tvalid_1's binary_logloss: 0.0230482\n",
            "[850]\ttraining's binary_logloss: 0.0131942\tvalid_1's binary_logloss: 0.0221201\n",
            "[900]\ttraining's binary_logloss: 0.012276\tvalid_1's binary_logloss: 0.021482\n",
            "[950]\ttraining's binary_logloss: 0.011519\tvalid_1's binary_logloss: 0.0209403\n",
            "[1000]\ttraining's binary_logloss: 0.0107914\tvalid_1's binary_logloss: 0.020398\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's binary_logloss: 0.0107914\tvalid_1's binary_logloss: 0.020398\n",
            "Recall Score: 0.9836254063912457\n",
            "Precision Score: 0.9821845678767964\n",
            "F1 Score: 0.982904459103425\n",
            "Threshold:  0.4981942164802951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlOxRa9dznLI"
      },
      "source": [
        "## Tuning Threshold "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKhwrsE2zUJn"
      },
      "source": [
        "y_pred = clf.predict(X)\n",
        "tolerance = 0.05\n",
        "boundary = (0., 1.)\n",
        "train_imbalance_rate = train_data['FRAUD_IND'].mean()\n",
        "print(\"imbalance rate of train data:\", train_imbalance_rate)\n",
        "threshold = 0.5\n",
        "y_result = (y_pred > threshold).astype(int).T\n",
        "result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "imbalance_rate = result_table['FRAUD_IND'].mean()\n",
        "print(\"imbalance rate of test data:\", imbalance_rate)\n",
        "while np.abs(train_imbalance_rate-imbalance_rate) >= tolerance:\n",
        "  print('threshold:', threshold)\n",
        "  y_result = (y_pred > threshold).astype(int).T\n",
        "  result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "  result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "  imbalance_rate = result_table['FRAUD_IND'].mean()\n",
        "  print(\"imbalance rate of test data:\", imbalance_rate)\n",
        "  if imbalance_rate > train_imbalance_rate:\n",
        "    threshold = (boundary[1] + threshold)/2.\n",
        "    boundary[0] = threshold\n",
        "  else:\n",
        "    threshold = (boundary[0] + threshold)/2.\n",
        "    boundary[1] = threshold\n",
        "  print(\"boundary\",boundary)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5pQDA41zz16"
      },
      "source": [
        "## Generate Result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "njT7OuG7zbxq",
        "outputId": "09d624d3-d2e3-46e4-f78f-dc0593a9c73d"
      },
      "source": [
        "y_pred = clf.predict(X)\n",
        "threshold = 0.97\n",
        "y_result = (y_pred > threshold).astype(int).T\n",
        "result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "print(\"imbalance rate of test data:\", result_table['FRAUD_IND'].mean())\n",
        "result_table.to_csv('tmp_submission.csv')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d67089bdaf96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.97\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresult_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TXKEY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mresult_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'TXKEY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FRAUD_IND'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imbalance rate of test data:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FRAUD_IND'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwyUbaRZz3xM"
      },
      "source": [
        "# Strategy 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYmGmeSN2vTg"
      },
      "source": [
        "## add log scale "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19hezKOKz3Rv"
      },
      "source": [
        "def extend_with_log_scale_features(data, log_scale_feature_list):\n",
        "  c_data = copy.copy(data)\n",
        "  for f_name in log_scale_feature_list:\n",
        "    c_data[f_name + '_LOG_SCALE'] = np.log10(data[f_name])\n",
        "  return c_data"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34Ymm5va1KjZ",
        "outputId": "1cec4092-0b6e-48e7-df92-6d44252eb10f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "\n",
        "log_scale_feature_list = [\n",
        "  'BNSPT',\n",
        "  'FLAM1',\n",
        "  'ACCT_VINTAGE',\n",
        "  'AVAILABLE_LIMIT_AMT',\n",
        "  'BONUS_POINTS',\n",
        "  'CREDIT_LIMIT_AMT',\n",
        "  'CREDIT_REVOLVING_RATE',\n",
        "  'CREDIT_USE_RATE',\n",
        "  'CURRENT_CASH_ADV_AMT',\n",
        "  'CURRENT_FEE',\n",
        "  'CURRENT_INSTALLMENT_BAL',\n",
        "  'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "  'CURRENT_PURCH_AMT',\n",
        "  'LST_CYCLE_UNPAID_BAL',\n",
        "  'REVOLVING_AMT'\n",
        "]\n",
        "tmp_train_data = extend_with_log_scale_features(train_data, log_scale_feature_list)\n",
        "tmp_train_data = extend_with_detailed_time(tmp_train_data, \n",
        "  weekday = True, hour = True)\n",
        "train_tmp_data = extend_with_time_difference_features(tmp_train_data, \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "preprocessed_train_data = preprocessing(train_tmp_data)\n",
        "resampled_train_data = resample(preprocessed_train_data, \n",
        "  sampling_rate=0.7, sample_type='downsample')\n",
        "removed_unimportant_feature_count = 5\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "  test_size=val_percentage, shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.05, n_estimators = 1000)\n",
        "evaluate(clf, x_test, y_test)\n",
        "#important_feature_table = get_important_feature_table(clf, x_train)\n",
        "#important_feature_table"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "shape of train data: (533202, 59)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.0834672\tvalid_1's binary_logloss: 0.0836714\n",
            "[100]\ttraining's binary_logloss: 0.0522038\tvalid_1's binary_logloss: 0.0537736\n",
            "[150]\ttraining's binary_logloss: 0.0434702\tvalid_1's binary_logloss: 0.0459496\n",
            "[200]\ttraining's binary_logloss: 0.0393683\tvalid_1's binary_logloss: 0.0424598\n",
            "[250]\ttraining's binary_logloss: 0.0356321\tvalid_1's binary_logloss: 0.0393477\n",
            "[300]\ttraining's binary_logloss: 0.0325958\tvalid_1's binary_logloss: 0.0368766\n",
            "[350]\ttraining's binary_logloss: 0.0299755\tvalid_1's binary_logloss: 0.0347561\n",
            "[400]\ttraining's binary_logloss: 0.0270696\tvalid_1's binary_logloss: 0.0324058\n",
            "[450]\ttraining's binary_logloss: 0.024545\tvalid_1's binary_logloss: 0.0303967\n",
            "[500]\ttraining's binary_logloss: 0.0225986\tvalid_1's binary_logloss: 0.0289356\n",
            "[550]\ttraining's binary_logloss: 0.0208548\tvalid_1's binary_logloss: 0.0276641\n",
            "[600]\ttraining's binary_logloss: 0.0191038\tvalid_1's binary_logloss: 0.0262661\n",
            "[650]\ttraining's binary_logloss: 0.0176442\tvalid_1's binary_logloss: 0.0252084\n",
            "[700]\ttraining's binary_logloss: 0.0163318\tvalid_1's binary_logloss: 0.0241666\n",
            "[750]\ttraining's binary_logloss: 0.0152681\tvalid_1's binary_logloss: 0.0233549\n",
            "[800]\ttraining's binary_logloss: 0.0140877\tvalid_1's binary_logloss: 0.0224846\n",
            "[850]\ttraining's binary_logloss: 0.0132602\tvalid_1's binary_logloss: 0.0218781\n",
            "[900]\ttraining's binary_logloss: 0.0122839\tvalid_1's binary_logloss: 0.0211579\n",
            "[950]\ttraining's binary_logloss: 0.0114488\tvalid_1's binary_logloss: 0.0205553\n",
            "[1000]\ttraining's binary_logloss: 0.0107839\tvalid_1's binary_logloss: 0.0201218\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's binary_logloss: 0.0107839\tvalid_1's binary_logloss: 0.0201218\n",
            "Recall Score: 0.9848148441836492\n",
            "Precision Score: 0.9814683104156788\n",
            "F1 Score: 0.983138729467643\n",
            "Threshold:  0.48441210277113395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bbwiiFD7h81"
      },
      "source": [
        "## check if the log-scale version features have larger importance "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp1AvMK16A4n"
      },
      "source": [
        "important_feature_table_with_log_features = get_important_feature_table(clf, x_train)\n",
        "important_feature_table_with_log_features = important_feature_table_with_log_features.set_index('col')\n",
        "for f_name in log_scale_feature_list:\n",
        "  if f_name not in important_feature_table.set_index('col').index[\n",
        "              -(removed_unimportant_feature_count):].tolist():\n",
        "    linear_importance_score = important_feature_table_with_log_features.loc[f_name]['imp']\n",
        "    log_importance_score = important_feature_table_with_log_features.loc[f_name+'_LOG_SCALE']['imp']\n",
        "    print(f_name, linear_importance_score, log_importance_score)\n",
        "    if log_importance_score > linear_importance_score:\n",
        "      print('log-scale is better')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJOEE4Iy8NFG"
      },
      "source": [
        "## remove unimportant log-scale feature and run again "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23Bfulh65k8I",
        "outputId": "947f9127-2112-489e-cc11-6d9be1ba7fdb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "\n",
        "log_scale_feature_list = [\n",
        "  'CREDIT_USE_RATE',\n",
        "  'REVOLVING_AMT'\n",
        "]\n",
        "tmp_train_data = extend_with_log_scale_features(train_data, log_scale_feature_list)\n",
        "tmp_train_data = extend_with_detailed_time(tmp_train_data, \n",
        "  weekday = True, hour = True)\n",
        "train_tmp_data = extend_with_time_difference_features(tmp_train_data, \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "preprocessed_train_data = preprocessing(train_tmp_data)\n",
        "resampled_train_data = resample(preprocessed_train_data, \n",
        "  sampling_rate=0.7, sample_type='downsample')\n",
        "removed_unimportant_feature_count = 5\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "  test_size=val_percentage, shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.05, n_estimators = 1000)\n",
        "evaluate(clf, x_test, y_test)\n",
        "#important_feature_table = get_important_feature_table(clf, x_train)\n",
        "#important_feature_table"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "shape of train data: (533202, 59)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.0838535\tvalid_1's binary_logloss: 0.0840345\n",
            "[100]\ttraining's binary_logloss: 0.0523608\tvalid_1's binary_logloss: 0.0539582\n",
            "[150]\ttraining's binary_logloss: 0.0434902\tvalid_1's binary_logloss: 0.0460947\n",
            "[200]\ttraining's binary_logloss: 0.0390921\tvalid_1's binary_logloss: 0.0422317\n",
            "[250]\ttraining's binary_logloss: 0.0355646\tvalid_1's binary_logloss: 0.0392506\n",
            "[300]\ttraining's binary_logloss: 0.0323527\tvalid_1's binary_logloss: 0.0366978\n",
            "[350]\ttraining's binary_logloss: 0.0297353\tvalid_1's binary_logloss: 0.0346748\n",
            "[400]\ttraining's binary_logloss: 0.0271154\tvalid_1's binary_logloss: 0.0325232\n",
            "[450]\ttraining's binary_logloss: 0.0248799\tvalid_1's binary_logloss: 0.0307638\n",
            "[500]\ttraining's binary_logloss: 0.0227364\tvalid_1's binary_logloss: 0.0291772\n",
            "[550]\ttraining's binary_logloss: 0.0209168\tvalid_1's binary_logloss: 0.0278375\n",
            "[600]\ttraining's binary_logloss: 0.0194334\tvalid_1's binary_logloss: 0.0267184\n",
            "[650]\ttraining's binary_logloss: 0.0180099\tvalid_1's binary_logloss: 0.0256114\n",
            "[700]\ttraining's binary_logloss: 0.0167661\tvalid_1's binary_logloss: 0.0247089\n",
            "[750]\ttraining's binary_logloss: 0.0155863\tvalid_1's binary_logloss: 0.0237878\n",
            "[800]\ttraining's binary_logloss: 0.0145563\tvalid_1's binary_logloss: 0.0230601\n",
            "[850]\ttraining's binary_logloss: 0.0136393\tvalid_1's binary_logloss: 0.0223866\n",
            "[900]\ttraining's binary_logloss: 0.0126895\tvalid_1's binary_logloss: 0.0217046\n",
            "[950]\ttraining's binary_logloss: 0.0116906\tvalid_1's binary_logloss: 0.0209267\n",
            "[1000]\ttraining's binary_logloss: 0.0109423\tvalid_1's binary_logloss: 0.0204035\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's binary_logloss: 0.0109423\tvalid_1's binary_logloss: 0.0204035\n",
            "Recall Score: 0.9824359685988423\n",
            "Precision Score: 0.9836449525624231\n",
            "F1 Score: 0.9830400888659672\n",
            "Threshold:  0.5309037539994582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqEEP83V9VXF"
      },
      "source": [
        "Without selection of log-scale feature \n",
        "Recall Score: 0.9848148441836492\n",
        "Precision Score: 0.9814683104156788\n",
        "F1 Score: 0.983138729467643\n",
        "Threshold:  0.48441210277113395\n",
        "\n",
        "With selection of log-scale feature \n",
        "Recall Score: 0.9824359685988423\n",
        "Precision Score: 0.9836449525624231\n",
        "F1 Score: 0.9830400888659672\n",
        "Threshold:  0.5309037539994582"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0IzTdBA9Gf3"
      },
      "source": [
        "## conclusion: should try both above cases "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES462dGg9ivO"
      },
      "source": [
        "# Strategy 2: add NULL-OR-NOT factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSRfIXvB-2ub"
      },
      "source": [
        "def extend_with_null_or_not_features(data, has_null_feature_list):\n",
        "  c_data = copy.copy(data)\n",
        "  for f_name in has_null_feature_list:\n",
        "    c_data[f_name + '_NULL_OR_NOT'] = data[f_name].isna().astype(int)\n",
        "  return c_data"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "incjdP1F-zFM",
        "outputId": "f34a980f-3482-4b20-9907-e265073db917"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "has_null_feature_list = [\n",
        "   \"AVAILABLE_LIMIT_AMT\",\n",
        "   \"BONUS_POINTS\",\n",
        "   \"CURRENT_CASH_ADV_AMT\",\n",
        "   \"CURRENT_FEE\",\n",
        "   \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "   \"CURRENT_PURCH_AMT\",\n",
        "   \"LST_CYCLE_UNPAID_BAL\"\n",
        "  ]\n",
        "tmp_train_data = extend_with_null_or_not_features(train_data, has_null_feature_list)\n",
        "\n",
        "\n",
        "log_scale_feature_list = [\n",
        "  'BNSPT',\n",
        "  'FLAM1',\n",
        "  'ACCT_VINTAGE',\n",
        "  'AVAILABLE_LIMIT_AMT',\n",
        "  'BONUS_POINTS',\n",
        "  'CREDIT_LIMIT_AMT',\n",
        "  'CREDIT_REVOLVING_RATE',\n",
        "  'CREDIT_USE_RATE',\n",
        "  'CURRENT_CASH_ADV_AMT',\n",
        "  'CURRENT_FEE',\n",
        "  'CURRENT_INSTALLMENT_BAL',\n",
        "  'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "  'CURRENT_PURCH_AMT',\n",
        "  'LST_CYCLE_UNPAID_BAL',\n",
        "  'REVOLVING_AMT'\n",
        "]\n",
        "tmp_train_data = extend_with_log_scale_features(tmp_train_data, log_scale_feature_list)\n",
        "tmp_train_data = extend_with_detailed_time(tmp_train_data, \n",
        "  weekday = True, hour = True)\n",
        "train_tmp_data = extend_with_time_difference_features(tmp_train_data, \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "preprocessed_train_data = preprocessing(train_tmp_data)\n",
        "resampled_train_data = resample(preprocessed_train_data, \n",
        "  sampling_rate=0.7, sample_type='downsample')\n",
        "removed_unimportant_feature_count = 5\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "  test_size=val_percentage, shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.05, n_estimators = 1000)\n",
        "evaluate(clf, x_test, y_test)\n",
        "#important_feature_table = get_important_feature_table(clf, x_train)\n",
        "#important_feature_table"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "shape of train data: (533202, 59)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.0834672\tvalid_1's binary_logloss: 0.0836714\n",
            "[100]\ttraining's binary_logloss: 0.0522038\tvalid_1's binary_logloss: 0.0537736\n",
            "[150]\ttraining's binary_logloss: 0.0434702\tvalid_1's binary_logloss: 0.0459496\n",
            "[200]\ttraining's binary_logloss: 0.0393683\tvalid_1's binary_logloss: 0.0424598\n",
            "[250]\ttraining's binary_logloss: 0.0356321\tvalid_1's binary_logloss: 0.0393477\n",
            "[300]\ttraining's binary_logloss: 0.0325958\tvalid_1's binary_logloss: 0.0368766\n",
            "[350]\ttraining's binary_logloss: 0.0299755\tvalid_1's binary_logloss: 0.0347561\n",
            "[400]\ttraining's binary_logloss: 0.0270696\tvalid_1's binary_logloss: 0.0324058\n",
            "[450]\ttraining's binary_logloss: 0.024545\tvalid_1's binary_logloss: 0.0303967\n",
            "[500]\ttraining's binary_logloss: 0.0225986\tvalid_1's binary_logloss: 0.0289356\n",
            "[550]\ttraining's binary_logloss: 0.0208548\tvalid_1's binary_logloss: 0.0276641\n",
            "[600]\ttraining's binary_logloss: 0.0191038\tvalid_1's binary_logloss: 0.0262661\n",
            "[650]\ttraining's binary_logloss: 0.0176442\tvalid_1's binary_logloss: 0.0252084\n",
            "[700]\ttraining's binary_logloss: 0.0163318\tvalid_1's binary_logloss: 0.0241666\n",
            "[750]\ttraining's binary_logloss: 0.0152681\tvalid_1's binary_logloss: 0.0233549\n",
            "[800]\ttraining's binary_logloss: 0.0140877\tvalid_1's binary_logloss: 0.0224846\n",
            "[850]\ttraining's binary_logloss: 0.0132602\tvalid_1's binary_logloss: 0.0218781\n",
            "[900]\ttraining's binary_logloss: 0.0122839\tvalid_1's binary_logloss: 0.0211579\n",
            "[950]\ttraining's binary_logloss: 0.0114488\tvalid_1's binary_logloss: 0.0205553\n",
            "[1000]\ttraining's binary_logloss: 0.0107839\tvalid_1's binary_logloss: 0.0201218\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's binary_logloss: 0.0107839\tvalid_1's binary_logloss: 0.0201218\n",
            "Recall Score: 0.9848148441836492\n",
            "Precision Score: 0.9814683104156788\n",
            "F1 Score: 0.983138729467643\n",
            "Threshold:  0.48441210277113395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Wj-ytNARjp",
        "outputId": "ebe6f4c5-8de7-42ef-a294-089d9f8c7aaa"
      },
      "source": [
        "important_feature_table_with_null_features = get_important_feature_table(clf, x_train)\n",
        "important_feature_table_with_null_features = important_feature_table_with_null_features.set_index('col')\n",
        "for f_name in has_null_feature_list:\n",
        "  if f_name not in important_feature_table.set_index('col').index[\n",
        "              -(removed_unimportant_feature_count):].tolist():\n",
        "    linear_importance_score = important_feature_table_with_null_features.loc[f_name]['imp']\n",
        "    null_importance_score = important_feature_table_with_null_features.loc[f_name+'_LOG_SCALE']['imp']\n",
        "    print(f_name, linear_importance_score, null_importance_score)\n",
        "    if null_importance_score > linear_importance_score:\n",
        "      print('null feature is better')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AVAILABLE_LIMIT_AMT 616 454\n",
            "BONUS_POINTS 771 370\n",
            "CURRENT_CASH_ADV_AMT 55 3\n",
            "CURRENT_FEE 802 273\n",
            "CURRENT_INSTALLMENT_PURCH_AMT 331 201\n",
            "CURRENT_PURCH_AMT 747 558\n",
            "LST_CYCLE_UNPAID_BAL 109 88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDqWTf1RBA8-"
      },
      "source": [
        "## Generate Testing Result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMKLrBOvBIpc",
        "outputId": "b5a992ff-37ab-481a-ebc5-dac7911e4b18"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/test.csv')\n",
        "#查看資料筆數\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "has_null_feature_list = [\n",
        "   \"AVAILABLE_LIMIT_AMT\",\n",
        "   \"BONUS_POINTS\",\n",
        "   \"CURRENT_CASH_ADV_AMT\",\n",
        "   \"CURRENT_FEE\",\n",
        "   \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "   \"CURRENT_PURCH_AMT\",\n",
        "   \"LST_CYCLE_UNPAID_BAL\"\n",
        "  ]\n",
        "tmp_data = extend_with_null_or_not_features(test_data, has_null_feature_list)\n",
        "log_scale_feature_list = [\n",
        "  'BNSPT',\n",
        "  'FLAM1',\n",
        "  'ACCT_VINTAGE',\n",
        "  'AVAILABLE_LIMIT_AMT',\n",
        "  'BONUS_POINTS',\n",
        "  'CREDIT_LIMIT_AMT',\n",
        "  'CREDIT_REVOLVING_RATE',\n",
        "  'CREDIT_USE_RATE',\n",
        "  'CURRENT_CASH_ADV_AMT',\n",
        "  'CURRENT_FEE',\n",
        "  'CURRENT_INSTALLMENT_BAL',\n",
        "  'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "  'CURRENT_PURCH_AMT',\n",
        "  'LST_CYCLE_UNPAID_BAL',\n",
        "  'REVOLVING_AMT'\n",
        "]\n",
        "tmp_data = extend_with_log_scale_features(tmp_data, log_scale_feature_list)\n",
        "tmp_data = extend_with_detailed_time(tmp_data, \n",
        "  weekday = True, hour = True)\n",
        "tmp_data = extend_with_time_difference_features(tmp_data, \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "preprocessed_data = preprocessing(tmp_data)\n",
        "removed_unimportant_feature_count = 5\n",
        "X = create_X(preprocessed_data, \n",
        "  drop_list = list(set([\"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqJFtjF79LLs"
      },
      "source": [
        "def get_best_threshold(tolerance = 0.05, boundary = (0., 1.)):\n",
        "  y_pred = clf.predict(X)\n",
        "  train_imbalance_rate = train_data['FRAUD_IND'].mean()\n",
        "  print(\"imbalance rate of train data:\", train_imbalance_rate)\n",
        "  threshold = 0.5\n",
        "  y_result = (y_pred > threshold).astype(int).T\n",
        "  result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "  result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "  imbalance_rate = result_table['FRAUD_IND'].mean()\n",
        "  print(\"imbalance rate of test data:\", imbalance_rate)\n",
        "  while np.abs(train_imbalance_rate-imbalance_rate) >= tolerance:\n",
        "    print('threshold:', threshold)\n",
        "    y_result = (y_pred > threshold).astype(int).T\n",
        "    result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "    result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "    imbalance_rate = result_table['FRAUD_IND'].mean()\n",
        "    print(\"imbalance rate of test data:\", imbalance_rate)\n",
        "    if imbalance_rate > train_imbalance_rate:\n",
        "      threshold = (boundary[1] + threshold)/2.\n",
        "      boundary = threshold, boundary[1]\n",
        "    else:\n",
        "      threshold = (boundary[0] + threshold)/2.\n",
        "      boundary = boundary[0], threshold\n",
        "    print(\"boundary\",boundary)\n",
        "  return threshold"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueIDRZ3rLc9A"
      },
      "source": [
        "threshold = get_best_threshold(tolerance = 0.05, boundary = (0., 1.))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmvdwyj3D5PN",
        "outputId": "05c1695f-9b57-4cdf-9545-30f10270652a"
      },
      "source": [
        "threshold = 0.96875\n",
        "y_result = (y_pred > threshold).astype(int).T\n",
        "result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "print(\"imbalance rate of test data:\", result_table['FRAUD_IND'].mean())\n",
        "result_table.to_csv('tmp_submission.csv')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imbalance rate of test data: 0.1362740427874284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7ZeR4FDE-n3"
      },
      "source": [
        "## performance: 0.03998"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7FMXUVSEH67"
      },
      "source": [
        "# Strategy 3 : Add same shop feature or not (Not Effect)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDCwBj-nDR9N"
      },
      "source": [
        "def extend_with_same_shop_features(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_shop_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['MCHNO'].shift(time_shift)\n",
        "    name = pivot_feature + '_SAME' + str(time_shift)\n",
        "    df[name] = (df[\"MCHNO\"] == df['shift']).astype(int)\n",
        "    df[name][df['MCHNO'].isna()] = -1\n",
        "    df[name][df['shift'].isna()] = -1\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add shop identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_shop_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21OcMsTl-Gd8",
        "outputId": "d92efae4-e929-4440-b2de-a0e84266a187"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "has_null_feature_list = [\n",
        "   \"AVAILABLE_LIMIT_AMT\",\n",
        "   \"BONUS_POINTS\",\n",
        "   \"CURRENT_CASH_ADV_AMT\",\n",
        "   \"CURRENT_FEE\",\n",
        "   \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "   \"CURRENT_PURCH_AMT\",\n",
        "   \"LST_CYCLE_UNPAID_BAL\"\n",
        "  ]\n",
        "tmp_train_data = extend_with_null_or_not_features(train_data, has_null_feature_list)\n",
        "\n",
        "\n",
        "log_scale_feature_list = [\n",
        "  'BNSPT',\n",
        "  'FLAM1',\n",
        "  'ACCT_VINTAGE',\n",
        "  'AVAILABLE_LIMIT_AMT',\n",
        "  'BONUS_POINTS',\n",
        "  'CREDIT_LIMIT_AMT',\n",
        "  'CREDIT_REVOLVING_RATE',\n",
        "  'CREDIT_USE_RATE',\n",
        "  'CURRENT_CASH_ADV_AMT',\n",
        "  'CURRENT_FEE',\n",
        "  'CURRENT_INSTALLMENT_BAL',\n",
        "  'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "  'CURRENT_PURCH_AMT',\n",
        "  'LST_CYCLE_UNPAID_BAL',\n",
        "  'REVOLVING_AMT'\n",
        "]\n",
        "tmp_train_data = extend_with_log_scale_features(tmp_train_data, log_scale_feature_list)\n",
        "tmp_train_data = extend_with_detailed_time(tmp_train_data, \n",
        "  weekday = True, hour = True)\n",
        "train_tmp_data = extend_with_time_difference_features(tmp_train_data, \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "\n",
        "train_tmp_data = extend_with_same_shop_features(tmp_train_data, \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "preprocessed_train_data = preprocessing(train_tmp_data)\n",
        "resampled_train_data = resample(preprocessed_train_data, \n",
        "  sampling_rate=0.7, sample_type='downsample')\n",
        "removed_unimportant_feature_count = 5\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "  test_size=val_percentage, shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.05, n_estimators = 1000)\n",
        "evaluate(clf, x_test, y_test)\n",
        "#important_feature_table = get_important_feature_table(clf, x_train)\n",
        "#important_feature_table"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "shape of train data: (533202, 59)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n",
            "add shop identical index between current and 1th-last transaction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "add shop identical index between current and 2th-last transaction\n",
            "add shop identical index between current and 3th-last transaction\n",
            "add shop identical index between current and 4th-last transaction\n",
            "add shop identical index between current and 5th-last transaction\n",
            "add shop identical index between current and 6th-last transaction\n",
            "add shop identical index between current and 7th-last transaction\n",
            "add shop identical index between current and 8th-last transaction\n",
            "add shop identical index between current and 9th-last transaction\n",
            "add shop identical index between current and 10th-last transaction\n",
            "add shop identical index between current and 11th-last transaction\n",
            "add shop identical index between current and 12th-last transaction\n",
            "add shop identical index between current and 13th-last transaction\n",
            "add shop identical index between current and 14th-last transaction\n",
            "add shop identical index between current and 15th-last transaction\n",
            "add shop identical index between current and 16th-last transaction\n",
            "add shop identical index between current and 17th-last transaction\n",
            "add shop identical index between current and 18th-last transaction\n",
            "add shop identical index between current and 19th-last transaction\n",
            "add shop identical index between current and 20th-last transaction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.0966026\tvalid_1's binary_logloss: 0.097329\n",
            "[100]\ttraining's binary_logloss: 0.065091\tvalid_1's binary_logloss: 0.0667404\n",
            "[150]\ttraining's binary_logloss: 0.0537046\tvalid_1's binary_logloss: 0.0563106\n",
            "[200]\ttraining's binary_logloss: 0.0470863\tvalid_1's binary_logloss: 0.0504664\n",
            "[250]\ttraining's binary_logloss: 0.0422039\tvalid_1's binary_logloss: 0.0463074\n",
            "[300]\ttraining's binary_logloss: 0.0382488\tvalid_1's binary_logloss: 0.0430045\n",
            "[350]\ttraining's binary_logloss: 0.0349299\tvalid_1's binary_logloss: 0.0403005\n",
            "[400]\ttraining's binary_logloss: 0.0322256\tvalid_1's binary_logloss: 0.0381258\n",
            "[450]\ttraining's binary_logloss: 0.0297539\tvalid_1's binary_logloss: 0.0362427\n",
            "[500]\ttraining's binary_logloss: 0.0274873\tvalid_1's binary_logloss: 0.0344338\n",
            "[550]\ttraining's binary_logloss: 0.0256312\tvalid_1's binary_logloss: 0.0330677\n",
            "[600]\ttraining's binary_logloss: 0.0239068\tvalid_1's binary_logloss: 0.031755\n",
            "[650]\ttraining's binary_logloss: 0.0223529\tvalid_1's binary_logloss: 0.0306798\n",
            "[700]\ttraining's binary_logloss: 0.0209064\tvalid_1's binary_logloss: 0.0295729\n",
            "[750]\ttraining's binary_logloss: 0.0196271\tvalid_1's binary_logloss: 0.0286033\n",
            "[800]\ttraining's binary_logloss: 0.0184569\tvalid_1's binary_logloss: 0.0277054\n",
            "[850]\ttraining's binary_logloss: 0.0173391\tvalid_1's binary_logloss: 0.0268714\n",
            "[900]\ttraining's binary_logloss: 0.0163718\tvalid_1's binary_logloss: 0.0261995\n",
            "[950]\ttraining's binary_logloss: 0.0154336\tvalid_1's binary_logloss: 0.0255419\n",
            "[1000]\ttraining's binary_logloss: 0.0145724\tvalid_1's binary_logloss: 0.0249102\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's binary_logloss: 0.0145724\tvalid_1's binary_logloss: 0.0249102\n",
            "Recall Score: 0.9794623741178337\n",
            "Precision Score: 0.9802785603745883\n",
            "F1 Score: 0.9798702972849691\n",
            "Threshold:  0.4889086325987303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrSMewOlJA05"
      },
      "source": [
        "# Strategy 4 : Upsample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTxmU0RAH6Uj",
        "outputId": "1e2fcb7c-053f-4fab-f6d2-e55c21d3eb7e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "has_null_feature_list = [\n",
        "   \"AVAILABLE_LIMIT_AMT\",\n",
        "   \"BONUS_POINTS\",\n",
        "   \"CURRENT_CASH_ADV_AMT\",\n",
        "   \"CURRENT_FEE\",\n",
        "   \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "   \"CURRENT_PURCH_AMT\",\n",
        "   \"LST_CYCLE_UNPAID_BAL\"\n",
        "  ]\n",
        "tmp_train_data = extend_with_null_or_not_features(train_data, has_null_feature_list)\n",
        "\n",
        "\n",
        "log_scale_feature_list = [\n",
        "  'BNSPT',\n",
        "  'FLAM1',\n",
        "  'ACCT_VINTAGE',\n",
        "  'AVAILABLE_LIMIT_AMT',\n",
        "  'BONUS_POINTS',\n",
        "  'CREDIT_LIMIT_AMT',\n",
        "  'CREDIT_REVOLVING_RATE',\n",
        "  'CREDIT_USE_RATE',\n",
        "  'CURRENT_CASH_ADV_AMT',\n",
        "  'CURRENT_FEE',\n",
        "  'CURRENT_INSTALLMENT_BAL',\n",
        "  'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "  'CURRENT_PURCH_AMT',\n",
        "  'LST_CYCLE_UNPAID_BAL',\n",
        "  'REVOLVING_AMT'\n",
        "]\n",
        "tmp_train_data = extend_with_log_scale_features(tmp_train_data, log_scale_feature_list)\n",
        "tmp_train_data = extend_with_detailed_time(tmp_train_data, \n",
        "  weekday = True, hour = True)\n",
        "train_tmp_data = extend_with_time_difference_features(tmp_train_data, \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "preprocessed_train_data = preprocessing(train_tmp_data)\n",
        "resampled_train_data = resample(preprocessed_train_data, \n",
        "  sampling_rate=0.7, sample_type='upsample')\n",
        "removed_unimportant_feature_count = 5\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "  test_size=val_percentage, shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.05, n_estimators = 1000)\n",
        "evaluate(clf, x_test, y_test)\n",
        "#important_feature_table = get_important_feature_table(clf, x_train)\n",
        "#important_feature_table"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "shape of train data: (533202, 59)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.0823547\tvalid_1's binary_logloss: 0.082804\n",
            "[100]\ttraining's binary_logloss: 0.0510114\tvalid_1's binary_logloss: 0.0525342\n",
            "[150]\ttraining's binary_logloss: 0.0424765\tvalid_1's binary_logloss: 0.0444875\n",
            "[200]\ttraining's binary_logloss: 0.0382781\tvalid_1's binary_logloss: 0.0405338\n",
            "[250]\ttraining's binary_logloss: 0.0348107\tvalid_1's binary_logloss: 0.0374528\n",
            "[300]\ttraining's binary_logloss: 0.0316671\tvalid_1's binary_logloss: 0.0345926\n",
            "[350]\ttraining's binary_logloss: 0.0291227\tvalid_1's binary_logloss: 0.0323767\n",
            "[400]\ttraining's binary_logloss: 0.0267736\tvalid_1's binary_logloss: 0.030258\n",
            "[450]\ttraining's binary_logloss: 0.0243756\tvalid_1's binary_logloss: 0.0282147\n",
            "[500]\ttraining's binary_logloss: 0.0223505\tvalid_1's binary_logloss: 0.0264626\n",
            "[550]\ttraining's binary_logloss: 0.0206306\tvalid_1's binary_logloss: 0.0249432\n",
            "[600]\ttraining's binary_logloss: 0.0192743\tvalid_1's binary_logloss: 0.0237706\n",
            "[650]\ttraining's binary_logloss: 0.0180005\tvalid_1's binary_logloss: 0.0226893\n",
            "[700]\ttraining's binary_logloss: 0.0167293\tvalid_1's binary_logloss: 0.0216046\n",
            "[750]\ttraining's binary_logloss: 0.015457\tvalid_1's binary_logloss: 0.0204764\n",
            "[800]\ttraining's binary_logloss: 0.0142903\tvalid_1's binary_logloss: 0.0195072\n",
            "[850]\ttraining's binary_logloss: 0.0133838\tvalid_1's binary_logloss: 0.0187569\n",
            "[900]\ttraining's binary_logloss: 0.0124686\tvalid_1's binary_logloss: 0.0179265\n",
            "[950]\ttraining's binary_logloss: 0.0116815\tvalid_1's binary_logloss: 0.0172772\n",
            "[1000]\ttraining's binary_logloss: 0.01096\tvalid_1's binary_logloss: 0.0166139\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's binary_logloss: 0.01096\tvalid_1's binary_logloss: 0.0166139\n",
            "Recall Score: 0.9882652496402081\n",
            "Precision Score: 0.9855920507866409\n",
            "F1 Score: 0.9869268400541721\n",
            "Threshold:  0.508056870862968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_vIvBPRMIg_"
      },
      "source": [
        "## Generate Testing Result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN-HkxQ9MFug",
        "outputId": "0ca5324c-f8b0-4932-b893-72e0d6edf481"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/test.csv')\n",
        "#查看資料筆數\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "has_null_feature_list = [\n",
        "   \"AVAILABLE_LIMIT_AMT\",\n",
        "   \"BONUS_POINTS\",\n",
        "   \"CURRENT_CASH_ADV_AMT\",\n",
        "   \"CURRENT_FEE\",\n",
        "   \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "   \"CURRENT_PURCH_AMT\",\n",
        "   \"LST_CYCLE_UNPAID_BAL\"\n",
        "  ]\n",
        "tmp_data = extend_with_null_or_not_features(test_data, has_null_feature_list)\n",
        "log_scale_feature_list = [\n",
        "  'BNSPT',\n",
        "  'FLAM1',\n",
        "  'ACCT_VINTAGE',\n",
        "  'AVAILABLE_LIMIT_AMT',\n",
        "  'BONUS_POINTS',\n",
        "  'CREDIT_LIMIT_AMT',\n",
        "  'CREDIT_REVOLVING_RATE',\n",
        "  'CREDIT_USE_RATE',\n",
        "  'CURRENT_CASH_ADV_AMT',\n",
        "  'CURRENT_FEE',\n",
        "  'CURRENT_INSTALLMENT_BAL',\n",
        "  'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "  'CURRENT_PURCH_AMT',\n",
        "  'LST_CYCLE_UNPAID_BAL',\n",
        "  'REVOLVING_AMT'\n",
        "]\n",
        "tmp_data = extend_with_log_scale_features(tmp_data, log_scale_feature_list)\n",
        "tmp_data = extend_with_detailed_time(tmp_data, \n",
        "  weekday = True, hour = True)\n",
        "tmp_data = extend_with_time_difference_features(tmp_data, \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "preprocessed_data = preprocessing(tmp_data)\n",
        "removed_unimportant_feature_count = 5\n",
        "X = create_X(preprocessed_data, \n",
        "  drop_list = list(set([\"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "### Threshold Tuning ########################################################\n",
        "tolerance = 0.01\n",
        "boundary = (0., 1.)\n",
        "threshold = 0.5\n",
        "\n",
        "y_pred = clf.predict(X)\n",
        "train_imbalance_rate = train_data['FRAUD_IND'].mean()\n",
        "print(\"imbalance rate of train data:\", train_imbalance_rate)\n",
        "\n",
        "y_result = (y_pred > threshold).astype(int).T\n",
        "result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "imbalance_rate = result_table['FRAUD_IND'].mean()\n",
        "print(\"imbalance rate of test data:\", imbalance_rate)\n",
        "while np.abs(train_imbalance_rate-imbalance_rate) >= tolerance:\n",
        "  print('threshold:', threshold)\n",
        "  y_result = (y_pred > threshold).astype(int).T\n",
        "  result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "  result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "  imbalance_rate = result_table['FRAUD_IND'].mean()\n",
        "  print(\"imbalance rate of test data:\", imbalance_rate)\n",
        "  if imbalance_rate > train_imbalance_rate:\n",
        "    threshold = (boundary[1] + threshold)/2.\n",
        "    boundary = threshold, boundary[1]\n",
        "  else:\n",
        "    threshold = (boundary[0] + threshold)/2.\n",
        "    boundary = boundary[0], threshold\n",
        "  print(\"boundary\",boundary)\n",
        "### Generate CSV ########################################################\n",
        "threshold = 0.96875\n",
        "y_result = (y_pred > threshold).astype(int).T\n",
        "result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "print(\"imbalance rate of test data:\", result_table['FRAUD_IND'].mean())\n",
        "result_table.to_csv('tmp_submission.csv')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XE8VC3kbbi6"
      },
      "source": [
        "## performance: 0.04284"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYicAYVoNI63"
      },
      "source": [
        "# Strategy 5: Add same shop type, same country, or same MCC, FLAM1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXhtp0wmLjK6"
      },
      "source": [
        "def extend_with_same_MCC(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_MCC_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['MCC'].shift(time_shift)\n",
        "    name = pivot_feature + '_SAME' + str(time_shift)\n",
        "    df[name] = (df[\"MCC\"] == df['shift']).astype(int)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add MCC identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_MCC_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def extend_with_same_STOCN(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_STOCN_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['STOCN'].shift(time_shift)\n",
        "    name = pivot_feature + '_SAME' + str(time_shift)\n",
        "    df[name] = (df[\"STOCN\"] == df['shift']).astype(int)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add STOCN identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_STOCN_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def extend_with_same_FLAM1(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_FLAM1_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['FLAM1'].shift(time_shift)\n",
        "    name = pivot_feature + '_DIFF' + str(time_shift)\n",
        "    df[name] = (df[\"FLAM1\"] - df['shift']).fillna(0)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add FLAM1 identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_FLAM1_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST2q1sMbPGZ8",
        "outputId": "81a30d19-7093-4ffb-98b4-733cff5ab6ae"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "has_null_feature_list = [\n",
        "   \"AVAILABLE_LIMIT_AMT\",\n",
        "   \"BONUS_POINTS\",\n",
        "   \"CURRENT_CASH_ADV_AMT\",\n",
        "   \"CURRENT_FEE\",\n",
        "   \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "   \"CURRENT_PURCH_AMT\",\n",
        "   \"LST_CYCLE_UNPAID_BAL\"\n",
        "  ]\n",
        "tmp_train_data = extend_with_null_or_not_features(train_data, has_null_feature_list)\n",
        "\n",
        "log_scale_feature_list = [\n",
        "  'BNSPT',\n",
        "  'FLAM1',\n",
        "  'ACCT_VINTAGE',\n",
        "  'AVAILABLE_LIMIT_AMT',\n",
        "  'BONUS_POINTS',\n",
        "  'CREDIT_LIMIT_AMT',\n",
        "  'CREDIT_REVOLVING_RATE',\n",
        "  'CREDIT_USE_RATE',\n",
        "  'CURRENT_CASH_ADV_AMT',\n",
        "  'CURRENT_FEE',\n",
        "  'CURRENT_INSTALLMENT_BAL',\n",
        "  'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "  'CURRENT_PURCH_AMT',\n",
        "  'LST_CYCLE_UNPAID_BAL',\n",
        "  'REVOLVING_AMT'\n",
        "]\n",
        "tmp_train_data = extend_with_log_scale_features(tmp_train_data, log_scale_feature_list)\n",
        "tmp_train_data = extend_with_detailed_time(tmp_train_data, \n",
        "  weekday = True, hour = True)\n",
        "train_tmp_data = extend_with_time_difference_features(tmp_train_data, \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "train_tmp_data = extend_with_same_MCC(tmp_train_data, \n",
        "  max_time_shift = 5, pivot_feature = 'CHID')\n",
        "train_tmp_data = extend_with_same_STOCN(tmp_train_data, \n",
        "  max_time_shift = 5, pivot_feature = 'CHID')\n",
        "train_tmp_data = extend_with_same_FLAM1(tmp_train_data, \n",
        "  max_time_shift = 5, pivot_feature = 'CHID')\n",
        "preprocessed_train_data = preprocessing(train_tmp_data)\n",
        "resampled_train_data = resample(preprocessed_train_data, \n",
        "  sampling_rate=0.14, sample_type='upsample')\n",
        "removed_unimportant_feature_count = 5\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "  test_size=val_percentage, shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.1, n_estimators = 3000)\n",
        "evaluate(clf, x_test, y_test)\n",
        "#important_feature_table = get_important_feature_table(clf, x_train)\n",
        "#important_feature_table"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "shape of train data: (533202, 59)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n",
            "add MCC identical index between current and 1th-last transaction\n",
            "add MCC identical index between current and 2th-last transaction\n",
            "add MCC identical index between current and 3th-last transaction\n",
            "add MCC identical index between current and 4th-last transaction\n",
            "add MCC identical index between current and 5th-last transaction\n",
            "add STOCN identical index between current and 1th-last transaction\n",
            "add STOCN identical index between current and 2th-last transaction\n",
            "add STOCN identical index between current and 3th-last transaction\n",
            "add STOCN identical index between current and 4th-last transaction\n",
            "add STOCN identical index between current and 5th-last transaction\n",
            "add FLAM1 identical index between current and 1th-last transaction\n",
            "add FLAM1 identical index between current and 2th-last transaction\n",
            "add FLAM1 identical index between current and 3th-last transaction\n",
            "add FLAM1 identical index between current and 4th-last transaction\n",
            "add FLAM1 identical index between current and 5th-last transaction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.117393\tvalid_1's binary_logloss: 0.11786\n",
            "[100]\ttraining's binary_logloss: 0.0900356\tvalid_1's binary_logloss: 0.0916258\n",
            "[150]\ttraining's binary_logloss: 0.0760189\tvalid_1's binary_logloss: 0.0782397\n",
            "[200]\ttraining's binary_logloss: 0.0658633\tvalid_1's binary_logloss: 0.0686938\n",
            "[250]\ttraining's binary_logloss: 0.0578633\tvalid_1's binary_logloss: 0.0611055\n",
            "[300]\ttraining's binary_logloss: 0.0516989\tvalid_1's binary_logloss: 0.0552256\n",
            "[350]\ttraining's binary_logloss: 0.0469727\tvalid_1's binary_logloss: 0.0508441\n",
            "[400]\ttraining's binary_logloss: 0.0425604\tvalid_1's binary_logloss: 0.0467101\n",
            "[450]\ttraining's binary_logloss: 0.0389268\tvalid_1's binary_logloss: 0.0433301\n",
            "[500]\ttraining's binary_logloss: 0.0357765\tvalid_1's binary_logloss: 0.0404164\n",
            "[550]\ttraining's binary_logloss: 0.0330955\tvalid_1's binary_logloss: 0.0379361\n",
            "[600]\ttraining's binary_logloss: 0.0303492\tvalid_1's binary_logloss: 0.0353812\n",
            "[650]\ttraining's binary_logloss: 0.0283404\tvalid_1's binary_logloss: 0.0335083\n",
            "[700]\ttraining's binary_logloss: 0.0263457\tvalid_1's binary_logloss: 0.0316392\n",
            "[750]\ttraining's binary_logloss: 0.0243726\tvalid_1's binary_logloss: 0.0297605\n",
            "[800]\ttraining's binary_logloss: 0.0226274\tvalid_1's binary_logloss: 0.0281575\n",
            "[850]\ttraining's binary_logloss: 0.0210582\tvalid_1's binary_logloss: 0.0267186\n",
            "[900]\ttraining's binary_logloss: 0.0196901\tvalid_1's binary_logloss: 0.0254192\n",
            "[950]\ttraining's binary_logloss: 0.0184683\tvalid_1's binary_logloss: 0.0242801\n",
            "[1000]\ttraining's binary_logloss: 0.0173315\tvalid_1's binary_logloss: 0.0232224\n",
            "[1050]\ttraining's binary_logloss: 0.0161237\tvalid_1's binary_logloss: 0.0220235\n",
            "[1100]\ttraining's binary_logloss: 0.0149944\tvalid_1's binary_logloss: 0.0209116\n",
            "[1150]\ttraining's binary_logloss: 0.0141244\tvalid_1's binary_logloss: 0.0201013\n",
            "[1200]\ttraining's binary_logloss: 0.0132851\tvalid_1's binary_logloss: 0.0192579\n",
            "[1250]\ttraining's binary_logloss: 0.0124811\tvalid_1's binary_logloss: 0.0185195\n",
            "[1300]\ttraining's binary_logloss: 0.0117336\tvalid_1's binary_logloss: 0.0178308\n",
            "[1350]\ttraining's binary_logloss: 0.0110528\tvalid_1's binary_logloss: 0.0171876\n",
            "[1400]\ttraining's binary_logloss: 0.0104844\tvalid_1's binary_logloss: 0.0166709\n",
            "[1450]\ttraining's binary_logloss: 0.00978042\tvalid_1's binary_logloss: 0.0159651\n",
            "[1500]\ttraining's binary_logloss: 0.00924446\tvalid_1's binary_logloss: 0.0154323\n",
            "[1550]\ttraining's binary_logloss: 0.00868837\tvalid_1's binary_logloss: 0.0149096\n",
            "[1600]\ttraining's binary_logloss: 0.00815713\tvalid_1's binary_logloss: 0.0143847\n",
            "[1650]\ttraining's binary_logloss: 0.00765297\tvalid_1's binary_logloss: 0.0138907\n",
            "[1700]\ttraining's binary_logloss: 0.00720894\tvalid_1's binary_logloss: 0.0134435\n",
            "[1750]\ttraining's binary_logloss: 0.0068329\tvalid_1's binary_logloss: 0.0130825\n",
            "[1800]\ttraining's binary_logloss: 0.00647067\tvalid_1's binary_logloss: 0.012696\n",
            "[1850]\ttraining's binary_logloss: 0.00609842\tvalid_1's binary_logloss: 0.0123385\n",
            "[1900]\ttraining's binary_logloss: 0.00579077\tvalid_1's binary_logloss: 0.0120482\n",
            "[1950]\ttraining's binary_logloss: 0.00544737\tvalid_1's binary_logloss: 0.0117032\n",
            "[2000]\ttraining's binary_logloss: 0.00508341\tvalid_1's binary_logloss: 0.011352\n",
            "[2050]\ttraining's binary_logloss: 0.00477131\tvalid_1's binary_logloss: 0.0110367\n",
            "[2100]\ttraining's binary_logloss: 0.00454372\tvalid_1's binary_logloss: 0.0108105\n",
            "[2150]\ttraining's binary_logloss: 0.00429277\tvalid_1's binary_logloss: 0.0105421\n",
            "[2200]\ttraining's binary_logloss: 0.00407222\tvalid_1's binary_logloss: 0.0103039\n",
            "[2250]\ttraining's binary_logloss: 0.00383006\tvalid_1's binary_logloss: 0.010063\n",
            "[2300]\ttraining's binary_logloss: 0.00361431\tvalid_1's binary_logloss: 0.00983216\n",
            "[2350]\ttraining's binary_logloss: 0.00340634\tvalid_1's binary_logloss: 0.00962845\n",
            "[2400]\ttraining's binary_logloss: 0.00323596\tvalid_1's binary_logloss: 0.00945957\n",
            "[2450]\ttraining's binary_logloss: 0.00306867\tvalid_1's binary_logloss: 0.00928718\n",
            "[2500]\ttraining's binary_logloss: 0.00289604\tvalid_1's binary_logloss: 0.0090962\n",
            "[2550]\ttraining's binary_logloss: 0.00274204\tvalid_1's binary_logloss: 0.00892744\n",
            "[2600]\ttraining's binary_logloss: 0.00259412\tvalid_1's binary_logloss: 0.0087766\n",
            "[2650]\ttraining's binary_logloss: 0.0024515\tvalid_1's binary_logloss: 0.00863941\n",
            "[2700]\ttraining's binary_logloss: 0.00232042\tvalid_1's binary_logloss: 0.00850605\n",
            "[2750]\ttraining's binary_logloss: 0.00220052\tvalid_1's binary_logloss: 0.00837395\n",
            "[2800]\ttraining's binary_logloss: 0.0021011\tvalid_1's binary_logloss: 0.00825374\n",
            "[2850]\ttraining's binary_logloss: 0.00198252\tvalid_1's binary_logloss: 0.00812696\n",
            "[2900]\ttraining's binary_logloss: 0.00186889\tvalid_1's binary_logloss: 0.00800075\n",
            "[2950]\ttraining's binary_logloss: 0.00177163\tvalid_1's binary_logloss: 0.00788015\n",
            "[3000]\ttraining's binary_logloss: 0.00168715\tvalid_1's binary_logloss: 0.00777516\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[3000]\ttraining's binary_logloss: 0.00168715\tvalid_1's binary_logloss: 0.00777516\n",
            "Recall Score: 0.9993519836500491\n",
            "Precision Score: 0.9984395404944775\n",
            "F1 Score: 0.9988955537040471\n",
            "Threshold:  0.8528432148328422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMJFs2AoT2nQ"
      },
      "source": [
        "## Generate Testing Result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-zcYrZKRqkH",
        "outputId": "c76c7c70-0e5d-4580-b0d7-c6c9378b2035"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/test.csv')\n",
        "#查看資料筆數\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "has_null_feature_list = [\n",
        "   \"AVAILABLE_LIMIT_AMT\",\n",
        "   \"BONUS_POINTS\",\n",
        "   \"CURRENT_CASH_ADV_AMT\",\n",
        "   \"CURRENT_FEE\",\n",
        "   \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "   \"CURRENT_PURCH_AMT\",\n",
        "   \"LST_CYCLE_UNPAID_BAL\"\n",
        "  ]\n",
        "tmp_data = extend_with_null_or_not_features(test_data, has_null_feature_list)\n",
        "log_scale_feature_list = [\n",
        "  'BNSPT',\n",
        "  'FLAM1',\n",
        "  'ACCT_VINTAGE',\n",
        "  'AVAILABLE_LIMIT_AMT',\n",
        "  'BONUS_POINTS',\n",
        "  'CREDIT_LIMIT_AMT',\n",
        "  'CREDIT_REVOLVING_RATE',\n",
        "  'CREDIT_USE_RATE',\n",
        "  'CURRENT_CASH_ADV_AMT',\n",
        "  'CURRENT_FEE',\n",
        "  'CURRENT_INSTALLMENT_BAL',\n",
        "  'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "  'CURRENT_PURCH_AMT',\n",
        "  'LST_CYCLE_UNPAID_BAL',\n",
        "  'REVOLVING_AMT'\n",
        "]\n",
        "tmp_data = extend_with_log_scale_features(tmp_data, log_scale_feature_list)\n",
        "tmp_data = extend_with_detailed_time(tmp_data, \n",
        "  weekday = True, hour = True)\n",
        "tmp_data = extend_with_time_difference_features(tmp_data, \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "tmp_data = extend_with_same_MCC(tmp_data, \n",
        "  max_time_shift = 5, pivot_feature = 'CHID')\n",
        "tmp_data = extend_with_same_STOCN(tmp_data, \n",
        "  max_time_shift = 5, pivot_feature = 'CHID')\n",
        "tmp_data = extend_with_same_FLAM1(tmp_data, \n",
        "  max_time_shift = 5, pivot_feature = 'CHID')\n",
        "preprocessed_data = preprocessing(tmp_data)\n",
        "removed_unimportant_feature_count = 5\n",
        "X = create_X(preprocessed_data, \n",
        "  drop_list = list(set([\"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "### Threshold Tuning ########################################################\n",
        "tolerance = 0.01\n",
        "boundary = (0.9, 0.99)\n",
        "threshold = 0.95\n",
        "\n",
        "y_pred = clf.predict(X)\n",
        "train_imbalance_rate = train_data['FRAUD_IND'].mean()\n",
        "print(\"imbalance rate of train data:\", train_imbalance_rate)\n",
        "\n",
        "y_result = (y_pred > threshold).astype(int).T\n",
        "result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "imbalance_rate = result_table['FRAUD_IND'].mean()\n",
        "print(\"imbalance rate of test data:\", imbalance_rate)\n",
        "while np.abs(train_imbalance_rate-imbalance_rate) >= tolerance and boundary[0]<boundary[1]:\n",
        "  print('threshold:', threshold)\n",
        "  y_result = (y_pred > threshold).astype(int).T\n",
        "  result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "  result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "  imbalance_rate = result_table['FRAUD_IND'].mean()\n",
        "  print(\"imbalance rate of test data:\", imbalance_rate)\n",
        "  if imbalance_rate > train_imbalance_rate:\n",
        "    threshold = (boundary[1] + threshold)/2.\n",
        "    boundary = threshold, boundary[1]\n",
        "  else:\n",
        "    threshold = (boundary[0] + threshold)/2.\n",
        "    boundary = boundary[0], threshold\n",
        "  print(\"boundary\",boundary)\n",
        "### Generate CSV ########################################################\n",
        "threshold = threshold\n",
        "y_result = (y_pred > threshold).astype(int).T\n",
        "result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "print(\"imbalance rate of test data:\", result_table['FRAUD_IND'].mean())\n",
        "result_table.to_csv('tmp_submission.csv')\n",
        "print('csv saved.')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n",
            "add MCC identical index between current and 1th-last transaction\n",
            "add MCC identical index between current and 2th-last transaction\n",
            "add MCC identical index between current and 3th-last transaction\n",
            "add MCC identical index between current and 4th-last transaction\n",
            "add MCC identical index between current and 5th-last transaction\n",
            "add STOCN identical index between current and 1th-last transaction\n",
            "add STOCN identical index between current and 2th-last transaction\n",
            "add STOCN identical index between current and 3th-last transaction\n",
            "add STOCN identical index between current and 4th-last transaction\n",
            "add STOCN identical index between current and 5th-last transaction\n",
            "add FLAM1 identical index between current and 1th-last transaction\n",
            "add FLAM1 identical index between current and 2th-last transaction\n",
            "add FLAM1 identical index between current and 3th-last transaction\n",
            "add FLAM1 identical index between current and 4th-last transaction\n",
            "add FLAM1 identical index between current and 5th-last transaction\n",
            "imbalance rate of train data: 0.14388730724941018\n",
            "imbalance rate of test data: 0.642078186033218\n",
            "threshold: 0.5\n",
            "imbalance rate of test data: 0.642078186033218\n",
            "boundary (0.75, 1.0)\n",
            "threshold: 0.75\n",
            "imbalance rate of test data: 0.542661458498735\n",
            "boundary (0.875, 1.0)\n",
            "threshold: 0.875\n",
            "imbalance rate of test data: 0.4572877301068098\n",
            "boundary (0.9375, 1.0)\n",
            "threshold: 0.9375\n",
            "imbalance rate of test data: 0.37895137984693067\n",
            "boundary (0.96875, 1.0)\n",
            "threshold: 0.96875\n",
            "imbalance rate of test data: 0.3082536758868176\n",
            "boundary (0.984375, 1.0)\n",
            "threshold: 0.984375\n",
            "imbalance rate of test data: 0.24562016365503297\n",
            "boundary (0.9921875, 1.0)\n",
            "threshold: 0.9921875\n",
            "imbalance rate of test data: 0.19218351381964072\n",
            "boundary (0.99609375, 1.0)\n",
            "threshold: 0.99609375\n",
            "imbalance rate of test data: 0.14586257634941302\n",
            "boundary (0.998046875, 1.0)\n",
            "imbalance rate of test data: 0.10760794774894937\n",
            "csv saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mub4YqNRVsuw"
      },
      "source": [
        "# Strategy 6: Add same ECFG, PAY_TYPE, CONTP, ETYMD\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvhWnBiNaz5Q"
      },
      "source": [
        "def extend_with_same_class_between_transactions(data, f_name, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def identical_MCC_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])[f_name].shift(time_shift)\n",
        "    name = pivot_feature + '_SAME' + str(time_shift)\n",
        "    df[name] = (df[f_name] == df['shift']).astype(int)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add \" + f_name + \" identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = identical_MCC_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahb-axtPm4LB"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "has_null_feature_list = [\n",
        "   \"AVAILABLE_LIMIT_AMT\",\n",
        "   \"BONUS_POINTS\",\n",
        "   \"CURRENT_CASH_ADV_AMT\",\n",
        "   \"CURRENT_FEE\",\n",
        "   \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "   \"CURRENT_PURCH_AMT\",\n",
        "   \"LST_CYCLE_UNPAID_BAL\"\n",
        "  ]\n",
        "tmp_train_data = extend_with_null_or_not_features(train_data, has_null_feature_list)\n",
        "\n",
        "log_scale_feature_list = [\n",
        "  'BNSPT',\n",
        "  'FLAM1',\n",
        "  'ACCT_VINTAGE',\n",
        "  'AVAILABLE_LIMIT_AMT',\n",
        "  'BONUS_POINTS',\n",
        "  'CREDIT_LIMIT_AMT',\n",
        "  'CREDIT_REVOLVING_RATE',\n",
        "  'CREDIT_USE_RATE',\n",
        "  'CURRENT_CASH_ADV_AMT',\n",
        "  'CURRENT_FEE',\n",
        "  'CURRENT_INSTALLMENT_BAL',\n",
        "  'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "  'CURRENT_PURCH_AMT',\n",
        "  'LST_CYCLE_UNPAID_BAL',\n",
        "  'REVOLVING_AMT'\n",
        "]\n",
        "tmp_train_data = extend_with_log_scale_features(tmp_train_data, log_scale_feature_list)\n",
        "tmp_train_data = extend_with_detailed_time(tmp_train_data, \n",
        "  weekday = True, hour = True)\n",
        "train_tmp_data = extend_with_time_difference_features(tmp_train_data, \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "train_tmp_data = extend_with_same_MCC(tmp_train_data, \n",
        "  max_time_shift = 5, pivot_feature = 'CHID')\n",
        "train_tmp_data = extend_with_same_STOCN(tmp_train_data, \n",
        "  max_time_shift = 5, pivot_feature = 'CHID')\n",
        "train_tmp_data = extend_with_same_FLAM1(tmp_train_data, \n",
        "  max_time_shift = 5, pivot_feature = 'CHID')\n",
        "# ECFG, PAY_TYPE, CONTP, ETYMD\n",
        "train_tmp_data = extend_with_same_class_between_transactions(tmp_train_data, 'ECFG',\n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "train_tmp_data = extend_with_same_class_between_transactions(tmp_train_data, 'PAY_TYPE', \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "train_tmp_data = extend_with_same_class_between_transactions(tmp_train_data, 'CONTP', \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "train_tmp_data = extend_with_same_class_between_transactions(tmp_train_data, 'ETYMD', \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "\n",
        "preprocessed_train_data = preprocessing(train_tmp_data)\n",
        "resampled_train_data = resample(preprocessed_train_data, \n",
        "  sampling_rate=0.14, sample_type='upsample')\n",
        "removed_unimportant_feature_count = 5\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "  test_size=val_percentage, shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.1, n_estimators = 3000)\n",
        "evaluate(clf, x_test, y_test)\n",
        "#important_feature_table = get_important_feature_table(clf, x_train)\n",
        "#important_feature_table\n",
        "# performance F1: 0.9984"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ1CfvVdnJZW"
      },
      "source": [
        "## generate testing result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh14FiHAnH6-"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/test.csv')\n",
        "#查看資料筆數\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "has_null_feature_list = [\n",
        "   \"AVAILABLE_LIMIT_AMT\",\n",
        "   \"BONUS_POINTS\",\n",
        "   \"CURRENT_CASH_ADV_AMT\",\n",
        "   \"CURRENT_FEE\",\n",
        "   \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "   \"CURRENT_PURCH_AMT\",\n",
        "   \"LST_CYCLE_UNPAID_BAL\"\n",
        "  ]\n",
        "tmp_data = extend_with_null_or_not_features(test_data, has_null_feature_list)\n",
        "log_scale_feature_list = [\n",
        "  'BNSPT',\n",
        "  'FLAM1',\n",
        "  'ACCT_VINTAGE',\n",
        "  'AVAILABLE_LIMIT_AMT',\n",
        "  'BONUS_POINTS',\n",
        "  'CREDIT_LIMIT_AMT',\n",
        "  'CREDIT_REVOLVING_RATE',\n",
        "  'CREDIT_USE_RATE',\n",
        "  'CURRENT_CASH_ADV_AMT',\n",
        "  'CURRENT_FEE',\n",
        "  'CURRENT_INSTALLMENT_BAL',\n",
        "  'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "  'CURRENT_PURCH_AMT',\n",
        "  'LST_CYCLE_UNPAID_BAL',\n",
        "  'REVOLVING_AMT'\n",
        "]\n",
        "tmp_data = extend_with_log_scale_features(tmp_data, log_scale_feature_list)\n",
        "tmp_data = extend_with_detailed_time(tmp_data, \n",
        "  weekday = True, hour = True)\n",
        "tmp_data = extend_with_time_difference_features(tmp_data, \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "tmp_data = extend_with_same_MCC(tmp_data, \n",
        "  max_time_shift = 5, pivot_feature = 'CHID')\n",
        "tmp_data = extend_with_same_STOCN(tmp_data, \n",
        "  max_time_shift = 5, pivot_feature = 'CHID')\n",
        "tmp_data = extend_with_same_FLAM1(tmp_data, \n",
        "  max_time_shift = 5, pivot_feature = 'CHID')\n",
        "\n",
        "# ECFG, PAY_TYPE, CONTP, ETYMD\n",
        "tmp_data = extend_with_same_class_between_transactions(tmp_data, 'ECFG',\n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "tmp_data = extend_with_same_class_between_transactions(tmp_data, 'PAY_TYPE', \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "tmp_data = extend_with_same_class_between_transactions(tmp_data, 'CONTP', \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "tmp_data = extend_with_same_class_between_transactions(tmp_data, 'ETYMD', \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "\n",
        "preprocessed_data = preprocessing(tmp_data)\n",
        "removed_unimportant_feature_count = 5\n",
        "X = create_X(preprocessed_data, \n",
        "  drop_list = list(set([\"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "### Threshold Tuning ########################################################\n",
        "'''tolerance = 0.01\n",
        "boundary = (0., 1.)\n",
        "threshold = 0.5\n",
        "\n",
        "y_pred = clf.predict(X)\n",
        "train_imbalance_rate = train_data['FRAUD_IND'].mean()\n",
        "print(\"imbalance rate of train data:\", train_imbalance_rate)\n",
        "\n",
        "y_result = (y_pred > threshold).astype(int).T\n",
        "result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "imbalance_rate = result_table['FRAUD_IND'].mean()\n",
        "print(\"imbalance rate of test data:\", imbalance_rate)\n",
        "while np.abs(train_imbalance_rate-imbalance_rate) >= tolerance and boundary[0]<boundary[1]:\n",
        "  print('threshold:', threshold)\n",
        "  y_result = (y_pred > threshold).astype(int).T\n",
        "  result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "  result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "  imbalance_rate = result_table['FRAUD_IND'].mean()\n",
        "  print(\"imbalance rate of test data:\", imbalance_rate)\n",
        "  if imbalance_rate > train_imbalance_rate:\n",
        "    threshold = (boundary[1] + threshold)/2.\n",
        "    boundary = threshold, boundary[1]\n",
        "  else:\n",
        "    threshold = (boundary[0] + threshold)/2.\n",
        "    boundary = boundary[0], threshold\n",
        "  print(\"boundary\",boundary)'''\n",
        "### Generate CSV ########################################################\n",
        "y_pred = clf.predict(X)\n",
        "threshold = 0.998\n",
        "y_result = (y_pred > threshold).astype(int).T\n",
        "result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "print(\"imbalance rate of test data:\", result_table['FRAUD_IND'].mean())\n",
        "result_table.to_csv('tmp_submission.csv')\n",
        "print('csv saved.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT45rUixmXX2"
      },
      "source": [
        "## Performance: 0.05558"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0jHdFZauPfZ"
      },
      "source": [
        "# Strategy 7: Re-tune threshold "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW25ZmQluWGZ"
      },
      "source": [
        "y_pred = clf.predict(X)\n",
        "threshold = 0.99985\n",
        "y_result = (y_pred > threshold).astype(int).T\n",
        "result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "print(\"imbalance rate of test data:\", result_table['FRAUD_IND'].mean())\n",
        "result_table.to_csv('tmp_submission.csv')\n",
        "print('csv saved.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18Pl5mZevI2r"
      },
      "source": [
        "# Strategy 8: Add same 'ECFG', 'PAY_TYPE', 'CONTP', 'ETYMD', 'STOCN', 'SCITY', 'APPFG', 'MCC' and Add weekday fraud factor "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56g0NaBXxc7x"
      },
      "source": [
        "def extend_with_strang_weekday_transaction_change(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def strange_week_index(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])['WEEKDAY'].shift(time_shift)\n",
        "    name = pivot_feature + '_SAME' + str(time_shift)\n",
        "    df[name] = ((df['WEEKDAY']!=6) & (df['WEEKDAY']!=7) & ((df['shift']==6)|(df['shift']==7))).astype(int)\n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add \" + 'WEEKDAY' + \" identical index between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = strange_week_index(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "def overall_preprocessing(train_data):\n",
        "  has_null_feature_list = [\n",
        "    \"AVAILABLE_LIMIT_AMT\",\n",
        "    \"BONUS_POINTS\",\n",
        "    \"CURRENT_CASH_ADV_AMT\",\n",
        "    \"CURRENT_FEE\",\n",
        "    \"CURRENT_INSTALLMENT_PURCH_AMT\",\n",
        "    \"CURRENT_PURCH_AMT\",\n",
        "    \"LST_CYCLE_UNPAID_BAL\"\n",
        "    ]\n",
        "  tmp_data = extend_with_null_or_not_features(train_data, has_null_feature_list)\n",
        "\n",
        "  log_scale_feature_list = [\n",
        "    'BNSPT',\n",
        "    'FLAM1',\n",
        "    'ACCT_VINTAGE',\n",
        "    'AVAILABLE_LIMIT_AMT',\n",
        "    'BONUS_POINTS',\n",
        "    'CREDIT_LIMIT_AMT',\n",
        "    'CREDIT_REVOLVING_RATE',\n",
        "    'CREDIT_USE_RATE',\n",
        "    'CURRENT_CASH_ADV_AMT',\n",
        "    'CURRENT_FEE',\n",
        "    'CURRENT_INSTALLMENT_BAL',\n",
        "    'CURRENT_INSTALLMENT_PURCH_AMT',\n",
        "    'CURRENT_PURCH_AMT',\n",
        "    'LST_CYCLE_UNPAID_BAL',\n",
        "    'REVOLVING_AMT'\n",
        "  ]\n",
        "  tmp_data = extend_with_log_scale_features(tmp_data, log_scale_feature_list)\n",
        "  tmp_data = extend_with_detailed_time(tmp_data, \n",
        "    weekday = True, hour = True)\n",
        "  tmp_data = extend_with_time_difference_features(tmp_data, \n",
        "    max_time_shift = 20, pivot_feature = 'CHID')\n",
        "  tmp_data = extend_with_same_FLAM1(tmp_data, \n",
        "    max_time_shift = 20, pivot_feature = 'CHID')\n",
        "  tmp_data = extend_with_strang_weekday_transaction_change(tmp_data, \n",
        "    max_time_shift = 5, pivot_feature = 'CHID')\n",
        "  for class_name in ['ECFG', 'PAY_TYPE', 'CONTP', 'ETYMD', 'STOCN', 'SCITY', 'APPFG', 'MCC', 'MCHNO', 'FALLBACK_IND']:\n",
        "    tmp_data = extend_with_same_class_between_transactions(tmp_data, class_name,\n",
        "      max_time_shift = 20, pivot_feature = 'CHID')\n",
        "    \n",
        "  tmp_data = preprocessing(tmp_data)\n",
        "  return tmp_data"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohPZjdCu8dUd"
      },
      "source": [
        ""
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdmKU6i0vRnM",
        "outputId": "c04926ce-a50b-4ad5-f476-75072c90eeff"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "\n",
        "tmp_data = overall_preprocessing(train_data)\n",
        "resampled_train_data = resample(tmp_data, \n",
        "  sampling_rate=0.14, sample_type='upsample')\n",
        "removed_unimportant_feature_count = 5\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "  test_size=val_percentage, shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.1, n_estimators = 3000)\n",
        "evaluate(clf, x_test, y_test)\n",
        "#important_feature_table = get_important_feature_table(clf, x_train)\n",
        "#important_feature_table"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "shape of train data: (533202, 59)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log10\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n",
            "add FLAM1 identical index between current and 1th-last transaction\n",
            "add FLAM1 identical index between current and 2th-last transaction\n",
            "add FLAM1 identical index between current and 3th-last transaction\n",
            "add FLAM1 identical index between current and 4th-last transaction\n",
            "add FLAM1 identical index between current and 5th-last transaction\n",
            "add FLAM1 identical index between current and 6th-last transaction\n",
            "add FLAM1 identical index between current and 7th-last transaction\n",
            "add FLAM1 identical index between current and 8th-last transaction\n",
            "add FLAM1 identical index between current and 9th-last transaction\n",
            "add FLAM1 identical index between current and 10th-last transaction\n",
            "add FLAM1 identical index between current and 11th-last transaction\n",
            "add FLAM1 identical index between current and 12th-last transaction\n",
            "add FLAM1 identical index between current and 13th-last transaction\n",
            "add FLAM1 identical index between current and 14th-last transaction\n",
            "add FLAM1 identical index between current and 15th-last transaction\n",
            "add FLAM1 identical index between current and 16th-last transaction\n",
            "add FLAM1 identical index between current and 17th-last transaction\n",
            "add FLAM1 identical index between current and 18th-last transaction\n",
            "add FLAM1 identical index between current and 19th-last transaction\n",
            "add FLAM1 identical index between current and 20th-last transaction\n",
            "add WEEKDAY identical index between current and 1th-last transaction\n",
            "add WEEKDAY identical index between current and 2th-last transaction\n",
            "add WEEKDAY identical index between current and 3th-last transaction\n",
            "add WEEKDAY identical index between current and 4th-last transaction\n",
            "add WEEKDAY identical index between current and 5th-last transaction\n",
            "add ECFG identical index between current and 1th-last transaction\n",
            "add ECFG identical index between current and 2th-last transaction\n",
            "add ECFG identical index between current and 3th-last transaction\n",
            "add ECFG identical index between current and 4th-last transaction\n",
            "add ECFG identical index between current and 5th-last transaction\n",
            "add ECFG identical index between current and 6th-last transaction\n",
            "add ECFG identical index between current and 7th-last transaction\n",
            "add ECFG identical index between current and 8th-last transaction\n",
            "add ECFG identical index between current and 9th-last transaction\n",
            "add ECFG identical index between current and 10th-last transaction\n",
            "add ECFG identical index between current and 11th-last transaction\n",
            "add ECFG identical index between current and 12th-last transaction\n",
            "add ECFG identical index between current and 13th-last transaction\n",
            "add ECFG identical index between current and 14th-last transaction\n",
            "add ECFG identical index between current and 15th-last transaction\n",
            "add ECFG identical index between current and 16th-last transaction\n",
            "add ECFG identical index between current and 17th-last transaction\n",
            "add ECFG identical index between current and 18th-last transaction\n",
            "add ECFG identical index between current and 19th-last transaction\n",
            "add ECFG identical index between current and 20th-last transaction\n",
            "add PAY_TYPE identical index between current and 1th-last transaction\n",
            "add PAY_TYPE identical index between current and 2th-last transaction\n",
            "add PAY_TYPE identical index between current and 3th-last transaction\n",
            "add PAY_TYPE identical index between current and 4th-last transaction\n",
            "add PAY_TYPE identical index between current and 5th-last transaction\n",
            "add PAY_TYPE identical index between current and 6th-last transaction\n",
            "add PAY_TYPE identical index between current and 7th-last transaction\n",
            "add PAY_TYPE identical index between current and 8th-last transaction\n",
            "add PAY_TYPE identical index between current and 9th-last transaction\n",
            "add PAY_TYPE identical index between current and 10th-last transaction\n",
            "add PAY_TYPE identical index between current and 11th-last transaction\n",
            "add PAY_TYPE identical index between current and 12th-last transaction\n",
            "add PAY_TYPE identical index between current and 13th-last transaction\n",
            "add PAY_TYPE identical index between current and 14th-last transaction\n",
            "add PAY_TYPE identical index between current and 15th-last transaction\n",
            "add PAY_TYPE identical index between current and 16th-last transaction\n",
            "add PAY_TYPE identical index between current and 17th-last transaction\n",
            "add PAY_TYPE identical index between current and 18th-last transaction\n",
            "add PAY_TYPE identical index between current and 19th-last transaction\n",
            "add PAY_TYPE identical index between current and 20th-last transaction\n",
            "add CONTP identical index between current and 1th-last transaction\n",
            "add CONTP identical index between current and 2th-last transaction\n",
            "add CONTP identical index between current and 3th-last transaction\n",
            "add CONTP identical index between current and 4th-last transaction\n",
            "add CONTP identical index between current and 5th-last transaction\n",
            "add CONTP identical index between current and 6th-last transaction\n",
            "add CONTP identical index between current and 7th-last transaction\n",
            "add CONTP identical index between current and 8th-last transaction\n",
            "add CONTP identical index between current and 9th-last transaction\n",
            "add CONTP identical index between current and 10th-last transaction\n",
            "add CONTP identical index between current and 11th-last transaction\n",
            "add CONTP identical index between current and 12th-last transaction\n",
            "add CONTP identical index between current and 13th-last transaction\n",
            "add CONTP identical index between current and 14th-last transaction\n",
            "add CONTP identical index between current and 15th-last transaction\n",
            "add CONTP identical index between current and 16th-last transaction\n",
            "add CONTP identical index between current and 17th-last transaction\n",
            "add CONTP identical index between current and 18th-last transaction\n",
            "add CONTP identical index between current and 19th-last transaction\n",
            "add CONTP identical index between current and 20th-last transaction\n",
            "add ETYMD identical index between current and 1th-last transaction\n",
            "add ETYMD identical index between current and 2th-last transaction\n",
            "add ETYMD identical index between current and 3th-last transaction\n",
            "add ETYMD identical index between current and 4th-last transaction\n",
            "add ETYMD identical index between current and 5th-last transaction\n",
            "add ETYMD identical index between current and 6th-last transaction\n",
            "add ETYMD identical index between current and 7th-last transaction\n",
            "add ETYMD identical index between current and 8th-last transaction\n",
            "add ETYMD identical index between current and 9th-last transaction\n",
            "add ETYMD identical index between current and 10th-last transaction\n",
            "add ETYMD identical index between current and 11th-last transaction\n",
            "add ETYMD identical index between current and 12th-last transaction\n",
            "add ETYMD identical index between current and 13th-last transaction\n",
            "add ETYMD identical index between current and 14th-last transaction\n",
            "add ETYMD identical index between current and 15th-last transaction\n",
            "add ETYMD identical index between current and 16th-last transaction\n",
            "add ETYMD identical index between current and 17th-last transaction\n",
            "add ETYMD identical index between current and 18th-last transaction\n",
            "add ETYMD identical index between current and 19th-last transaction\n",
            "add ETYMD identical index between current and 20th-last transaction\n",
            "add STOCN identical index between current and 1th-last transaction\n",
            "add STOCN identical index between current and 2th-last transaction\n",
            "add STOCN identical index between current and 3th-last transaction\n",
            "add STOCN identical index between current and 4th-last transaction\n",
            "add STOCN identical index between current and 5th-last transaction\n",
            "add STOCN identical index between current and 6th-last transaction\n",
            "add STOCN identical index between current and 7th-last transaction\n",
            "add STOCN identical index between current and 8th-last transaction\n",
            "add STOCN identical index between current and 9th-last transaction\n",
            "add STOCN identical index between current and 10th-last transaction\n",
            "add STOCN identical index between current and 11th-last transaction\n",
            "add STOCN identical index between current and 12th-last transaction\n",
            "add STOCN identical index between current and 13th-last transaction\n",
            "add STOCN identical index between current and 14th-last transaction\n",
            "add STOCN identical index between current and 15th-last transaction\n",
            "add STOCN identical index between current and 16th-last transaction\n",
            "add STOCN identical index between current and 17th-last transaction\n",
            "add STOCN identical index between current and 18th-last transaction\n",
            "add STOCN identical index between current and 19th-last transaction\n",
            "add STOCN identical index between current and 20th-last transaction\n",
            "add SCITY identical index between current and 1th-last transaction\n",
            "add SCITY identical index between current and 2th-last transaction\n",
            "add SCITY identical index between current and 3th-last transaction\n",
            "add SCITY identical index between current and 4th-last transaction\n",
            "add SCITY identical index between current and 5th-last transaction\n",
            "add SCITY identical index between current and 6th-last transaction\n",
            "add SCITY identical index between current and 7th-last transaction\n",
            "add SCITY identical index between current and 8th-last transaction\n",
            "add SCITY identical index between current and 9th-last transaction\n",
            "add SCITY identical index between current and 10th-last transaction\n",
            "add SCITY identical index between current and 11th-last transaction\n",
            "add SCITY identical index between current and 12th-last transaction\n",
            "add SCITY identical index between current and 13th-last transaction\n",
            "add SCITY identical index between current and 14th-last transaction\n",
            "add SCITY identical index between current and 15th-last transaction\n",
            "add SCITY identical index between current and 16th-last transaction\n",
            "add SCITY identical index between current and 17th-last transaction\n",
            "add SCITY identical index between current and 18th-last transaction\n",
            "add SCITY identical index between current and 19th-last transaction\n",
            "add SCITY identical index between current and 20th-last transaction\n",
            "add APPFG identical index between current and 1th-last transaction\n",
            "add APPFG identical index between current and 2th-last transaction\n",
            "add APPFG identical index between current and 3th-last transaction\n",
            "add APPFG identical index between current and 4th-last transaction\n",
            "add APPFG identical index between current and 5th-last transaction\n",
            "add APPFG identical index between current and 6th-last transaction\n",
            "add APPFG identical index between current and 7th-last transaction\n",
            "add APPFG identical index between current and 8th-last transaction\n",
            "add APPFG identical index between current and 9th-last transaction\n",
            "add APPFG identical index between current and 10th-last transaction\n",
            "add APPFG identical index between current and 11th-last transaction\n",
            "add APPFG identical index between current and 12th-last transaction\n",
            "add APPFG identical index between current and 13th-last transaction\n",
            "add APPFG identical index between current and 14th-last transaction\n",
            "add APPFG identical index between current and 15th-last transaction\n",
            "add APPFG identical index between current and 16th-last transaction\n",
            "add APPFG identical index between current and 17th-last transaction\n",
            "add APPFG identical index between current and 18th-last transaction\n",
            "add APPFG identical index between current and 19th-last transaction\n",
            "add APPFG identical index between current and 20th-last transaction\n",
            "add MCC identical index between current and 1th-last transaction\n",
            "add MCC identical index between current and 2th-last transaction\n",
            "add MCC identical index between current and 3th-last transaction\n",
            "add MCC identical index between current and 4th-last transaction\n",
            "add MCC identical index between current and 5th-last transaction\n",
            "add MCC identical index between current and 6th-last transaction\n",
            "add MCC identical index between current and 7th-last transaction\n",
            "add MCC identical index between current and 8th-last transaction\n",
            "add MCC identical index between current and 9th-last transaction\n",
            "add MCC identical index between current and 10th-last transaction\n",
            "add MCC identical index between current and 11th-last transaction\n",
            "add MCC identical index between current and 12th-last transaction\n",
            "add MCC identical index between current and 13th-last transaction\n",
            "add MCC identical index between current and 14th-last transaction\n",
            "add MCC identical index between current and 15th-last transaction\n",
            "add MCC identical index between current and 16th-last transaction\n",
            "add MCC identical index between current and 17th-last transaction\n",
            "add MCC identical index between current and 18th-last transaction\n",
            "add MCC identical index between current and 19th-last transaction\n",
            "add MCC identical index between current and 20th-last transaction\n",
            "add MCHNO identical index between current and 1th-last transaction\n",
            "add MCHNO identical index between current and 2th-last transaction\n",
            "add MCHNO identical index between current and 3th-last transaction\n",
            "add MCHNO identical index between current and 4th-last transaction\n",
            "add MCHNO identical index between current and 5th-last transaction\n",
            "add MCHNO identical index between current and 6th-last transaction\n",
            "add MCHNO identical index between current and 7th-last transaction\n",
            "add MCHNO identical index between current and 8th-last transaction\n",
            "add MCHNO identical index between current and 9th-last transaction\n",
            "add MCHNO identical index between current and 10th-last transaction\n",
            "add MCHNO identical index between current and 11th-last transaction\n",
            "add MCHNO identical index between current and 12th-last transaction\n",
            "add MCHNO identical index between current and 13th-last transaction\n",
            "add MCHNO identical index between current and 14th-last transaction\n",
            "add MCHNO identical index between current and 15th-last transaction\n",
            "add MCHNO identical index between current and 16th-last transaction\n",
            "add MCHNO identical index between current and 17th-last transaction\n",
            "add MCHNO identical index between current and 18th-last transaction\n",
            "add MCHNO identical index between current and 19th-last transaction\n",
            "add MCHNO identical index between current and 20th-last transaction\n",
            "add FALLBACK_IND identical index between current and 1th-last transaction\n",
            "add FALLBACK_IND identical index between current and 2th-last transaction\n",
            "add FALLBACK_IND identical index between current and 3th-last transaction\n",
            "add FALLBACK_IND identical index between current and 4th-last transaction\n",
            "add FALLBACK_IND identical index between current and 5th-last transaction\n",
            "add FALLBACK_IND identical index between current and 6th-last transaction\n",
            "add FALLBACK_IND identical index between current and 7th-last transaction\n",
            "add FALLBACK_IND identical index between current and 8th-last transaction\n",
            "add FALLBACK_IND identical index between current and 9th-last transaction\n",
            "add FALLBACK_IND identical index between current and 10th-last transaction\n",
            "add FALLBACK_IND identical index between current and 11th-last transaction\n",
            "add FALLBACK_IND identical index between current and 12th-last transaction\n",
            "add FALLBACK_IND identical index between current and 13th-last transaction\n",
            "add FALLBACK_IND identical index between current and 14th-last transaction\n",
            "add FALLBACK_IND identical index between current and 15th-last transaction\n",
            "add FALLBACK_IND identical index between current and 16th-last transaction\n",
            "add FALLBACK_IND identical index between current and 17th-last transaction\n",
            "add FALLBACK_IND identical index between current and 18th-last transaction\n",
            "add FALLBACK_IND identical index between current and 19th-last transaction\n",
            "add FALLBACK_IND identical index between current and 20th-last transaction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.0658336\tvalid_1's binary_logloss: 0.0669881\n",
            "[100]\ttraining's binary_logloss: 0.0472499\tvalid_1's binary_logloss: 0.0489691\n",
            "[150]\ttraining's binary_logloss: 0.0381733\tvalid_1's binary_logloss: 0.0403189\n",
            "[200]\ttraining's binary_logloss: 0.0320485\tvalid_1's binary_logloss: 0.0346059\n",
            "[250]\ttraining's binary_logloss: 0.0271291\tvalid_1's binary_logloss: 0.0299395\n",
            "[300]\ttraining's binary_logloss: 0.0230709\tvalid_1's binary_logloss: 0.0260649\n",
            "[350]\ttraining's binary_logloss: 0.0199578\tvalid_1's binary_logloss: 0.0232195\n",
            "[400]\ttraining's binary_logloss: 0.0172758\tvalid_1's binary_logloss: 0.0207946\n",
            "[450]\ttraining's binary_logloss: 0.0150305\tvalid_1's binary_logloss: 0.0186554\n",
            "[500]\ttraining's binary_logloss: 0.0131036\tvalid_1's binary_logloss: 0.0168639\n",
            "[550]\ttraining's binary_logloss: 0.0114064\tvalid_1's binary_logloss: 0.0158174\n",
            "[600]\ttraining's binary_logloss: 0.0102802\tvalid_1's binary_logloss: 0.0149728\n",
            "[650]\ttraining's binary_logloss: 0.00900565\tvalid_1's binary_logloss: 0.0137161\n",
            "[700]\ttraining's binary_logloss: 0.00803193\tvalid_1's binary_logloss: 0.0127634\n",
            "Early stopping, best iteration is:\n",
            "[715]\ttraining's binary_logloss: 0.0077653\tvalid_1's binary_logloss: 0.0124946\n",
            "Recall Score: 0.9985821180719021\n",
            "Precision Score: 0.9960334340627469\n",
            "F1 Score: 0.9973061477359471\n",
            "Threshold:  0.5918564557086559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmXJbigp74v3"
      },
      "source": [
        "## Generate Testing Result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqxgniXH74Ew"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/test.csv')\n",
        "#查看資料筆數\n",
        "print(\"shape of test data:\" , test_data.shape)\n",
        "\n",
        "tmp_data = overall_preprocessing(test_data)\n",
        "\n",
        "X = create_X(resampled_train_data, \n",
        "  drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "y_pred = clf.predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkETzy7-_u51"
      },
      "source": [
        "threshold = 0.998\n",
        "y_result = (y_pred > threshold).astype(int).T\n",
        "result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "print(\"imbalance rate of test data:\", result_table['FRAUD_IND'].mean())\n",
        "result_table.to_csv('tmp_submission.csv')\n",
        "print('csv saved.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0Ybm5M9xbGh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH5HjwDRnWbM"
      },
      "source": [
        "tmp_data = extend_with_detailed_time(train_data, \n",
        "  weekday = True, hour = True)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "riKTAicXtvrD",
        "outputId": "81305cc1-3211-444b-e0ff-31c8b134a623"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe = OneHotEncoder()\n",
        "df3 = pd.DataFrame(ohe.fit_transform(train_data['']).toarray())\n",
        "df3"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TXKEY</th>\n",
              "      <th>DATETIME</th>\n",
              "      <th>CHID</th>\n",
              "      <th>CANO</th>\n",
              "      <th>MCHNO</th>\n",
              "      <th>ACQIC</th>\n",
              "      <th>MCC</th>\n",
              "      <th>CONTP</th>\n",
              "      <th>ETYMD</th>\n",
              "      <th>ECFG</th>\n",
              "      <th>INSFG</th>\n",
              "      <th>ITERM</th>\n",
              "      <th>BNSFG</th>\n",
              "      <th>BNSPT</th>\n",
              "      <th>FLAM1</th>\n",
              "      <th>STOCN</th>\n",
              "      <th>SCITY</th>\n",
              "      <th>OVRLT</th>\n",
              "      <th>PAY_TYPE</th>\n",
              "      <th>FALLBACK_IND</th>\n",
              "      <th>AGNO</th>\n",
              "      <th>CATP1</th>\n",
              "      <th>CUORG</th>\n",
              "      <th>FEEFG</th>\n",
              "      <th>FEDFG</th>\n",
              "      <th>CATP2</th>\n",
              "      <th>TSCFG</th>\n",
              "      <th>LSCFG</th>\n",
              "      <th>CGDCT</th>\n",
              "      <th>APPFG</th>\n",
              "      <th>SAMFG</th>\n",
              "      <th>ANDFG</th>\n",
              "      <th>AGE</th>\n",
              "      <th>CC_CUST_LEVEL</th>\n",
              "      <th>CC_VINTAGE</th>\n",
              "      <th>EDU_CODE</th>\n",
              "      <th>GENDER_CODE</th>\n",
              "      <th>INCOME_RANGE_CODE</th>\n",
              "      <th>MARITAL_STATUS_CODE</th>\n",
              "      <th>NATION_CODE</th>\n",
              "      <th>OCUP_CODE</th>\n",
              "      <th>POSITION_CODE</th>\n",
              "      <th>ACCT_VINTAGE</th>\n",
              "      <th>AVAILABLE_LIMIT_AMT</th>\n",
              "      <th>BONUS_POINTS</th>\n",
              "      <th>CC_PAY_LEVEL_CODE</th>\n",
              "      <th>CREDIT_LIMIT_AMT</th>\n",
              "      <th>CREDIT_REVOLVING_RATE</th>\n",
              "      <th>CREDIT_USE_RATE</th>\n",
              "      <th>CURRENT_CASH_ADV_AMT</th>\n",
              "      <th>CURRENT_FEE</th>\n",
              "      <th>CURRENT_INSTALLMENT_BAL</th>\n",
              "      <th>CURRENT_INSTALLMENT_PURCH_AMT</th>\n",
              "      <th>CURRENT_PURCH_AMT</th>\n",
              "      <th>CURRENT_VIO_AMT</th>\n",
              "      <th>LST_CYCLE_UNPAID_BAL</th>\n",
              "      <th>REVOLVING_AMT</th>\n",
              "      <th>REVOLVING_INTEREST</th>\n",
              "      <th>FRAUD_IND</th>\n",
              "      <th>HOUR</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WEEKDAY</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>1590</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>76253</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "      <td>73668</td>\n",
              "      <td>67787</td>\n",
              "      <td>67787</td>\n",
              "      <td>67787</td>\n",
              "      <td>67776</td>\n",
              "      <td>67787</td>\n",
              "      <td>67787</td>\n",
              "      <td>67787</td>\n",
              "      <td>67787</td>\n",
              "      <td>67787</td>\n",
              "      <td>67787</td>\n",
              "      <td>67787</td>\n",
              "      <td>67787</td>\n",
              "      <td>67787</td>\n",
              "      <td>67787</td>\n",
              "      <td>67787</td>\n",
              "      <td>67787</td>\n",
              "      <td>79577</td>\n",
              "      <td>79577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>1570</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>70757</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "      <td>68493</td>\n",
              "      <td>62969</td>\n",
              "      <td>62969</td>\n",
              "      <td>62969</td>\n",
              "      <td>62932</td>\n",
              "      <td>62969</td>\n",
              "      <td>62969</td>\n",
              "      <td>62969</td>\n",
              "      <td>62969</td>\n",
              "      <td>62969</td>\n",
              "      <td>62969</td>\n",
              "      <td>62969</td>\n",
              "      <td>62969</td>\n",
              "      <td>62969</td>\n",
              "      <td>62969</td>\n",
              "      <td>62969</td>\n",
              "      <td>62969</td>\n",
              "      <td>74044</td>\n",
              "      <td>74044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>1590</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>72308</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "      <td>69465</td>\n",
              "      <td>64576</td>\n",
              "      <td>64576</td>\n",
              "      <td>64576</td>\n",
              "      <td>64563</td>\n",
              "      <td>64576</td>\n",
              "      <td>64576</td>\n",
              "      <td>64576</td>\n",
              "      <td>64576</td>\n",
              "      <td>64576</td>\n",
              "      <td>64576</td>\n",
              "      <td>64576</td>\n",
              "      <td>64576</td>\n",
              "      <td>64576</td>\n",
              "      <td>64576</td>\n",
              "      <td>64576</td>\n",
              "      <td>64576</td>\n",
              "      <td>75445</td>\n",
              "      <td>75445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>1624</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>67838</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "      <td>65602</td>\n",
              "      <td>61955</td>\n",
              "      <td>61955</td>\n",
              "      <td>61955</td>\n",
              "      <td>61949</td>\n",
              "      <td>61955</td>\n",
              "      <td>61955</td>\n",
              "      <td>61955</td>\n",
              "      <td>61955</td>\n",
              "      <td>61955</td>\n",
              "      <td>61955</td>\n",
              "      <td>61955</td>\n",
              "      <td>61955</td>\n",
              "      <td>61955</td>\n",
              "      <td>61955</td>\n",
              "      <td>61955</td>\n",
              "      <td>61955</td>\n",
              "      <td>70868</td>\n",
              "      <td>70868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>1698</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>72388</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "      <td>70391</td>\n",
              "      <td>65926</td>\n",
              "      <td>65926</td>\n",
              "      <td>65926</td>\n",
              "      <td>65920</td>\n",
              "      <td>65926</td>\n",
              "      <td>65926</td>\n",
              "      <td>65926</td>\n",
              "      <td>65926</td>\n",
              "      <td>65926</td>\n",
              "      <td>65926</td>\n",
              "      <td>65926</td>\n",
              "      <td>65926</td>\n",
              "      <td>65926</td>\n",
              "      <td>65926</td>\n",
              "      <td>65926</td>\n",
              "      <td>65926</td>\n",
              "      <td>75843</td>\n",
              "      <td>75843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>2015</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>75992</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "      <td>73926</td>\n",
              "      <td>68850</td>\n",
              "      <td>68850</td>\n",
              "      <td>68850</td>\n",
              "      <td>68840</td>\n",
              "      <td>68850</td>\n",
              "      <td>68850</td>\n",
              "      <td>68850</td>\n",
              "      <td>68850</td>\n",
              "      <td>68850</td>\n",
              "      <td>68850</td>\n",
              "      <td>68850</td>\n",
              "      <td>68850</td>\n",
              "      <td>68850</td>\n",
              "      <td>68850</td>\n",
              "      <td>68850</td>\n",
              "      <td>68850</td>\n",
              "      <td>79315</td>\n",
              "      <td>79315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>1776</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>75025</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "      <td>72802</td>\n",
              "      <td>68355</td>\n",
              "      <td>68355</td>\n",
              "      <td>68355</td>\n",
              "      <td>68349</td>\n",
              "      <td>68355</td>\n",
              "      <td>68355</td>\n",
              "      <td>68355</td>\n",
              "      <td>68355</td>\n",
              "      <td>68355</td>\n",
              "      <td>68355</td>\n",
              "      <td>68355</td>\n",
              "      <td>68355</td>\n",
              "      <td>68355</td>\n",
              "      <td>68355</td>\n",
              "      <td>68355</td>\n",
              "      <td>68355</td>\n",
              "      <td>78110</td>\n",
              "      <td>78110</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         TXKEY  DATETIME   CHID  ...  REVOLVING_INTEREST  FRAUD_IND   HOUR\n",
              "WEEKDAY                          ...                                      \n",
              "1        79577     79577  79577  ...               67787      79577  79577\n",
              "2        74044     74044  74044  ...               62969      74044  74044\n",
              "3        75445     75445  75445  ...               64576      75445  75445\n",
              "4        70868     70868  70868  ...               61955      70868  70868\n",
              "5        75843     75843  75843  ...               65926      75843  75843\n",
              "6        79315     79315  79315  ...               68850      79315  79315\n",
              "7        78110     78110  78110  ...               68355      78110  78110\n",
              "\n",
              "[7 rows x 60 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcTdPQCmuHse"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}