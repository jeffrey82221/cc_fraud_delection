{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FraudDetectionTrainModulized_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMeyie9s3V5mkD+D6lyaa7P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffrey82221/cc_fraud_delection/blob/main/FraudDetectionTrainModulized_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHKUXmpFyUik"
      },
      "source": [
        "# Import Packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4td8ka0yM-8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import recall_score, precision_score, precision_recall_curve\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR_-LzQbyX_b"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwtoIWvXyZzy"
      },
      "source": [
        "import copy\n",
        "############################ Preprocessing ###################################\n",
        "def extend_with_detailed_time(data, weekday = True, hour = True):\n",
        "  '''\n",
        "  Add WEEKDAY and HOUR and convert DATETIME into strptime format. \n",
        "  '''\n",
        "  c_data = copy.copy(data)\n",
        "  c_data[\"DATETIME\"] = c_data[\"DATETIME\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
        "  if weekday:\n",
        "    c_data[\"WEEKDAY\"] = c_data[\"DATETIME\"].apply(lambda x: x.weekday() + 1)\n",
        "  if hour:\n",
        "    c_data[\"HOUR\"] = c_data[\"DATETIME\"].apply(lambda x: x.hour + 1)\n",
        "  return c_data \n",
        "\n",
        "def extend_with_time_difference_features(data, max_time_shift = 5, pivot_feature = 'CHID'):\n",
        "  # CHID: 卡人ID\n",
        "  # CANO: 交易卡號\n",
        "  c_data = copy.copy(data)\n",
        "  assert max_time_shift > 2\n",
        "  def date_diff(data, time_shift, pivot_feature):\n",
        "    df = copy.copy(data)\n",
        "    df[\"shift\"] = df.groupby([pivot_feature])[\"DATETIME\"].shift(time_shift)\n",
        "    name = pivot_feature + '_DIF' + str(time_shift)\n",
        "    df[name] = (df[\"DATETIME\"] - df['shift']).dt.total_seconds().fillna(0)\n",
        "    # \n",
        "    df = df.drop(\"shift\", 1)\n",
        "    return df\n",
        "  for time_shift in range(1, max_time_shift + 1):\n",
        "    print(\"add time difference between current and \" + str(time_shift) + \"th-last transaction\")\n",
        "    c_data = date_diff(c_data, time_shift, pivot_feature)\n",
        "  return c_data\n",
        "\n",
        "def preprocess_null_values(data):\n",
        "  # 將空值填補\n",
        "  c_data = copy.copy(data)\n",
        "  c_data[\n",
        "        c_data.select_dtypes(include=['object']).columns\n",
        "      ] = c_data[\n",
        "        c_data.select_dtypes(include=['object']).columns\n",
        "      ].fillna(\"NULL\")\n",
        "  c_data[\n",
        "      c_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    ] = c_data[\n",
        "      c_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    ].fillna(-1)\n",
        "  return c_data\n",
        "\n",
        "\n",
        "def encode_labels(data):\n",
        "  #將object欄位使用Label Encoder\n",
        "  c_data = copy.copy(data)\n",
        "  labelencoder = LabelEncoder()\n",
        "  obj_col = c_data.select_dtypes(include=['object']).columns.to_list()\n",
        "  for col in obj_col:\n",
        "      c_data[col] = labelencoder.fit_transform(c_data[col])\n",
        "  return c_data\n",
        "def preprocessing(data):\n",
        "  r_data = preprocess_null_values(data)\n",
        "  return encode_labels(r_data)\n",
        "############################ Training Preprocess ############################\n",
        "def resample(data, sampling_rate=0.7, sample_type='downsample'):\n",
        "  # note that testing data should not be re-sampled. \n",
        "  assert sample_type == 'downsample' or sample_type == 'upsample'\n",
        "  c_data = copy.copy(data) \n",
        "  #將資料切分為train&test\n",
        "  if sample_type == 'downsample': \n",
        "    df_fraud = c_data[c_data[\"FRAUD_IND\"] == 1]\n",
        "    df_not_fraud = c_data[c_data[\"FRAUD_IND\"] != 1].sample(frac=sampling_rate, random_state=42)\n",
        "  elif sample_type == 'upsample':\n",
        "    df_fraud = c_data[c_data[\"FRAUD_IND\"] == 1].sample(frac=1./sampling_rate, replace = True, random_state=42)\n",
        "    df_not_fraud = c_data[c_data[\"FRAUD_IND\"] != 1]\n",
        "  df_train = pd.concat([df_fraud, df_not_fraud], 0)\n",
        "  return df_train\n",
        "def create_X_y(data, drop_list = ['FRAUD_IND']):\n",
        "  X = data.drop(drop_list, 1)\n",
        "  y = data[\"FRAUD_IND\"]\n",
        "  return X,y\n",
        "\n",
        "############################ Model Build ####################################\n",
        "def train_lgb(x_train, x_test, y_train, y_test, max_depth = 8, learning_rate = 0.05, n_estimators = 1000):\n",
        "  # n_estimators: number of trees \n",
        "  lgb_train = lgb.Dataset(x_train, y_train)\n",
        "  lgb_test = lgb.Dataset(x_test, y_test)\n",
        "  params = {\n",
        "      \"boosting_type\": \"gbdt\",\n",
        "      \"objective\": \"binary\",\n",
        "      \"metric\": \"binary_logloss\",\n",
        "      \"max_depth\": max_depth,\n",
        "      \"learning_rate\": learning_rate,\n",
        "      \"n_estimators\": n_estimators,\n",
        "  }\n",
        "  trained_model = lgb.train(\n",
        "      params,\n",
        "      lgb_train,\n",
        "      num_boost_round=5000,\n",
        "      valid_sets=[lgb_train, lgb_test],\n",
        "      early_stopping_rounds=30,\n",
        "      verbose_eval=50\n",
        "  )\n",
        "  return trained_model\n",
        "##### Get Result Generated from Model #####################################\n",
        "def evaluate(clf, x_test, y_test):\n",
        "  y_pred = clf.predict(x_test)\n",
        "  precision, recall, threshold = precision_recall_curve(y_test, y_pred)\n",
        "  performance = {\"precision\": precision[0:-1],\n",
        "                \"recall\": recall[0:-1],\n",
        "                \"threshold\": threshold\n",
        "                }\n",
        "  performance[\"f1\"] = 2 * (performance[\"precision\"] * performance[\"recall\"]) / (performance[\"precision\"] + performance[\"recall\"])\n",
        "  performance = pd.DataFrame(performance)\n",
        "  thr = performance[performance[\"f1\"] == max(performance[\"f1\"])][\"threshold\"].values[0]\n",
        "  recall = performance[performance[\"f1\"] == max(performance[\"f1\"])][\"recall\"].values[0]\n",
        "  precision = performance[performance[\"f1\"] == max(performance[\"f1\"])][\"precision\"].values[0]\n",
        "  print(\"Recall Score:\", recall)\n",
        "  print(\"Precision Score:\", precision)\n",
        "  print(\"F1 Score:\", 2 * (precision * recall) / (precision + recall))\n",
        "  print(\"Threshold: \", thr)\n",
        "def get_important_feature_table(clf, x_train):\n",
        "  importance = {\n",
        "  \"col\": np.array(x_train.columns),\n",
        "  \"imp\": lgb.Booster.feature_importance(clf)\n",
        "  }\n",
        "  df_imp = pd.DataFrame(importance).sort_values(by='imp', ascending=False)\n",
        "  return df_imp"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSuPU5SyykV5"
      },
      "source": [
        "# First Run (for selecting unimportant features) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "iRMDS2I9yk2B",
        "outputId": "c0a9f337-e97a-47e9-9725-4022f7cfe7bb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "# add AGE \n",
        "# remove weekday and hour \n",
        "tmp_train_data = extend_with_detailed_time(train_data, \n",
        "  weekday = False, hour = False)\n",
        "preprocessed_train_data = preprocessing(tmp_train_data)\n",
        "resampled_train_data = resample(preprocessed_train_data, \n",
        "  sampling_rate=0.7, sample_type='downsample')\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = [\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"])\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=val_percentage, \n",
        "  shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.05, n_estimators = 1000)\n",
        "evaluate(clf, x_test, y_test)\n",
        "important_feature_table = get_important_feature_table(clf, x_train)\n",
        "important_feature_table.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "shape of train data: (533202, 59)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.162201\tvalid_1's binary_logloss: 0.162768\n",
            "[100]\ttraining's binary_logloss: 0.13457\tvalid_1's binary_logloss: 0.13604\n",
            "[150]\ttraining's binary_logloss: 0.12292\tvalid_1's binary_logloss: 0.125427\n",
            "[200]\ttraining's binary_logloss: 0.115362\tvalid_1's binary_logloss: 0.118842\n",
            "[250]\ttraining's binary_logloss: 0.109307\tvalid_1's binary_logloss: 0.113742\n",
            "[300]\ttraining's binary_logloss: 0.103638\tvalid_1's binary_logloss: 0.108953\n",
            "[350]\ttraining's binary_logloss: 0.0989313\tvalid_1's binary_logloss: 0.105065\n",
            "[400]\ttraining's binary_logloss: 0.0948469\tvalid_1's binary_logloss: 0.101827\n",
            "[450]\ttraining's binary_logloss: 0.0910325\tvalid_1's binary_logloss: 0.0986804\n",
            "[500]\ttraining's binary_logloss: 0.0874282\tvalid_1's binary_logloss: 0.0957317\n",
            "[550]\ttraining's binary_logloss: 0.0840682\tvalid_1's binary_logloss: 0.0930062\n",
            "[600]\ttraining's binary_logloss: 0.081057\tvalid_1's binary_logloss: 0.0906978\n",
            "[650]\ttraining's binary_logloss: 0.0782779\tvalid_1's binary_logloss: 0.0886361\n",
            "[700]\ttraining's binary_logloss: 0.0756893\tvalid_1's binary_logloss: 0.0866863\n",
            "[750]\ttraining's binary_logloss: 0.0733625\tvalid_1's binary_logloss: 0.084959\n",
            "[800]\ttraining's binary_logloss: 0.0712952\tvalid_1's binary_logloss: 0.0834625\n",
            "[850]\ttraining's binary_logloss: 0.0691694\tvalid_1's binary_logloss: 0.0818883\n",
            "[900]\ttraining's binary_logloss: 0.0674926\tvalid_1's binary_logloss: 0.0807932\n",
            "[950]\ttraining's binary_logloss: 0.0655437\tvalid_1's binary_logloss: 0.0793593\n",
            "[1000]\ttraining's binary_logloss: 0.0636926\tvalid_1's binary_logloss: 0.0779864\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's binary_logloss: 0.0636926\tvalid_1's binary_logloss: 0.0779864\n",
            "Recall Score: 0.9389421933232892\n",
            "Precision Score: 0.926380848067595\n",
            "F1 Score: 0.9326192257708819\n",
            "Threshold:  0.5136044918384114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col</th>\n",
              "      <th>imp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>CC_VINTAGE</td>\n",
              "      <td>2721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MCC</td>\n",
              "      <td>2437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>SCITY</td>\n",
              "      <td>1885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>FLAM1</td>\n",
              "      <td>1810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>BONUS_POINTS</td>\n",
              "      <td>1545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             col   imp\n",
              "27    CC_VINTAGE  2721\n",
              "0            MCC  2437\n",
              "10         SCITY  1885\n",
              "8          FLAM1  1810\n",
              "37  BONUS_POINTS  1545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b4ggPwyy_55"
      },
      "source": [
        "# Best Run in v1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRJTAIYTy-aj",
        "outputId": "32f3d727-8827-4aae-a137-061ff6fe0edd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#匯入資料\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/智金輪習Kaggle/train.csv')\n",
        "test_data = \"先不給你們\"\n",
        "#查看資料筆數\n",
        "print(\"shape of train data:\" , train_data.shape)\n",
        "#print(\"shape of test data:\" , test_data.shape)\n",
        "tmp_train_data = extend_with_detailed_time(train_data, \n",
        "  weekday = True, hour = True)\n",
        "train_tmp_data = extend_with_time_difference_features(tmp_train_data, \n",
        "  max_time_shift = 20, pivot_feature = 'CHID')\n",
        "preprocessed_train_data = preprocessing(train_tmp_data)\n",
        "resampled_train_data = resample(preprocessed_train_data, \n",
        "  sampling_rate=0.7, sample_type='downsample')\n",
        "removed_unimportant_feature_count = 5\n",
        "X, y = create_X_y(resampled_train_data, \n",
        "  drop_list = list(set([\"FRAUD_IND\", \"TXKEY\", \"DATETIME\", \"CANO\", \"CHID\", \"ACQIC\", \"MCHNO\", \"AGE\"] + \\\n",
        "  important_feature_table.set_index('col').index[-(removed_unimportant_feature_count):].tolist()))\n",
        ")\n",
        "val_percentage = 0.33\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "  test_size=val_percentage, shuffle=True, random_state=42)\n",
        "clf = train_lgb(x_train, x_test, y_train, y_test, \n",
        "  max_depth = 8, learning_rate = 0.05, n_estimators = 1000)\n",
        "evaluate(clf, x_test, y_test)\n",
        "#important_feature_table = get_important_feature_table(clf, x_train)\n",
        "#important_feature_table"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "shape of train data: (533202, 59)\n",
            "add time difference between current and 1th-last transaction\n",
            "add time difference between current and 2th-last transaction\n",
            "add time difference between current and 3th-last transaction\n",
            "add time difference between current and 4th-last transaction\n",
            "add time difference between current and 5th-last transaction\n",
            "add time difference between current and 6th-last transaction\n",
            "add time difference between current and 7th-last transaction\n",
            "add time difference between current and 8th-last transaction\n",
            "add time difference between current and 9th-last transaction\n",
            "add time difference between current and 10th-last transaction\n",
            "add time difference between current and 11th-last transaction\n",
            "add time difference between current and 12th-last transaction\n",
            "add time difference between current and 13th-last transaction\n",
            "add time difference between current and 14th-last transaction\n",
            "add time difference between current and 15th-last transaction\n",
            "add time difference between current and 16th-last transaction\n",
            "add time difference between current and 17th-last transaction\n",
            "add time difference between current and 18th-last transaction\n",
            "add time difference between current and 19th-last transaction\n",
            "add time difference between current and 20th-last transaction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "[50]\ttraining's binary_logloss: 0.083666\tvalid_1's binary_logloss: 0.0839237\n",
            "[100]\ttraining's binary_logloss: 0.0522189\tvalid_1's binary_logloss: 0.0540179\n",
            "[150]\ttraining's binary_logloss: 0.0434371\tvalid_1's binary_logloss: 0.0461507\n",
            "[200]\ttraining's binary_logloss: 0.0390603\tvalid_1's binary_logloss: 0.0424071\n",
            "[250]\ttraining's binary_logloss: 0.0351947\tvalid_1's binary_logloss: 0.0391872\n",
            "[300]\ttraining's binary_logloss: 0.0323212\tvalid_1's binary_logloss: 0.036856\n",
            "[350]\ttraining's binary_logloss: 0.0296248\tvalid_1's binary_logloss: 0.0347479\n",
            "[400]\ttraining's binary_logloss: 0.027224\tvalid_1's binary_logloss: 0.0327181\n",
            "[450]\ttraining's binary_logloss: 0.0251559\tvalid_1's binary_logloss: 0.0310547\n",
            "[500]\ttraining's binary_logloss: 0.0229929\tvalid_1's binary_logloss: 0.0294841\n",
            "[550]\ttraining's binary_logloss: 0.0211482\tvalid_1's binary_logloss: 0.0281075\n",
            "[600]\ttraining's binary_logloss: 0.0193928\tvalid_1's binary_logloss: 0.0266691\n",
            "[650]\ttraining's binary_logloss: 0.0179639\tvalid_1's binary_logloss: 0.0256464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlOxRa9dznLI"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKhwrsE2zUJn"
      },
      "source": [
        "y_pred = clf.predict(X)\n",
        "for threshold in [0.96, 0.97, 0.98, 0.99]:\n",
        "  print('threshold:', threshold)\n",
        "  y_result = (y_pred > threshold).astype(int).T\n",
        "  result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "  result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "  print(\"imbalance rate of test data:\", result_table['FRAUD_IND'].mean())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njT7OuG7zbxq"
      },
      "source": [
        "threshold = 0.97\n",
        "y_result = (y_pred > threshold).astype(int).T\n",
        "result_table = pd.DataFrame([test_data['TXKEY'], y_result]).T\n",
        "result_table.columns = ['TXKEY', 'FRAUD_IND']\n",
        "print(\"imbalance rate of test data:\", result_table['FRAUD_IND'].mean())\n",
        "result_table.to_csv('tmp_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}